[["index.html", "DLL 2021, R section 1 Workshop Introduction", " DLL 2021, R section mesako Margaret Samson Zac darachm Typeset on 2021-06-08 1 Workshop Introduction This short 3-day course in R aims to give you a basic framework and skills for working effectively with your research mentors. Together, we will get oriented with basic skills (e.g. using RStudio, documenting your process with R Markdown, reading data in, basic data analysis, and visualization) and concepts for how to organize your research workflows in R. We intend for this starting point to empower you to accomplish research-related tasks in R. R is a high-level data analysis scripting language 1. While it is very easy to write programs in this language, it is designed first as an environment that stitches together cutting edge research methods with flexible visualization and reporting frameworks. R has swept to be the de facto high-level language for data analysis because of the rich ecosystem of dispersed open-source developers. Here’s some examples of plots you generate in R. Here’s an example of the types of workflows and analyses you can generate in R (all the plots, and the website too). Heck, this website is generated by the R package bookdown from Rmd files, which you will learn to write. and also a “GNU” project, apparently!↩ "],["workshop-goals.html", "1.1 Workshop goals", " 1.1 Workshop goals We aim for all participants to be able to: use the Rstudio IDE (open source edition) know how to store and manipulate data in variables read in data from computer files in various formats process these with functions to generate statistical summaries turn these into various plots using the base graphics and ggplot2 library read in packages from various sources and know how to start using them do these steps in workflows that scale to analyzing many many files write all of this up as an Rmarkdown file to report your analysis and findings to collaborators "],["structure-and-resources.html", "1.2 Structure and resources", " 1.2 Structure and resources 1.2.1 This syllabus and videos We developed this website/document for your reference, as a living textbook and collection of “slides” and code snippets. We have also made short teaching videos uploaded on Youtube that are embedded where appropriate on this site. These mini-lectures are intended to complement the text for those with different learning preferences. You will be expected to progress through this website during asynchronous work time in this 3-day period. During synchronous meetings, you should plan to work directly with your peers and us on focused tasks. We will also be there to help with confusing or challenging topics that you want to discuss with someone live. Tips: You can shrink the table of contents (left) by clicking the four lines icon in the top menu. You can click on footnotes 2. 1.2.2 Slack channel While you are working in asynchronous sessions, or if you just need help during the program, there is a Slack channel available where you can go for ideas/help. The channel is called #codingtroubleshooting and should be accessible to you on the SSRP Slack server. to read them, then go back by clicking the arrow↩ "],["workshop-expectations.html", "1.3 Workshop expectations", " 1.3 Workshop expectations Be respectful and compassionate. Teach one another, learn from one another. Aim for productive struggle. You will learn best if you make a good faith effort before seeking help. However, you should always seek help if you feel truly stuck. Create your own sense of challenge. Pick activities that you will learn and grow from. If you don’t find something challenging, make it challenging for yourself. "],["day-3-intoduction-to-r.html", "2 Day 3 - intoduction to R", " 2 Day 3 - intoduction to R Learning Goals: By the end of today’s session, students will be able to: Navigate the RStudio environment and move between the different panels. Open, save, and run an Rmd (R Markdown) file within RStudio. Apply internet searches, help functions, and documentation to learn to use the appropriate functions. Assign and manipulate variables within the current environment/session in R. Create, modify, and access into a vector (an ordered grouping of elements). Create, modify, and access into a dataframe (an ordered two-dimensional grouping of elements with rows and columns). Apply existing functions to accomplish a specific task in R. Preparing for Today: Please read through all parts of Section 1: Workshop Introduction. Today’s Schedule: Start Time End Time Activity Description 10:00 AM PDT 10:45 AM PDT Synchronous Welcome session on Zoom 10:45 AM PDT 11:30 AM PDT Asynchronous Work through sections 2.1-2.3 11:30 AM PDT 12:15 PM PDT Synchronous Peer coding exercises on Zoom 12:15 PM PDT 12:45 PM PDT Lunch Break Take a computer break! 12:45 PM PDT 1:30 PM PDT Asynchronous Work through sections 2.4-2.5 1:30 PM PDT 2:30 PM PDT Synchronous Peer coding exercises on Zoom 2:30 PM PDT 3:00 PM PDT Snack Break Take a computer break! 3:00 PM PDT 3:30 PM PDT Synchronous Wrap-up session on Zoom "],["navigating-rstudio.html", "2.1 Navigating Rstudio", " 2.1 Navigating Rstudio Rstudio x &lt;- 1 x ## [1] 1 "],["using-rmds.html", "2.2 Using Rmds", " 2.2 Using Rmds Approach: an Rmd tutorial with a simple dataset in R. This is meant to be a motivation for what one can do in R, not necessarily understanding syntax of every function/operation used Topics/Exercises: Present a conceptual framework for a pipeline - more of a tutorial Use an Rmd file for this and introduce how to navigate the Rmd file Scholars can execute a given Rmd Scholars can modify a given Rmd Scholars know what the major pieces of an Rmd are (code and not code) Scholars know about caching and arguments for code chunks Scholars know how to make their own new Rmd file and run it Load a simple dataset in R - the iris dataset, peek at the table with head, summarize the columns, and make a scatter plot with base R. This introduces several topics Requires: interacting with R, calling functions, and saving variables "],["getting-started-in-r.html", "2.3 Getting started in R", " 2.3 Getting started in R Let’s get started programming in R! Our goal is to be able to interact with datasets like the built-in iris dataset in R. By the end of today, we aim to be able to pull out information from this dataset and modify it using R programming. head(iris, n = 10) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5.0 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa 2.3.1 Getting help in R If you run into any error while using R, it is a great idea to look up your error message in Google and read through forum posts on StackOverflow. You may find it also helps to know more about your session info such as the version of R you are using, and what packages you have currently loaded. sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 20.04.2 LTS ## ## Matrix products: default ## BLAS: /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.9.0 ## LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.9.0 ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8 ## [5] LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## loaded via a namespace (and not attached): ## [1] compiler_3.6.3 magrittr_2.0.1 bookdown_0.22 htmltools_0.5.0 ## [5] tools_3.6.3 yaml_2.2.1 codetools_0.2-16 stringi_1.5.3 ## [9] rmarkdown_2.8 knitr_1.30 stringr_1.4.0 digest_0.6.27 ## [13] xfun_0.23 rlang_0.4.9 evaluate_0.14 You can look up what functions do using either a question mark ? or the help function. ?rm help(rm) You can try running the following command to look up variables that load in with base R or packages that we will use. For example, you can use the help function to read more details about the iris dataset. ?iris 2.3.2 Using variables and data types Variables are a short-hand name or label that stores a piece of data. We can save information into variables and then call them by name (invoke the variable) to use that information when needed. You can assign variables using the = or &lt;- operators. We will use the &lt;- operator exclusively going forward. save.num &lt;- 7 save.num ## [1] 7 Here I have chosen the name save.num but you can give it a different name. Note that variable names must with a letter and cannot contain special characters. Variables are mutable: you can overwrite the saved value of a variable with another value. save.num &lt;- 10 save.num ## [1] 10 You can see that save.num does not remember the value 7 anymore and instead returns 10. Variables can be saved as other values such as character strings or boolean values (TRUE or FALSE). These are examples of other datatypes in R. save.string &lt;- &quot;hello&quot; save.bool &lt;- TRUE You can check what variables you have assigned in your current working environment using the ls function. Try running ?ls to learn more about this function! ls() ## [1] &quot;save.bool&quot; &quot;save.num&quot; &quot;save.string&quot; You can also remove saved variables using the rm function. rm(save.num) ls() ## [1] &quot;save.bool&quot; &quot;save.string&quot; You will notice that our session no longer remembers save.num. If you tried to call save.num after it was removed, you would get an error message in R. 2.3.3 Using functions We have already shared several functions with you, including the help function, the ls function, and the rm function. Even the command sessionInfo is an example of a function in R! Functions can be recognized by a string (letters) followed by parentheses (). Functions may take information inside the parentheses that are called arguments. ?help When you look at documentation for the help function, you will notice that there are many things you can provide inside the parentheses. The documentation section called “Arguments” describes one necessary input for help called topic that must be provided inside the parentheses. Arguments are often named and described in the documentation. When you provide an argument to a function, you can provide it by name explicitly or just let R figure it out based on the order you give it. help(topic = iris) help(iris) In the second line, we provide only one input iris and R assumes that we intend for topic = iris as the argument. One function we will use often is the c function which can be used to create a vector or ordered collection of pieces of information. ?c If you look up the documentation for the c function, you will notice that it accepts ... arguments. This can be confusing but it often means that it accepts multiple arguments, more than can be named or described. This makes sense for the c function because it will accept as many inputs as you give it, the number does not need to be consistent. save.data &lt;- c(1, 4, 6, 2, 3, 8, NA) Functions generally take the arguments in the parentheses as an input and then produce some output. A clear example of this is using the mean function. ?mean As implied, the mean function will take a grouping of numbers and return the mean or the average. mean(save.data) ## [1] NA However, here we ran into an issue because one of the elements in our initial vector save.data is NA. NA means “not available” or in other words the data is missing. This is not the same as being zero. So R is not sure how to calculate the mean since that last element is essentially a question mark. We can make use of an additional argument that the mean function takes, namely the na.rm argument. This argument functions more like a setting, where you can provide a flag (i.e. TRUE or FALSE) or a distinct option (e.g. top, bottom, left, or right) that modifies how it produces the output. This argument has a default that is shown in the “Usage” section of the documentation. We are going to change that setting. mean(save.data, na.rm = TRUE) ## [1] 4 Now you can see that R runs the function and decides to leave out or ignore the NA value and is able to return an average based on the other values in save.data. "],["working-with-vectors.html", "2.4 Working with vectors", " 2.4 Working with vectors The power of using programming like R is to be able to process, analyze, and visualize large sets of data. We will build our way up to thinking about tabular data like that in the iris dataset starting first with vectors. Remember that you can create vectors (an ordered list of elements) by combining elements with the c function. 2.4.1 Building vectors Let us pretend that we measured and recorded the resting heart rates of several patients. We will create these vectors and assign them to some variables. heart.rates &lt;- c(78, 68, 95, 82, 69, 63, 86, 74, 64, 62) more.heart.rates &lt;- c(86, 79, 64, 74, 80) heart.rates ## [1] 78 68 95 82 69 63 86 74 64 62 more.heart.rates ## [1] 86 79 64 74 80 We have used the c function to combine several individual elements that we typed out, but you can combine vectors together to make an even bigger vector. all.heart.rates &lt;- c(heart.rates, more.heart.rates) all.heart.rates ## [1] 78 68 95 82 69 63 86 74 64 62 86 79 64 74 80 Vectors can contain different datatypes besides numbers. We can also provide several character strings in a vector, like the names of our patients. patient.names &lt;- c(&quot;oakley&quot;, &quot;rashmi&quot;, &quot;kiran&quot;, &quot;eun&quot;, &quot;sasha&quot;, &quot;mattie&quot;) However, note that vectors cannot handle multiple different datatypes at once. If you try to provide a vector with multiple datatypes: mixed.data &lt;- c(&quot;oakley&quot;, 18, TRUE, &quot;eun&quot;, NA, 50) mixed.data ## [1] &quot;oakley&quot; &quot;18&quot; &quot;TRUE&quot; &quot;eun&quot; NA &quot;50&quot; You can see this displays these elements enclosed in quotes, to indicate that R has converted them all to the same datatype (i.e. character string). There are several helpful functions and an operator that can speed up your ability to generate vectors. The : operator quickly creates a numeric vector for a range of values. 1:4 ## [1] 1 2 3 4 The seq function can help you quickly create numeric vectors. The rep function can be used to build either numeric or character string vectors. Reading the documentation will tell you which arguments these functions take. ?seq ?rep Here are examples of how they can be used: z &lt;- seq(0, 100, by = 20) z ## [1] 0 20 40 60 80 100 z &lt;- rep(&quot;A&quot;, times = 5) z ## [1] &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; 2.4.2 Indexing and subsetting vectors Indexing is a way to access into a vector (or a matrix or a data frame) and pull out certain elements. There are multiple ways to index into a vector, one of the easiest ways is to pull out an element based on its order/position in the grouping (its index). We use [] immediately after the name of the grouping (in this case a vector) to access into it. all.heart.rates[2] # pull out second heart rate measurement ## [1] 68 patient.names[3] # pull out third patient name ## [1] &quot;kiran&quot; You can also pull out several elements at a time. To do this, you provide a vector of numeric values within the parentheses. patient.names[2:3] # pull out 2nd and 3rd patient name ## [1] &quot;rashmi&quot; &quot;kiran&quot; patient.names[c(1, 3)] # pull out 1st and 3rd patient name ## [1] &quot;oakley&quot; &quot;kiran&quot; You can remove elements of a vector by using the same syntax of indexing, but instead put a negative sign in front of the index number. patient.names[-1] ## [1] &quot;rashmi&quot; &quot;kiran&quot; &quot;eun&quot; &quot;sasha&quot; &quot;mattie&quot; There are times where we will want to pull out certain elements of a vector using something other than the position. What if we do not know where they are located, especially if the vector is very long? 2.4.3 Using logic and logicals You can also access elements in a vector that meet certain criteria using what we will call conditional logic. Logic will be a recurring idea in programming. Logic looks like evaluating whether something meets your criteria and evaluating it as either TRUE or FALSE (remember these words are special and represent a logical datatype). Let’s explore what it looks like to evaluate a statement. save.num &lt;- 7 save.num &lt; 8 ## [1] TRUE save.num &gt; 8 ## [1] FALSE save.num != 8 ## [1] TRUE save.num == 8 ## [1] FALSE You can use the standard comparison operators like &gt; or &lt; to check greater than or less than. You can also use == to check for equality or != to check that values are not equal. We can do the same thing with vectors, and it will be performed in a vectorized manner. That is, by default, R will evaluate each element in the vector to see if it meets the criteria. For example, we can evaluate each heart rate in our original vector to see whether or not the value is less than 80. all.heart.rates &lt; 80 ## [1] TRUE TRUE FALSE FALSE TRUE TRUE FALSE TRUE TRUE TRUE FALSE TRUE ## [13] TRUE TRUE FALSE This series of flags with TRUE and FALSE can be used to access into a vector and will only return the elements where there is a TRUE. all.heart.rates[all.heart.rates &lt; 80] ## [1] 78 68 69 63 74 64 62 79 64 74 In the context of bigger data analysis, we can use conditional logic to make choices between executing different sets of code. You can pair these conditional statements with an if/else statement that breaks up the code and only executes parts of the code where conditions are met. test.num &lt;- 30 if (test.num &lt; 10) { print(&quot;small&quot;) } else { print(&quot;big&quot;) } ## [1] &quot;big&quot; Here, the code within the brackets only runs if the condition is met. Since the statement next to the if evaluates as FALSE, it then only executes the code within the brackets of the else clause. 2.4.4 Modifying vectors Let’s look into how we change an existing vector. We can save data into a vector, access this data, but we will likely want to process it too! We can investigate an unknown vector using several functions, like the length function. As the name implies, it returns how many elements your vector contains. x &lt;- c(1, 3, 2) y &lt;- c(5, 4, 6) length(x) ## [1] 3 length(y) ## [1] 3 Let’s try modifying the values in an existing vector. Note that when you try to add values to a vector, it can be done in a pairwise manner or uniformly across the entire vector. Let’s see what it looks like when we have pairwise addition of two vectors of the same length. x + y ## [1] 6 7 8 Here, you can see that the first element of x is added to the first element of y to create a new value in the first element. The same happens with the second element and so forth. You can perform mathematical operations on an entire numeric vector all at once. Here we take the vector of our heart rate data and add 1 to it, which will add 1 to each individual element in the entire vector. all.heart.rates + 1 ## [1] 79 69 96 83 70 64 87 75 65 63 87 80 65 75 81 This is another example of a vectorized operation, where we perform a step automatically to all items in a collection of items (e.g. a vector). This works with operators like + or -, but also with certain functions. ?log For example, the log function calculates the log value of a number. If you provide it with a vector, it will do that calculation for each element, returning a vector of the same length. log(all.heart.rates) ## [1] 4.356709 4.219508 4.553877 4.406719 4.234107 4.143135 4.454347 4.304065 ## [9] 4.158883 4.127134 4.454347 4.369448 4.158883 4.304065 4.382027 This demonstrates how one can quickly transform or scale values in a dataset. For example, you might have taken the temperature of an experiment in Fahrenheit and using the power of vectorization, you can apply the same arithmetic steps to all of your measurements simultaneously to get the values in Celsius. Here is an additional example using the signif function, which returns your numeric values with the number of significant digits that you specify. ?signif Here we can specify one significant figure to get only the tens digit (or the hundreds digit if there is a value over 100). signif(all.heart.rates, digits = 1) ## [1] 80 70 100 80 70 60 90 70 60 60 90 80 60 70 80 Any of these results can also be saved into a new vector name (e.g. heart.rates2) or we can save over the original variable name if we are fixing the data. "],["working-with-dataframes.html", "2.5 Working with dataframes", " 2.5 Working with dataframes We can now work our way from vectors to dataframes. The most common data format we will deal with in research is a dataframe format. A dataframe has data is stored in a tabular format with the rows generally referring to individual measurements (single patients, samples, cells, etc.) and the columns referring to parameters (genes, proteins, etc.) measured in each individual. Essentially a dataframe can be thought of a bunch of vectors lined up in columns or lined up in rows. We work with a dataframe instead of a matrix (another datatype in R) because dataframes can tolerate different datatypes in the same table. As seen below, a matrix will easily accept data all of the same datatype, but do unexpected things when you provide multiple datatypes. matrix1 &lt;- matrix(data = c(1, 2, 3, 4, 5, 6, 7, 8, 9), nrow = 3, ncol = 3) matrix1 ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 matrix2 &lt;- matrix(data = c(1, &quot;apple&quot;, 3, TRUE, &quot;cat&quot;, 6, 7, NA, FALSE), nrow = 3, ncol = 3) matrix2 ## [,1] [,2] [,3] ## [1,] &quot;1&quot; &quot;TRUE&quot; &quot;7&quot; ## [2,] &quot;apple&quot; &quot;cat&quot; NA ## [3,] &quot;3&quot; &quot;6&quot; &quot;FALSE&quot; Now matrix1 looks normal, but you may notice that matrix2 has quotation marks around its elements, including the numbers and the TRUE/FALSE values. This means these elements are all being treated like character strings because we included elements like apple and R wants them to be one consistent type. While we are working mainly with dataframes, tibbles will pop up as we work in tidyverse. Tibbles are very similar to dataframes in their ability to handle different data types across their different columns. You can think of them as very similar entities. We will first explore the built-in iris dataset. If you look up its documentation, you will notice that it is described as a dataframe and does contain both numeric values and character strings for the names of species. ?iris For the sake of this exercise, we will assign a new variable called iris.temp that is a shorter version of the original dataset. The head function returns just the first few rows and here we use an argument to request the first 10. iris.temp &lt;- head(iris, n = 10) iris.temp ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5.0 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa 2.5.1 Indexing and subsetting dataframes You can index into a dataframe and pull out one or more cells within the dataframe. Note that we are providing two coordinates to explain which row (the first number before the comma) and which column (the second number after the comma) to find the exact element (or cell in the table). iris.temp[1, 3] ## [1] 1.4 You can pull out multiple elements at a time, specifying which row and column they reside in. iris.temp[c(1, 2), c(2, 3)] # gives us the 2nd and 3rd columns of the 1st and 2nd rows ## Sepal.Width Petal.Length ## 1 3.5 1.4 ## 2 3.0 1.4 iris.temp[1:3, 3:5] # gives us the 3rd through 5th columns of the 1st through 3rd rows ## Petal.Length Petal.Width Species ## 1 1.4 0.2 setosa ## 2 1.4 0.2 setosa ## 3 1.3 0.2 setosa If you provide the row and not the column, or vice versa, by default R will pull out all of the available columns and rows respectively. iris.temp[1:2, ] # pulls out the first two rows and all columns ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa iris.temp[, 1:2] # pulls out the first two columns and all rows ## Sepal.Length Sepal.Width ## 1 5.1 3.5 ## 2 4.9 3.0 ## 3 4.7 3.2 ## 4 4.6 3.1 ## 5 5.0 3.6 ## 6 5.4 3.9 ## 7 4.6 3.4 ## 8 5.0 3.4 ## 9 4.4 2.9 ## 10 4.9 3.1 It’s possible to also remove elements in a dataframe using the negative sign. iris.temp2 &lt;- iris.temp[-c(1, 3), ] # removes the first and third rows head(iris.temp) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa head(iris.temp2) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 2 4.9 3.0 1.4 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5.0 3.4 1.5 0.2 setosa We saved the shorter dataset into a new variable and when we preview iris.temp against iris.temp2 you may be able to see the missing rows. 2.5.2 Exploring dataframes When you are first presented with a dataframe, for example data that was collected in your research lab that you are tasked with analyzing, you will want to learn more about it. There are a few different functions you can use to investigate a dataframe, the size of it, and other aspects. The dim function, short for dimensions, returns the number of rows and columns. dim(iris) ## [1] 150 5 dim(iris.temp) ## [1] 10 5 This shows you that iris.temp is in fact just the first 10 rows of iris. You can use the following functions: str for structure, colnames for column names, and summary to investigate aspects of a given dataset. colnames(iris) ## [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; &quot;Species&quot; str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## The str function reveals that the iris dataframe contains different data types. Specifcally, it mostly contains columns of numbers as well as a column of factors or categorical data referring to which species the iris belonged to. The summary function tries to tell us more info about each column. For numerical data, it summarizes the min and max values, the quartiles, and the center values (e.g. median or mean). For categorical data like the Species column, this function shows how many rows belong to each category. We can try to look at the beginning of a specific column in this dataset to get a better understanding for it. Dataframes that have names for their columns allow you to index into the columns specifically by name using the $ operator as shown below. head(iris$Species) ## [1] setosa setosa setosa setosa setosa setosa ## Levels: setosa versicolor virginica Remember the head function lets us preview a longer set of data, either showing the first few elements of a vector or the first few rows of a dataframe. 2.5.3 Building and modifying dataframes We can also generate our own dataframes from vectors that we put together into a table. Revisiting our heart rate measurement example, let’s build a dataframe of patient data. patient.data &lt;- data.frame(name = c(&quot;oakley&quot;, &quot;rashmi&quot;, &quot;kiran&quot;), heart_rate = c(78, 68, 95), disease_status = c(FALSE, FALSE, TRUE)) patient.data ## name heart_rate disease_status ## 1 oakley 78 FALSE ## 2 rashmi 68 FALSE ## 3 kiran 95 TRUE We can inspect the dataframe we have created using the same functions. str(patient.data) ## &#39;data.frame&#39;: 3 obs. of 3 variables: ## $ name : Factor w/ 3 levels &quot;kiran&quot;,&quot;oakley&quot;,..: 2 3 1 ## $ heart_rate : num 78 68 95 ## $ disease_status: logi FALSE FALSE TRUE summary(patient.data) ## name heart_rate disease_status ## kiran :1 Min. :68.00 Mode :logical ## oakley:1 1st Qu.:73.00 FALSE:2 ## rashmi:1 Median :78.00 TRUE :1 ## Mean :80.33 ## 3rd Qu.:86.50 ## Max. :95.00 You can add new rows and columns using the rbind and cbind functions. Let’s pretend that we had collected additional information about our patients, such as their self-reported gender. We can add this as a new column (cbind short for bind column). patient.data &lt;- cbind(patient.data, gender = c(&quot;M&quot;, &quot;F&quot;, NA)) patient.data ## name heart_rate disease_status gender ## 1 oakley 78 FALSE M ## 2 rashmi 68 FALSE F ## 3 kiran 95 TRUE &lt;NA&gt; You can merge two dataframes together using the rbind function assuming that their columns match up correctly. Let’s pretend that we had another day at the clinic and collected additional patient measurements. more.patients &lt;- data.frame(name = c(&quot;eun&quot;, &quot;sasha&quot;, &quot;mattie&quot;), heart_rate = c(86, 79, 64), disease_status = c(TRUE, TRUE, FALSE), gender = c(NA, &quot;M&quot;, &quot;F&quot;)) more.patients ## name heart_rate disease_status gender ## 1 eun 86 TRUE &lt;NA&gt; ## 2 sasha 79 TRUE M ## 3 mattie 64 FALSE F Let’s use rbind, short for binding rows, to add these additional rows to the bottom of our first dataframe patient.data. all.patients &lt;- rbind(patient.data, more.patients) all.patients ## name heart_rate disease_status gender ## 1 oakley 78 FALSE M ## 2 rashmi 68 FALSE F ## 3 kiran 95 TRUE &lt;NA&gt; ## 4 eun 86 TRUE &lt;NA&gt; ## 5 sasha 79 TRUE M ## 6 mattie 64 FALSE F You can also remove rows and columns using the trick with a negative index. patient.data[, -1] # removes first column ## heart_rate disease_status gender ## 1 78 FALSE M ## 2 68 FALSE F ## 3 95 TRUE &lt;NA&gt; patient.data[-1, ] # removes first row ## name heart_rate disease_status gender ## 2 rashmi 68 FALSE F ## 3 kiran 95 TRUE &lt;NA&gt; 2.5.4 Handling datatypes in dataframes One of the great strengths of a dataframe is that it can handle each column containing different datatypes. Our patient data has columns of character strings, logicals, and numerical values. However, you should take care that sometimes unexpected behavior may arise when a column in your dataframe is one datatype and you add data that is not compatible with that datatype. Remember that vectors can only contain one datatype at a time? Each column in the dataframe is essentially a vector. We briefly discussed factors as categorical variables. Let’s pretend that for our analysis we wanted to treat gender as a categorical variable. Factors are a special datatype that deals with categorical data and can be handy for certain manipulations or visualizations. To do this, we can coerce data into a different datatype using functions like as.function. head(all.patients$gender) ## [1] M F &lt;NA&gt; &lt;NA&gt; M F ## Levels: F M all.patients$gender &lt;- as.factor(all.patients$gender) head(all.patients$gender) ## [1] M F &lt;NA&gt; &lt;NA&gt; M F ## Levels: F M This vector of factors shows which category each element belongs to, and then summarizes what the possible categories are down at the bottom where it prints the Levels of this factor. This can cause issues if we introduce data that does not match these categories. Let’s try adding a new patient’s data. all.patients &lt;- rbind(all.patients, c(name = &quot;lupe&quot;, heart_rate = 72, disease_status = FALSE, gender = &quot;NB&quot;)) ## Warning in `[&lt;-.factor`(`*tmp*`, ri, value = &quot;lupe&quot;): invalid factor level, NA ## generated ## Warning in `[&lt;-.factor`(`*tmp*`, ri, value = &quot;lupe&quot;): invalid factor level, NA ## generated all.patients ## name heart_rate disease_status gender ## 1 oakley 78 FALSE M ## 2 rashmi 68 FALSE F ## 3 kiran 95 TRUE &lt;NA&gt; ## 4 eun 86 TRUE &lt;NA&gt; ## 5 sasha 79 TRUE M ## 6 mattie 64 FALSE F ## 7 &lt;NA&gt; 72 FALSE &lt;NA&gt; What has happened with this new addition? If you tried to add a new patient to the dataframe that had a gender that wasn’t already represented in the data, chances are you had a warning and that gender was turned to NA. all.patients$gender ## [1] M F &lt;NA&gt; &lt;NA&gt; M F &lt;NA&gt; ## Levels: F M Once a factor is created, it doesn’t let you easily add new categories that were not in the original set. We will not get issues though if we add a new patient whose gender is represented as one of the levels in our gender factor. all.patients &lt;- rbind(all.patients, c(name = &quot;chihiro&quot;, heart_rate = 101, disease_status = TRUE, gender = &quot;M&quot;)) ## Warning in `[&lt;-.factor`(`*tmp*`, ri, value = &quot;chihiro&quot;): invalid factor level, ## NA generated all.patients$gender ## [1] M F &lt;NA&gt; &lt;NA&gt; M F &lt;NA&gt; M ## Levels: F M So how do we fix this? The easiest way around this is to treat the column as characters instead of as factors. all.patients$gender &lt;- as.character(all.patients$gender) all.patients &lt;- rbind(all.patients, c(name = &quot;ayodele&quot;, heart_rate = 101, disease_status = TRUE, gender = &quot;NB&quot;)) ## Warning in `[&lt;-.factor`(`*tmp*`, ri, value = &quot;ayodele&quot;): invalid factor level, ## NA generated all.patients$gender ## [1] &quot;M&quot; &quot;F&quot; NA NA &quot;M&quot; &quot;F&quot; NA &quot;M&quot; &quot;NB&quot; There will be times that we want to treat certain columns in our data as a factor, but take care that you add compatible data to each column of your existing dataset. "],["day-4-tidyverse-and-visualizations.html", "3 Day 4 - tidyverse and visualizations", " 3 Day 4 - tidyverse and visualizations Learning Goals: By the end of today’s session, students will be able to: Read in and process data starting from a local saved file Transform, scale, filter, and convert values within a dataset Descibe the roles of data, aesthetics, and geoms in ggplot functions. Choose the correct aesthetics and alter the geom parameters for a scatter plot, histogram, or box plot. Layer multiple geometries in a single plot. Customize plot scales, titles, subtitles, themes, fonts, layout, and orientation. Apply a facet to a plot. Save a ggplot to a file. Preparing for Today: Please make sure you have completed all readings and activities from Section 2: Day 3 - introduction to R. You will also have some time at the start of the day to do the first sections 3.1-3.3. Today’s Schedule: Start Time End Time Activity Description 10:00 AM PDT 10:45 AM PDT Asynchronous Work through sections 3.1-3.3 10:45 AM PDT 11:30 AM PDT Synchronous Peer coding exercises on Zoom 11:30 AM PDT 12:15 PM PDT Asynchronous Work through sections 3.4-3.5 12:15 PM PDT 12:45 PM PDT Lunch Break Take a computer break! 12:45 PM PDT 1:30 PM PDT Synchronous Peer coding exercises on Zoom 1:30 PM PDT 2:00 PM PDT Asynchronous Work through sections 3.6 2:00 PM PDT 2:30 PM PDT Snack Break Take a computer break! 2:30 PM PDT 3:30 PM PDT Synchronous Wrap-up session on Zoom "],["reading-and-processing-data.html", "3.1 Reading and processing data", " 3.1 Reading and processing data So far, we have largely used data that was already provided to us in base R or through the tidyverse packages. In reality, you will be working with data that comes from a file containing measurements collected by a researcher. 3.1.1 Loading a file into R There are multiple file types that can be read into R. The most common one is the csv file extension, though txt files also work. You can read in Excel files using additional packages such as the readxl library. Let’s practice reading in data using the read.csv function. ?read.csv The repository for this site has a folder called data that contains a file called iris.csv, which is simply the original iris dataset with some modifications. We read in this data and save it to a variable named irisz. irisz &lt;- read.csv(&quot;data/iris.csv&quot;) head(irisz) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species Location ## 1 5.1 3.5 1.4 0.2 setosa Korea ## 2 4.9 3 1.4 0.2 setosa China ## 3 4.7 3.2 1.3 0.2 setosa Korea ## 4 4.6 3.1 1.5 0.2 setosa China ## 5 5 3.6 1.4 0.2 setosa China ## 6 5.4 1.7 0.4 setosa Canada You can already see there are some differences compared to the original iris dataset, such as missing data and an additional column of information. You can see there is also a difference in how certain columns are handled. head(iris$Species) ## [1] setosa setosa setosa setosa setosa setosa ## Levels: setosa versicolor virginica head(irisz$Species) ## [1] setosa setosa setosa setosa setosa setosa ## Levels: setosa versicolor virginica For example, here it is clear that our data we read from a csv file is not treating species as categorical information. We can toggle a setting called stringsAsFactors that will turn these character string inputs into factors when the file is loaded into R. irisz &lt;- read.csv(&quot;data/iris.csv&quot;, stringsAsFactors = TRUE) head(irisz$Species) ## [1] setosa setosa setosa setosa setosa setosa ## Levels: setosa versicolor virginica head(irisz$Location) ## [1] Korea China Korea China China Canada ## Levels: Canada China Japan Korea Russia USA If we did not want to reload our data from a file, we can also transform columns (i.e. vectors) in our dataframe to be different types. For example, we could use irisz$Species &lt;- as.factor(irisz$Species) to turn it into a factor after it has already been loaded. There are similar functions like as.numeric, as.character, and as.logical that you may use to transform data types. Be aware though that certain conversions may lead to issues and loss of data! Now both of these columns are being treated as categorical data. There are other settings we can change, such as providing different names for the columns. irisz &lt;- read.csv(&quot;data/iris.csv&quot;, col.names = c(&quot;sep_len&quot;, &quot;sep_wid&quot;, &quot;pet_len&quot;, &quot;pet_wid&quot;, &quot;species&quot;, &quot;loc&quot;)) head(irisz) ## sep_len sep_wid pet_len pet_wid species loc ## 1 5.1 3.5 1.4 0.2 setosa Korea ## 2 4.9 3 1.4 0.2 setosa China ## 3 4.7 3.2 1.3 0.2 setosa Korea ## 4 4.6 3.1 1.5 0.2 setosa China ## 5 5 3.6 1.4 0.2 setosa China ## 6 5.4 1.7 0.4 setosa Canada 3.1.2 Handling issues in data Now that we have loaded in the data, we should preview it and look for any possible issues. You never know what can go wrong in the data pipeline: data can be recorded incorrectly or corrupted at any point in the process. str(irisz) ## &#39;data.frame&#39;: 150 obs. of 6 variables: ## $ sep_len: Factor w/ 36 levels &quot; &quot;,&quot;4.3&quot;,&quot;4.4&quot;,..: 10 8 6 5 9 13 5 9 3 8 ... ## $ sep_wid: Factor w/ 27 levels &quot;&quot;,&quot; &quot;,&quot;0.35&quot;,..: 18 13 15 14 19 2 17 17 12 14 ... ## $ pet_len: Factor w/ 46 levels &quot;&quot;,&quot; &quot;,&quot;1&quot;,&quot;1.1&quot;,..: 7 7 6 8 7 10 7 8 7 8 ... ## $ pet_wid: Factor w/ 25 levels &quot;&quot;,&quot;0.1&quot;,&quot;0.2&quot;,..: 3 3 3 3 3 5 4 3 3 2 ... ## $ species: Factor w/ 4 levels &quot;&quot;,&quot;setosa&quot;,&quot;versicolor&quot;,..: 2 2 2 2 2 2 2 1 2 2 ... ## $ loc : Factor w/ 7 levels &quot;&quot;,&quot;Canada&quot;,&quot;China&quot;,..: 5 3 5 3 3 2 3 3 6 4 ... summary(irisz) ## sep_len sep_wid pet_len pet_wid species ## 5 : 9 3 :26 1.4 :13 0.2 :29 : 2 ## 5.1 : 9 2.8 :14 1.5 :13 1.3 :13 setosa :49 ## 6.3 : 9 3.2 :13 4.5 : 8 1.5 :12 versicolor:50 ## 5.7 : 8 3.4 :12 5.1 : 8 1.8 :12 virginica :49 ## 6.7 : 8 3.1 :11 1.3 : 7 1.4 : 8 ## 5.5 : 7 2.7 : 9 1.6 : 6 0.3 : 7 ## (Other):100 (Other):65 (Other):95 (Other):69 ## loc ## :14 ## Canada:35 ## China : 6 ## Japan :11 ## Korea :10 ## Russia:11 ## USA :63 One immediate issue you can spot here is that all variables are being treated as character strings. We would expect that sepal length/width and petal length/width would be numeric variables. To inspect what might be happening, we can look at the unique elements in one of these columns using the unique function. unique(irisz$sep_len) ## [1] 5.1 4.9 4.7 4.6 5 5.4 4.4 4.8 4.3 5.8 5.7 5.2 5.5 4.5 n/a 5.3 7 6.4 6.9 ## [20] 6.5 6.3 6.6 5.9 6 6.1 5.6 6.7 6.2 6.8 7.1 7.6 7.2 7.7 7.4 7.9 ## 36 Levels: 4.3 4.4 4.5 4.6 4.7 4.8 4.9 5 5.1 5.2 5.3 5.4 5.5 5.6 5.7 ... n/a Looking at this, we can find the culprit for why these values are treated as a string. There are some entries that are simply a blank space \"\" and there are missing data encoded as \"n/a\" in this dataset. The read.csv function has a nice way to deal with this, specifically the argument na.strings that takes options for elements that might be present in the loaded data that should be treated as an NA. irisz &lt;- read.csv(&quot;data/iris.csv&quot;, na.strings = c(&quot;&quot;, &quot;n/a&quot;), stringsAsFactors = TRUE) str(irisz) ## &#39;data.frame&#39;: 150 obs. of 6 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 NA 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 NA 1 1 ... ## $ Location : Factor w/ 6 levels &quot;Canada&quot;,&quot;China&quot;,..: 4 2 4 2 2 1 2 2 5 3 ... summary(irisz) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :0.350 Min. :1.000 Min. : 0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.550 1st Qu.: 0.300 ## Median :5.800 Median :3.000 Median :4.300 Median : 1.300 ## Mean :5.839 Mean :3.028 Mean :3.752 Mean : 1.331 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.: 1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :23.000 ## NA&#39;s :2 NA&#39;s :3 NA&#39;s :3 NA&#39;s :2 ## Species Location ## setosa :49 Canada:35 ## versicolor:50 China : 6 ## virginica :49 Japan :11 ## NA&#39;s : 2 Korea :10 ## Russia:11 ## USA :63 ## NA&#39;s :14 Now we can see that the first columns are being properly treated as numeric vectors. Our next step would be to double-check our data for anything that looks anamolous. Let’s first take a look at the distributions of our numerical data using the base R hist function, which plots a simple histogram. hist(irisz$Sepal.Length) hist(irisz$Sepal.Width) From these figures, we can spot that there is one very low value for sepal width that we may want to investigate. hist(irisz$Petal.Length) hist(irisz$Petal.Width) From these figures, we see there is a very high value for petal width that we may want to investigate. It is possible that these values somehow got changed or corrupted during the recording process. We can filter out these anomalous pieces of data using conditional logic that we learned about when indexing and subsetting into a vector. Let’s start by filtering out rows that have a petal width more than 5. dim(irisz) ## [1] 150 6 irisz &lt;- irisz[irisz$Petal.Width &lt; 5, ] dim(irisz) ## [1] 149 6 Note that we used the statement irisz$Petal.Width &lt; 5 that evaluates as TRUE or FALSE for every row. We then indexed into the irisz dataframe using brackets [] and selected all rows that evaluate as TRUE. This led to one row being removed. We can repeat this with the anamolous value for sepal width. dim(irisz) ## [1] 149 6 irisz &lt;- irisz[irisz$Sepal.Width &gt; 1, ] dim(irisz) ## [1] 148 6 Now that we have removed observations that looked strange by eye inspection, we may have only some final pre-processing steps to do. Depending on what downstream analysis we want to conduct, we may want to standardize or normalize our numeric values. R comes with an existing scale function that we can use to standardize between values with different ranges and orders of magnitude. ?scale The scale function will by default scale and center a numeric vector, but you can choose to do only one of these steps using argument settings. x &lt;- 1:10 scale(x) ## [,1] ## [1,] -1.4863011 ## [2,] -1.1560120 ## [3,] -0.8257228 ## [4,] -0.4954337 ## [5,] -0.1651446 ## [6,] 0.1651446 ## [7,] 0.4954337 ## [8,] 0.8257228 ## [9,] 1.1560120 ## [10,] 1.4863011 ## attr(,&quot;scaled:center&quot;) ## [1] 5.5 ## attr(,&quot;scaled:scale&quot;) ## [1] 3.02765 scale(x, center = FALSE) ## [,1] ## [1,] 0.1528942 ## [2,] 0.3057883 ## [3,] 0.4586825 ## [4,] 0.6115766 ## [5,] 0.7644708 ## [6,] 0.9173649 ## [7,] 1.0702591 ## [8,] 1.2231533 ## [9,] 1.3760474 ## [10,] 1.5289416 ## attr(,&quot;scaled:scale&quot;) ## [1] 6.540472 scale(x, scale = FALSE) ## [,1] ## [1,] -4.5 ## [2,] -3.5 ## [3,] -2.5 ## [4,] -1.5 ## [5,] -0.5 ## [6,] 0.5 ## [7,] 1.5 ## [8,] 2.5 ## [9,] 3.5 ## [10,] 4.5 ## attr(,&quot;scaled:center&quot;) ## [1] 5.5 We can scale the columns in our dataset and create a more standardized range of values for each column. # before scaling hist(irisz$Sepal.Length) hist(irisz$Petal.Width) # after scaling hist(scale(irisz$Sepal.Length)) hist(scale(irisz$Petal.Width)) The shapes of our distributions for these measurements is not fundamentally different, but if you look along the x-axis, you will notice that the values (especially the highest and lowest) are more similar between these two columns after they are both scaled. You may also consider rescaling values or normalizing them to all be between 0 and 1. There is a function for this called rescale that comes with the scales library. library(scales) rescale(x) ## [1] 0.0000000 0.1111111 0.2222222 0.3333333 0.4444444 0.5555556 0.6666667 ## [8] 0.7777778 0.8888889 1.0000000 "],["using-tidyverse.html", "3.2 Using tidyverse", " 3.2 Using tidyverse 3.2.1 Goals to be reworked alongside content When it comes to manipulating data frames (creating new columns, filtering, etc.) - tidyverse is your friend! Scholars will understand the complex socio-technical context of tidyverse, ie that it’s not base R but oh boy it’s popular and well supported by a company - but not base R Scholars know that tidyverse/dplyr takes a more database-like approach to moving data tables around Scholars can identify and predict what a magrittr pipe does Scholars can pipe some stuff between functions, and write that out in a script Scholars can filter data tables, select and rename columns Scholars can mutate columns Scholars understand what group_by is doing Scholars can use group_by to summarise variables 3.2.2 Welcome to the tidyverse! Tidyverse is a suite of R packages that are very useful for data science, when we need to manipulate data frames and visualize data. The most important packages in tidyverse for our workshop will be dplyr and ggplot2. It builds on the base R functions and data types we’ve studied so far. It just provides a different design framework for working with data in R. A main difference is that instead of calling functions around an object, we will “pipe” objects into functions, and can easily string functions together in this way. For example, if you have a data frame, df, instead of peaking at it with head(df), you can pipe it into head() like this: df %&gt;% head(). This might seem like more work for this specific operation, but when you need to run a bunch of functions on a data frame, it becomes much TIDIER! Without worrying about the syntax, check out this more complex example. We have a dataframe, df, with measumerments of flowers. We’re interested in filtering the data for flower sizes over 10cm, then creating a new variable that is the size in mm, then finding the average size of a type of flower. df %&gt;% filter(size_cm &lt; 10) %&gt;% mutate(size_mm = size_cm * 10) %&gt;% group_by(flower_type) %&gt;% summarize(mean_size = mean(value_millions)) %&gt;% ungroup() In addition to piping with %&gt;%, the tidyverse has many handy functions to manipulate data frames. The ones shown above allow us to: Filter data using filter() Create new variables with mutate() Group variables with group_by() Create a new variable over a group using summarize() 3.2.3 Let’s get started with Tidyverse Download tidyverse Remember we only have to install once install.packages(&quot;tidyverse&quot;) Load the tidyverse library library(tidyverse) ## ── Attaching packages ───────────────────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.3.2 ✔ purrr 0.3.4 ## ✔ tibble 3.0.4 ✔ dplyr 1.0.2 ## ✔ tidyr 1.1.2 ✔ stringr 1.4.0 ## ✔ readr 1.4.0 ✔ forcats 0.5.0 ## ── Conflicts ──────────────────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() 3.2.4 Try tidyverse on the dataset we loaded in the previous section # Load the dataset and peak at it with the head() function irisz &lt;- read.csv(&quot;data/iris.csv&quot;) head(irisz) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species Location ## 1 5.1 3.5 1.4 0.2 setosa Korea ## 2 4.9 3 1.4 0.2 setosa China ## 3 4.7 3.2 1.3 0.2 setosa Korea ## 4 4.6 3.1 1.5 0.2 setosa China ## 5 5 3.6 1.4 0.2 setosa China ## 6 5.4 1.7 0.4 setosa Canada # Now try it with tidyverse piping library(tidyverse) ## ── Attaching packages ───────────────────────────────────────────────────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.3.2 ✔ purrr 0.3.4 ## ✔ tibble 3.0.4 ✔ dplyr 1.0.2 ## ✔ tidyr 1.1.2 ✔ stringr 1.4.0 ## ✔ readr 1.4.0 ✔ forcats 0.5.0 ## ── Conflicts ──────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() df &lt;- irisz df %&gt;% head() ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species Location ## 1 5.1 3.5 1.4 0.2 setosa Korea ## 2 4.9 3 1.4 0.2 setosa China ## 3 4.7 3.2 1.3 0.2 setosa Korea ## 4 4.6 3.1 1.5 0.2 setosa China ## 5 5 3.6 1.4 0.2 setosa China ## 6 5.4 1.7 0.4 setosa Canada # Let&#39;s pipe the data frame into a summary() function # to get summary statistics # In base R, we would do # summary(df) # In tidyverse we do it this way: df %&gt;% summary() ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 5 : 9 3 :26 1.4 :13 0.2 :29 : 2 ## 5.1 : 9 2.8 :14 1.5 :13 1.3 :13 setosa :49 ## 6.3 : 9 3.2 :13 4.5 : 8 1.5 :12 versicolor:50 ## 5.7 : 8 3.4 :12 5.1 : 8 1.8 :12 virginica :49 ## 6.7 : 8 3.1 :11 1.3 : 7 1.4 : 8 ## 5.5 : 7 2.7 : 9 1.6 : 6 0.3 : 7 ## (Other):100 (Other):65 (Other):95 (Other):69 ## Location ## :14 ## Canada:35 ## China : 6 ## Japan :11 ## Korea :10 ## Russia:11 ## USA :63 "],["using-tidyverse-1.html", "3.3 Using tidyverse", " 3.3 Using tidyverse When it comes to manipulating data frames (creating new columns, filtering, etc.) - tidyverse is your friend! Scholars will understand the complex socio-technical context of tidyverse, ie that it’s not base R but oh boy it’s popular and well supported by a company - but not base R Scholars know that tidyverse/dplyr takes a more database-like approach to moving data tables around Scholars can identify and predict what a magrittr pipe does Scholars can pipe some stuff between functions, and write that out in a script Scholars can filter data tables, select and rename columns Scholars can mutate columns Scholars understand what group_by is doing Scholars can use group_by to summarise variables # ? "],["making-plots-with-ggplot2.html", "3.4 Making plots with ggplot2", " 3.4 Making plots with ggplot2 We will primarily be working in ggplot2 as it has the greatest degree of customization for visualization and offers many additional features over the basic plotting in R. library(ggplot2) library(tidyverse) 3.4.1 Getting started with a ggplot Most ggplot calls to create a figure take the following form (you can read more using help(ggplot)): ggplot(data = &lt;DATA&gt;, mapping = aes(&lt;MAPPINGS&gt;)) + &lt;GEOM_FUNCTION&gt;() We will practice using our mammalian sleep dataset. You can look up more info about this dataset using the help function and the dataset name, msleep. head(msleep) ## # A tibble: 6 x 11 ## name genus vore order conservation sleep_total sleep_rem sleep_cycle awake ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Chee… Acin… carni Carn… lc 12.1 NA NA 11.9 ## 2 Owl … Aotus omni Prim… &lt;NA&gt; 17 1.8 NA 7 ## 3 Moun… Aplo… herbi Rode… nt 14.4 2.4 NA 9.6 ## 4 Grea… Blar… omni Sori… lc 14.9 2.3 0.133 9.1 ## 5 Cow Bos herbi Arti… domesticated 4 0.7 0.667 20 ## 6 Thre… Brad… herbi Pilo… &lt;NA&gt; 14.4 2.2 0.767 9.6 ## # … with 2 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt; You will first use the ggplot() function and bind the plot to a specific data frame using the data argument. ggplot(data = msleep) You will next need to define a mapping (using the aesthetic or aes function), by selecting the variables to be plotted and specifying how to present them in the graph, e.g. as x/y positions or characteristics such as size, shape, color, etc. ggplot(data = msleep, aes(x = brainwt, y = sleep_rem)) You can then add “geoms” or graphical representations of the data in the plot (points, lines, bars). ggplot2 offers many different geoms including: geom_point() for scatter plots, dot plots, etc. geom_boxplot() for, well, boxplots! geom_line() for trend lines, time series, etc. To add a geom to the plot use the + operator. To plot using two continuous variables, we will use geom_point() first. To save your work-in-progress, you can assign the plot to a variable. First we establish our coordinate system. my.plot &lt;- ggplot(data = msleep, aes(x = brainwt, y = sleep_rem)) We can now draw the plot as a scatterplot with points to represent each mammal’s measurements from the msleep dataset. my.plot + geom_point() ## Warning: Removed 35 rows containing missing values (geom_point). You might notice that all of the points are squished against the y-axis since many of the mammals in this dataset have low brain weights. summary(msleep$brainwt) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.00014 0.00290 0.01240 0.28158 0.12550 5.71200 27 As you can see with the summary function, the minimum and median values are very low, but there are a few mammals with high brainwt as you can see by the much larger maximum value in this vector. To make more useful plots, we can transform this value using log-scaling. While we will have to note that the new values do not exactly match the real-world measurements anymore, patterns we see (i.e. something that correlates with higher brain weights) will still hold true. msleep2 &lt;- msleep %&gt;% mutate(brainwt_log = log(brainwt)) ggplot(msleep2, aes(x = brainwt_log, y = sleep_rem)) + geom_point() ## Warning: Removed 35 rows containing missing values (geom_point). Here we use the mutate function to make a new variable called brainwt_log in our dataset (technically a new dataset copy that we have saved as msleep2). Plotting this variable as our x variable (i.e. independent variable), makes it easier to look for patterns. 3.4.2 Changing plot aesthetics We can modify the appearance of the plot in two ways: either uniformly changing the appearance or having the appearance vary depending on information present in our data. Let’s explore how to modify our plots uniformly. We can change aspects of the points we plot such as transparency (“alpha”) and color by supplying them as arguments in the geom_point function. ggplot(data = msleep2, aes(x = brainwt_log, y = sleep_rem)) + geom_point(alpha = 0.5, color = &quot;blue&quot;) ## Warning: Removed 35 rows containing missing values (geom_point). Here we have made the plots semi-transparent and colored blue. You can try varying these values (e.g. change blue to a different color). You can also supply other arguments such as shape to use something other than a dot. However, it is also possible to scale the color of the points by some variable present in the data. This approach means we can create a scatterplot that conveys more than just two variables’ worth of information (x-axis and y-axis) by having the color reflect a third variable. To do this, we specify the color inside the aesthetic mapping aes within the initial ggplot function. Same as how we told R to use a specific column by name for x or y coordinates, we specify which column to use for color. ggplot(msleep2, aes(x = brainwt_log, y = sleep_rem, color = vore)) + geom_point() ## Warning: Removed 35 rows containing missing values (geom_point). This plot conveys not only the relationship between brainwt_log and sleep_rem, but each plot representing a different mammal now conveys what the feeding behavior of that mammal is. When generating visualizations, it is important to annotate the figure with meaningful labels on the axes to make them accessible for the viewer. For that, we can use the labs function. ggplot(msleep2, aes(x = brainwt_log, y = sleep_rem, color = vore)) + geom_point() + labs(x = &quot;Brain Weight (log)&quot;, y = &quot;Duration of REM Sleep&quot;, color = &quot;Feeding Behavior&quot;) ## Warning: Removed 35 rows containing missing values (geom_point). 3.4.3 Exploring simple plots Let’s consider how we make other plots besides a scatterplot. Scatterplots are a great way to look at two quantitative (numerical) values at the same time to observe patterns (i.e. correlations) between the variables or to identify interesting outliers. However, other plots may be more useful to look at differing numbers of variables (i.e. one quantitative variable) or different types of variables (i.e. qualitative or categorical data). Here, we discuss two types of single variable plots that look at either a quantitative variable (histogram) or a categorical variable (barplot). We can create histograms in ggplot2 that are more aesthetically pleasing than the default hist function. This shows the distribution of one quantitative variable. ggplot(msleep, aes(x = sleep_total)) + geom_histogram(bins = 10) We can look at how many individuals in the dataset fall into each category, such as how many mammals have each kind of feeding behavior. ggplot(data = msleep, aes(x = vore)) + geom_bar() As you can see where we map the aesthetic, we only tell the ggplot function to refer to a single column in our dataset for our x-axis. 3.4.4 Visualizing between groups Let’s return to iris dataset to explore how we can visualize differences between groups/categories. These groups are often represented in our data as a factor. We can look at how the distributions of Sepal.Length differ depending on which species each iris belongs to. One plot that can do this easily is the geom_boxplot function. ggplot(data = iris, aes(x = Species, y = Sepal.Length)) + geom_boxplot() By adding a different parameter to fill in the aes we define throgh the ggplot function, we can separate out histograms according to different groupings. Here, we use Species to determine the color of the fill. ggplot(iris, aes(x = Sepal.Length, fill = Species)) + geom_histogram(bins = 10) While we can sort of see the trends on this plot, it may be helpful to separate out each histogram for each individual species. There is an easy way to do this in ggplot2 using facetting or the facet_wrap function. This function splits the figure into separate panel where the data has been filtered by the category (i.e. Species). ggplot(iris, aes(x = Sepal.Length, fill = Species)) + geom_histogram(bins = 10) + facet_wrap( ~ Species) This matches what we already saw in the boxplot, showing that there are different sepal lengths depending on which iris species we look at. We will explore in the next section how we know if these differences are significant. 3.4.5 Generating heatmaps Heatmaps are a useful way to show the values of multiple samples across many measurements. You can visualize a heatmap by thinking of your dataframe, this tabular data, if it had each cell colorcoded based on how high or low the value is. The base R heatmap function meets many needs while the ggplot2 equivalent (geom_tile) can be confusing so we will recommend that you not use ggplot2 for heatmaps. Let’s go back to msleep dataset to visualize trends between the different measurements taken on each mammal. We will first create a simplified dataset from msleep where we take the log value of both brainwt and bodywt. temp.data &lt;- cbind(log(msleep$brainwt), log(msleep$bodywt)) head(temp.data) ## [,1] [,2] ## [1,] NA 3.9120230 ## [2,] -4.1669153 -0.7339692 ## [3,] NA 0.3001046 ## [4,] -8.1456296 -3.9633163 ## [5,] -0.8603831 6.3969297 ## [6,] NA 1.3480731 Next, we feed these numeric values into the heatmap function along with some arguments that specify settings for displaying the figure. We use labCol to specify how to label these columns and cexCol to control the text size of these labels. We set labRow to be the names of each species from the msleep dataset. heatmap(temp.data, labRow = msleep$name, labCol = c(&quot;brainwt&quot;, &quot;bodywt&quot;), cexCol = 1) This first heatmap will look strange because it colors each box by its magnitude, but body weight of a mammal is always greater than its brain weight. We want to scale within each column so that the depth of the color reflects whether the mammal has a high brain weight or high body weight relative to the other mammals. heatmap(temp.data, scale = &quot;col&quot;, labRow = msleep$name, labCol = c(&quot;brainwt&quot;, &quot;bodywt&quot;), cexCol = 1) Using the scale argument which we set to \"col\", now the color of the columns is more meaningful. For example, it makes sense that the measurements taken on an Asian elephant are much higher than those from a mole rat, so the color of those cells is deeper. Let’s add some more data to our heatmap visualization. temp.data2 &lt;- cbind(temp.data, msleep$sleep_total, msleep$sleep_rem) heatmap(temp.data2, scale = &quot;col&quot;, labRow = msleep$name, labCol = c(&quot;brainwt&quot;, &quot;bodywt&quot;, &quot;total_sleep&quot;, &quot;rem_sleep&quot;), cexCol = 1) You may have noticed the weird diagrams along the top and left hand side of this heatmap. These strange line diagrams are trees that show how our samples cluster together. Mammals that have similar patterns of values across these four measurements are placed near each other in the diagram. "],["making-scientific-figures.html", "3.5 Making scientific figures", " 3.5 Making scientific figures Making plots can be a great way to develop an intuition for your dataset, though to derive and communicate scientific insights, we need to have an idea of the uncertainty in our interpretations. Uncertainty describes ideas such as: are the values between two groups different enough, that it is unlikely that the differences are due to chance? Is the correlation between these variables strong enough that one can predict the other, with some level of confidence? How statistically significant are the patterns we see? 3.5.1 Plotting error bars When we compare measurements taken from two samples (i.e. two groups), we might want to see if the two groups have very different values for that specific measurement. If we have multiple observations within each group, we can take a summary statistic such as the mean or median and plot those against each other. ggplot(msleep2, aes(x = vore, y = awake)) + geom_bar(stat = &quot;summary&quot;, fun = &quot;mean&quot;) For example, here we have asked our geom_bar function to plot a summary, specifically the mean of each group, instead of plotting identity which usually means the value as is. Looking at this figure, we can’t guess if the groups are significantly different without an idea of the uncertainty in our measurements through something like error bars. Here is the convention for plotting error bars in ggplot2, as you can see it is just another kind of geom that we can add to our plot: ggplot(data = &lt;SUMMARY DATA&gt;, mapping = aes(&lt;SUMMARY MAPPINGS&gt;) + geom_bar(stat = \"identity\") + geom_errorbar(aes(&lt;ERROR MAPPINGS&gt;)) This method is straightforward, but you need to have pre-calculated the summary statistic for each group and the amount of error (i.e. standard error) from your data. That “aggregated” dataframe becomes the data that you provide to ggplot, instead of the original dataset. feeding.data &lt;- msleep2 %&gt;% group_by(vore) %&gt;% summarize(mean_se(awake)) ## `summarise()` ungrouping output (override with `.groups` argument) feeding.data ## # A tibble: 5 x 4 ## vore y ymin ymax ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 carni 13.6 12.6 14.7 ## 2 herbi 14.5 13.6 15.4 ## 3 insecti 9.06 6.41 11.7 ## 4 omni 13.1 12.4 13.7 ## 5 &lt;NA&gt; 13.8 12.7 14.9 What does mean_se do? We can check. ?mean_se This function returns three values, y, ymin, and ymax which correspond to the mean, the mean minus one standard error, and the mean plus one standard error. The mean value will be the height of each bar on the barplot, while ymin corresponds to the bottom of the error bar and ymax to the top of the error bar. Let’s first plot the height of the bars using this new feeding.data dataset and mapping y to our new y column generated from the mean_se function. Here we create the same plot as before from this aggregated dataset, just showing the mean value in each group. my.plot &lt;- ggplot(feeding.data, aes(x = vore, y = y)) + geom_bar(stat = &quot;identity&quot;) my.plot Now we add the error bars, mapping the ymin and ymax values to the arguments that happen to have the same name in the geom_errorbar function. We add in the width setting just for aesthetics. my.plot &lt;- my.plot + geom_errorbar(aes(ymin = ymin, ymax = ymax), width = 0.2) my.plot Now it is more clear that insectivores have significantly less time awake compared to other kinds of mammals. 3.5.2 Showing trends in data Lots of these different figures summarize or aggregate the data. We may want to display the data with the individual points, but still show the overall trend across the data. Some good plots that do this are geom_density_2d which provides a contour plot. Let’s check out the trend in our msleep data between body weight and brain weight. We will create a new column where we take the log value of body weight, like we did with brain weight. msleep3 &lt;- msleep2 %&gt;% mutate(bodywt_log = log(bodywt)) ggplot(data = msleep3, mapping = aes(x = brainwt_log, y = bodywt_log)) + geom_density_2d() + geom_point() As you can see the density of points almost look like they fit a line. As brain weight increases then body weight increases, or vice versa. We can add a trendline to this plot with the geom_smooth function. ggplot(msleep3, aes(x = brainwt_log, y = bodywt_log)) + geom_point(alpha = 0.5) + geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; We can also do this with trendlines that summarize only certain subsets of the data, such as those that belong to a specific category. We will flip back to our iris dataset to look at how sepal length compares to sepal width between different species of iris. ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) + geom_point() + geom_smooth(aes(color = Species)) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; We create a seaprate trendline for each by specifiying the aes with color mapping to Species within the geom_smooth function. The above plot may make it hard to see the data that is contributing to each trend line. Using facet_wrap again, we can split the figure into separate panel where the data has been filtered by the category (i.e. species). ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) + geom_point() + geom_smooth(aes(color = Species)) + facet_wrap( ~ Species) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Now it is clear that there is a positive correlation (as one goes up, the other goes up) in each species of iris, but some species have a sharper upward trend than others. 3.5.3 Saving figures locally As you produce analysis in your research, you may want to create high-quality images of your figures to then use in presentations or publications. There are two easy ways to save images as an individual file on your computer, The first method uses ggsave to save the most recent ggplot figure you generated. ggplot(msleep2, aes(x = brainwt_log, y = sleep_rem)) + geom_point() + geom_smooth(method = &quot;lm&quot;) ggsave(&quot;plot.png&quot;, width = 5, height = 5) This function will save the figure just produced by this code wherever your directory is currently. You can check your current directory with getwd() and change with setwd(&lt;FOLDER NAME&gt;). You can also provide a precise file path in the new file name. ggsave(&quot;~/Downloads/plot.png&quot;, width = 5, height = 5) Here is an alternative method for saving your figures: pdf(&quot;plot.pdf&quot;) # creates the file # png() also works if you want a different file format ggplot(msleep2, aes(x = brainwt_log, y = sleep_rem)) + geom_point() + geom_smooth(method = &quot;lm&quot;) dev.off() # finishes editing the file Any changes to the figure that are contained between the initial creation of the figure (i.e. the pdf command) and the dev.off command will be included in the final saved image. However, the figure is being printed directly to the file it is writing and won’t appear elsewhere. "],["applying-basic-stats.html", "3.6 Applying basic stats", " 3.6 Applying basic stats Following up on our analysis using visualizations, we will review different types of statistical tests that we can perform on data. We will again revisit the mammalian sleep dataset msleep. head(msleep) ## # A tibble: 6 x 11 ## name genus vore order conservation sleep_total sleep_rem sleep_cycle awake ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Chee… Acin… carni Carn… lc 12.1 NA NA 11.9 ## 2 Owl … Aotus omni Prim… &lt;NA&gt; 17 1.8 NA 7 ## 3 Moun… Aplo… herbi Rode… nt 14.4 2.4 NA 9.6 ## 4 Grea… Blar… omni Sori… lc 14.9 2.3 0.133 9.1 ## 5 Cow Bos herbi Arti… domesticated 4 0.7 0.667 20 ## 6 Thre… Brad… herbi Pilo… &lt;NA&gt; 14.4 2.2 0.767 9.6 ## # … with 2 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt; 3.6.1 Describing quantitative variables There are several ways to describe quantitative measurements. We might first look at the range of values using the quantile function, which returns the min, max, and median values as well as the 25th and 75th percentiles. quantile(msleep$brainwt, na.rm = TRUE) ## 0% 25% 50% 75% 100% ## 0.00014 0.00290 0.01240 0.12550 5.71200 Note that we are using the setting na.rm = TRUE as an argument to these functions so that it ignores NA values in the data. You can also directly isolate some of these values using the functions min or max. We might describe the center of our data, that is the mean or the median values using those respective function names. median(msleep$brainwt, na.rm = TRUE) ## [1] 0.0124 mean(msleep$brainwt, na.rm = TRUE) ## [1] 0.2815814 We might also want to describe the spread in the data, such as the standard deviation or the interquartile range. sd(msleep$brainwt, na.rm = TRUE) ## [1] 0.9764137 IQR(msleep$brainwt, na.rm = TRUE) ## [1] 0.1226 Visualizations are a great way to see this information in one picture. We have learned to create a fancier histogram, but the hist function always works for quick viewing. hist(msleep$brainwt) It is clear from this image that the data is skewed towards approaching 0 with a few high outliers. 3.6.2 Finding differences in data For the following statistical tests, we want to determine if measurements of a quantitative variable taken on a certain group of samples are different from similar measurements taken from a different group of samples. To know if these differences are statistically significant, we need to be aware of the uncertainty in our data. Even if we have collect many observations, there is generally noisiness and error in those measurements. Thus, we have uncertainty if the mean value we calculate for a group is in fact the true mean. We can calculate a confidence interval to describe our guess for the mean in our data. That is, we can identify the mean based on our samples, but also provide an upper or lower bound for where the mean might be. t.test(msleep$brainwt) ## ## One Sample t-test ## ## data: msleep$brainwt ## t = 2.1581, df = 55, p-value = 0.03531 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 0.02009613 0.54306673 ## sample estimates: ## mean of x ## 0.2815814 We use this t.test function because we assume that our data is normally distributed (which is not necessarily always the case) and it has a small sample size. You can see that while the mean for brain weight from this popular is 0.28, the range is fairly large (between 0.2 and 0.54) because of how big the variability is in the data. If we want to determine if two populations have a difference in their mean values that is statistically significant, we can calculate the t-test between two sets of observations. Let’s look at whether the average time spent awake by mammals who are insectivores is significantly less than those who are not. insectivores &lt;- msleep %&gt;% filter(vore == &quot;insecti&quot;) %&gt;% select(awake) head(insectivores) ## # A tibble: 5 x 1 ## awake ## &lt;dbl&gt; ## 1 4.3 ## 2 4.1 ## 3 5.9 ## 4 15.6 ## 5 15.4 other.mammals &lt;- msleep %&gt;% filter(vore != &quot;insecti&quot;) %&gt;% select(awake) head(other.mammals) ## # A tibble: 6 x 1 ## awake ## &lt;dbl&gt; ## 1 11.9 ## 2 7 ## 3 9.6 ## 4 9.1 ## 5 20 ## 6 9.6 t.test(insectivores, other.mammals, alternative = &quot;less&quot;) ## ## Welch Two Sample t-test ## ## data: insectivores and other.mammals ## t = -1.7796, df = 4.3091, p-value = 0.0723 ## alternative hypothesis: true difference in means is less than 0 ## 95 percent confidence interval: ## -Inf 0.8336811 ## sample estimates: ## mean of x mean of y ## 9.06000 13.86056 These results show a p-value of 0.07, which is not the usual level of significance that most scientists accept (p-value &lt; 0.05). However, there are numerous feeding behaviors besides insectivore and we may be interested in comparing the multiple categories against each other. We can use ANOVA (analysis of variance) to see how our quantitative variable is affected by the different kinds of feeding behavior. anova.msleep &lt;- aov(awake ~ vore, data = msleep) summary(anova.msleep) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## vore 3 133.7 44.58 2.234 0.0916 . ## Residuals 72 1437.0 19.96 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## 7 observations deleted due to missingness The p-value of 0.09 suggests that there may not be a strongly significant difference in the average time spent awake between these groups. 3.6.3 Identifying correlations When we are examining the relationship between two quantitative variables, we might be interested in whether they correlate with one another. A positive correlation between x and y means that as x increases so does y. A negative correlation between x and y means that as x increases, y decreases. We can use the base R cor function to calculate the correlation between two numeric vectors. cor(msleep$brainwt, msleep$bodywt) ## [1] NA Notice that we get NA instead of an actual numeric value. Unfortunately there is not an easy setting to have the cor function ignore any NA values. Thus, we need to temporarily remove these values. Let’s first find them with the summary function. summary(msleep$bodywt) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.005 0.174 1.670 166.136 41.750 6654.000 summary(msleep$brainwt) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.00014 0.00290 0.01240 0.28158 0.12550 5.71200 27 There are no NA values in the bodywt column, but there are 27 NA values in the brainwt column. Let’s remove those using a helpful function in tidyverse and save the more complete dataset as msleep2. msleep2 &lt;- msleep %&gt;% drop_na(brainwt) dim(msleep) ## [1] 83 11 dim(msleep2) ## [1] 56 11 You can see that we dropped 27 rows, corresponding to the mammals for whom brain weight was not measured or available. Now we can try to use the cor function. cor(msleep2$brainwt, msleep2$bodywt) ## [1] 0.9337822 We can see that the correlation is 0.93, a value very close to 1. This suggests that these variabilities are almost perfectly correlated with each other, which we saw when we plotted these two variables against each other in the prior section. There are multiple types of correlations. The cor function uses a Pearson correlation by default and can use different methods like Spearman. Let’s explore the difference between these correlations by looking at the relationship between the log value of brain weight and length of sleep cycle msleep3 &lt;- msleep %&gt;% mutate(brainwt_log = log(brainwt)) %&gt;% drop_na(brainwt_log, sleep_cycle) ggplot(msleep3, aes(x = brainwt_log, y = sleep_cycle)) + geom_point() + geom_smooth(method = &quot;lm&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; When we plot these variables against each other, including a fitted linear trend line, we can see that there is a relationship between these variables. However, the points do not quite fit a straight line if we look at the edges and center of this fitted line. This will reflect in our calculation of two different correlations. cor(msleep2$brainwt_log, msleep2$sleep_cycle, method = &quot;pearson&quot;) ## [1] 0.8331014 cor(msleep2$brainwt_log, msleep2$sleep_cycle, method = &quot;spearman&quot;) ## [1] 0.8726893 Pearson correlations are higher when the data fits to a straight line like a trend line. However, Spearman correlations do not require a single slope: as long as one value goes up and the other goes up, these values are considered highly correlated. 3.6.4 Producing linear models You may be interested in studying the relationship between one or more variables and some outcome that you care about. To determine if this set of one or more variables are strong predictors of an outcome (some quantitative variable), you can fit a linear model to this data using the lm function. Here we train a linear model to try to predict brain weight from body weight in the msleep dataset. Linear models are established with a formula with the format of outcome ~ predictor where we are trying to determine if the predictor is able to help us accurately estimate an outcome. my.mod &lt;- lm(formula = brainwt ~ bodywt, data = msleep2) summary(my.mod) ## ## Call: ## lm(formula = brainwt ~ bodywt, data = msleep2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.20686 -0.07565 -0.07184 -0.03565 1.18663 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.0761566 0.0479290 1.589 0.12330 ## bodywt 0.0009228 0.0003162 2.919 0.00686 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2459 on 28 degrees of freedom ## Multiple R-squared: 0.2333, Adjusted R-squared: 0.2059 ## F-statistic: 8.52 on 1 and 28 DF, p-value: 0.006859 In the summary of the results of this modeling, we see that bodywt is a significant predictor of brainwt in the msleep dataset. We can provide more predictors into our formula in the format: outcome ~ predictor1 + predictor2 + .... my.mod2 &lt;- lm(formula = sleep_total ~ bodywt + brainwt, data = msleep2) summary(my.mod2) ## ## Call: ## lm(formula = sleep_total ~ bodywt + brainwt, data = msleep2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.3940 -2.2591 -0.1205 1.3586 7.5917 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 12.309310 0.675518 18.222 &lt; 2e-16 *** ## bodywt -0.014261 0.004874 -2.926 0.00688 ** ## brainwt -3.290750 2.551012 -1.290 0.20800 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.319 on 27 degrees of freedom ## Multiple R-squared: 0.4012, Adjusted R-squared: 0.3569 ## F-statistic: 9.046 on 2 and 27 DF, p-value: 0.0009843 These results suggest that neither bodywt nor brainwt are significant predictors of sleep_total. That means that the value of either of those measurements does not give us information that helps us guess the total sleep duration for a mammal accurately. "],["day-5-building-workflows.html", "4 Day 5 - building workflows", " 4 Day 5 - building workflows Today we’re going to learn about applying these R skills to repeat, reproduce, extend, and share your analyses. First, let’s review the last two days. What have we done? How do we do it many times? Crack open your code - can you use it again? Can you adapt it to modify your question, feed in new data, and modify the scientifically-important bits easily? Can you share it with someone and they follow along? Our goals are to learn how to: read lots of files, and different kinds of delimited files use loops to do similar steps for many inputs obtain and use packages to user other people’s work how to get more help and documentation write code, and design workflows, while thinking about future readers/users design steps/chunks of work as modular functions to make adaptable workflows more fun tricks in generating Rmarkdown reports Preparing for Today: Please make sure you have completed all readings and activities from Section 3: Day 4 - tidyverse and visualizations. You will also have some time at the start of the day to do the first sections 4.1-4.3. Today’s Schedule: Start Time End Time Activity Description 10:00 AM PDT 10:15 AM PDT Synchronous Review workflows (4.1) 10:15 AM PDT 10:45 AM PDT Asynchronous Work through sections 4.2-4.3 10:45 AM PDT 11:30 AM PDT Synchronous Peer coding exercises on Zoom 11:30 AM PDT 12:15 PM PDT Asynchronous Work through sections 4.4 12:15 PM PDT 12:45 PM PDT Lunch Break Take a computer break! 12:45 PM PDT 1:45 PM PDT Synchronous Peer coding exercises on Zoom 1:45 PM PDT 2:15 PM PDT Snack Break Take a computer break! 2:15 PM PDT 3:00 PM PDT Asynchronous Work through sections 4.5-4.8 3:00 PM PDT 3:30 PM PDT Synchronous Wrap-up session on Zoom 4.0.1 What is a workflow? What does it take to do an analysis? When you embark on some project, it’s helpful (but not necessary) to plan where you are going. It’s best to setup and keep track of factors like: What data am I analyzing? What format is it in, and is that going to change? Will there be a lot more of this data that I want to process in the future? What kind of analyses do I want to do? What major steps do I have in my process? What work from others can I re-use and build upon? What do I want to generate? What is the desired outcome, and for whom is this intended? In the author’s experience, bioinformatic workflows often involve reading in big (or many) flat files, processing the data into a useful form, sometimes using extra packages to analyze the data, then generating plots and statistical summaries of what you found - usually in an Rmarkdown-generated report. "],["analyzing-viral-protein-sequences.html", "4.1 Analyzing viral protein sequences", " 4.1 Analyzing viral protein sequences Let’s work through an analysis of the size of some structural viral proteins. We’ve done some of the work for you already, and would recommend that you always document where the data came from and how it was retrieved/processed: The NCBI Identital Protein Groups database was queried for “structural” with “Division” restricted to “Viruses” and “Source database” restricted to “UniProtKB/Swiss-Prot”. 243 results were retrieved as FASTA files and converted using bash commands into separate tab-delimited files. 3 These files are in the data/viral_structural_proteins folder, are tab-delimited three fields, and look like this: Q8V433.1 Membrane protein Bovine respiratory coronavirus (strain 98TXSF-110-LUN) MSSVTTPAPVYTWTA... How do you inspect these files? What bash tools do you use? 4.1.1 Reading in tab-delimited file Let’s read one of them into R. The records are “delimited” by tabs, so each field is tab-separated. We’ll need to use the read.delim function and specify a tab separator. 4.1.1.1 Review of looking up documentation You can look up the documentation for any named function or package by using the ?function syntax. Sometimes, you need to use extra backticks to make it work, like ?`+` If you don’t know what you’re looking for, you can search with ??. Extra backticks don’t hurt, and are necessary when you have spaces in the query. Once the documentation is open, you can search for text. If you are using the R console, you can use / to open a search bar and enter-key to search for it. 4.1.1.2 Back to the file-reading… What does ?read.delim say it does? How do you set the delimiter/separator as a tab? 4 Is the function expecting there to be a file “header”? ( This would where the first line of the file has the name of each column ) Do we want to set this to TRUE or FALSE ?) viral_protein_data &lt;- read.delim(&quot;data/viral_structural_proteins/viral_proteins_100.tsv&quot;, sep=&quot;\\t&quot;,header=F) viral_protein_data ## V1 ## 1 Q8V433.1 Membrane protein ## V2 ## 1 Bovine respiratory coronavirus (strain 98TXSF-110-LUN) ## V3 ## 1 MSSVTTPAPVYTWTADEAIKFLKEWNFSLGIILLFITVILQFGYTSRSMFVYVIKMIILWLMWPLTIILTIFNCVYALNNVYLGFSIVFTIVAIIMWIVYFVNSIRLFIRTGSWWSFNPETNNLMCIDMKGRMYVRPIIEDYHTLTVTIIRGHLYMQGIKLGTGYSLSDLPAYVTVAKVSHLLTYKRGFLDKIGDTSGFAVYVKSKVGNYRLPSTQKGSGLDTALLRNNI What is viral_protein_data? How do we find the column names? What are two ways to access the protein sequence? How do we calculate the number of characters in this protein sequence? ( Try searching with “??number of characters” ) What type is the V3 column? What should it be? How do we tell R to treat these strings as they are, and not convert to factors? 4.1.2 Loops - doing a similar task multiple times You will want to repeat this analysis for multiple file. This automation is great, really useful for scaling your analyses. Let’s do this for all the proteins. The simplest way of doing this is to copy and paste it, and change the filename. viral_protein_data &lt;- read.delim(&quot;data/viral_structural_proteins/viral_proteins_002.tsv&quot;, sep=&quot;\\t&quot;,header=F, as.is=T) nchar(viral_protein_data$V3[[1]]) ## [1] 177 viral_protein_data &lt;- read.delim(&quot;data/viral_structural_proteins/viral_proteins_003.tsv&quot;, sep=&quot;\\t&quot;,header=F, as.is=T) nchar(viral_protein_data[1,&quot;V3&quot;]) ## [1] 137 Go ahead and do this for all 242 proteins … just kidding. This is a lot of work, and each time we do this we can introduce errors. If we ever have more files, we have to copy and paste more. If we ever want to change an argument for all of them, we have to do each one. Instead, we can work with a list of all the files available: list.files(path=&quot;data/viral_structural_proteins&quot;)[1:5] ## [1] &quot;viral_proteins_000.tsv&quot; &quot;viral_proteins_001.tsv&quot; &quot;viral_proteins_002.tsv&quot; ## [4] &quot;viral_proteins_003.tsv&quot; &quot;viral_proteins_004.tsv&quot; Note that I put a [1:5] to limit it to the first 5. You could also use head(). It’s a good idea to work with a small subset of files while you are iterating through development, then scale it up to the entirety. Now how do you calculate the number of characters for each protein? How do we do this for every file listed … ? 4.1.2.1 What are loops? Loops are for running a “code block” as many times as the “condition” determines. A “code block” is either one line of code, or multiple lines of code surrounded by curly brackets - {} { code &lt;- &quot;in a block&quot; with &lt;- &quot;multiple lines&quot; } The code just runs. Yep. It’s that simple. A “condition” is an expression of code that can either evaluate to either TRUE or FALSE, or set a variable for each time running the code block. This is often just before the code block, in () parentheses. The most common form of these kind of “control statements” is a for loop. Other “control statements” or “flow control statements” are while, repeat, and if. Let’s look up what they do, with ?`for` Note the back ticks! These are a trick in R to make anything be interpreted as literally what you type, and not any special characters. Like ?`+`, or ?`?` Here’s an example loop: for (i in 1:4) { print(i) } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 The pieces: (i in 1:4) is what is being looped over - 1:4 is a vector of 1 through 4 that is created, and it is put one at a time into i (a new variable). You need the parentheses. { and } denote the opening and closing brackets, specify the “code block” that is run each time. inside this “code block” is print(i) - it prints the variable i, which is set to a value of 1, 2, 3, or 4 for each loop How do we loop through and print each file name? What are the (1) code block and (2) loop condition ? for (i in list.files(path=&quot;data/viral_structural_proteins&quot;)[1:5] ) { print(i) } ## [1] &quot;viral_proteins_000.tsv&quot; ## [1] &quot;viral_proteins_001.tsv&quot; ## [1] &quot;viral_proteins_002.tsv&quot; ## [1] &quot;viral_proteins_003.tsv&quot; ## [1] &quot;viral_proteins_004.tsv&quot; How do we modify this to calculate the protein length? I add a line where we use nchar, but i is the filename instead of me typing it in there. for (i in list.files(path=&quot;data/viral_structural_proteins&quot;)[1:5] ) { print(i) print(nchar(read.delim(i)$V3[[1]])) } ## [1] &quot;viral_proteins_000.tsv&quot; ## Warning in file(file, &quot;rt&quot;): cannot open file &#39;viral_proteins_000.tsv&#39;: No such ## file or directory ## Error in file(file, &quot;rt&quot;): cannot open the connection Error! It is looking for a file viral_proteins_000.tsv, but it is looking in this directory. It is actually in data/viral_structural_proteins. Look up the list.files documentation, and find how to get it to return the full name of the file. Next, this should work… for (i in list.files(path=&quot;data/viral_structural_proteins&quot;, full.names=T)[1:5] ) { print(nchar(read.delim(i,sep=&quot;\\t&quot;,header=F,as.is=T)$V3[[1]])) } ## [1] 1248 ## [1] 1254 ## [1] 177 ## [1] 137 ## [1] 117 And yep, we have lengths of protein sequences. … What now? 4.1.2.2 Storing values from a loop How do we store these values for later analysis? In other languages, “append”. But R is not built that way. It’ll work, but it’s very inefficient. The “R-way” to store values from a loop is to define a vector of the right length, then put each element in it. Here’s some example vectors that we can create. vector(&quot;character&quot;,10) ## [1] &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; vector(mode=&quot;numeric&quot;,length=5) ## [1] 0 0 0 0 0 vector(&quot;logical&quot;,2) ## [1] FALSE FALSE Okay, but how do we access each position to save the value? We need to turn out list of files into indicies. We’ll save it first so we can count how many there are. seq_along is handy function to create a number sequence along a vector, otherwise use something like seq(1,length(x)). first_five &lt;- list.files(path=&quot;data/viral_structural_proteins&quot;)[1:5] for (i in seq_along(first_five) ) { print(i) print(first_five[i]) } ## [1] 1 ## [1] &quot;viral_proteins_000.tsv&quot; ## [1] 2 ## [1] &quot;viral_proteins_001.tsv&quot; ## [1] 3 ## [1] &quot;viral_proteins_002.tsv&quot; ## [1] 4 ## [1] &quot;viral_proteins_003.tsv&quot; ## [1] 5 ## [1] &quot;viral_proteins_004.tsv&quot; Putting these together, we can create and save a vector of file names: first_five &lt;- list.files(path=&quot;data/viral_structural_proteins&quot;)[1:5] filenamez &lt;- vector(&quot;character&quot;,length(first_five)) for (i in seq_along(first_five) ) { filenamez[i] &lt;- first_five[i] } filenamez ## [1] &quot;viral_proteins_000.tsv&quot; &quot;viral_proteins_001.tsv&quot; &quot;viral_proteins_002.tsv&quot; ## [4] &quot;viral_proteins_003.tsv&quot; &quot;viral_proteins_004.tsv&quot; And finally calculate the length of each protein: first_five &lt;- list.files(path=&quot;data/viral_structural_proteins&quot;, full.names=T, pattern=&quot;.*tsv&quot;)[1:5] lengthz &lt;- vector(&quot;character&quot;,length(first_five)) for (i in seq_along(first_five) ) { lengthz[i] &lt;- nchar( read.delim(first_five[i], header=F,as.is=T,sep=&quot;\\t&quot;)$V3[[1]] ) } lengthz ## [1] &quot;1248&quot; &quot;1254&quot; &quot;177&quot; &quot;137&quot; &quot;117&quot; Now we can take off the [1:5] limiter, and do the whole set: first_five &lt;- list.files(path=&quot;data/viral_structural_proteins&quot;, full.names=T, pattern=&quot;.*tsv&quot;) lengthz &lt;- vector(&quot;character&quot;,length(first_five)) for (i in seq_along(first_five) ) { lengthz[i] &lt;- nchar( read.delim(first_five[i], header=F,as.is=T,sep=&quot;\\t&quot;)$V3[[1]] ) } How do we go about visualizing/analyzing this? hist(lengthz) ## Error in hist.default(lengthz): &#39;x&#39; must be numeric er what…? Debug! What is lengthz? str(lengthz) ## chr [1:244] &quot;1248&quot; &quot;1254&quot; &quot;177&quot; &quot;137&quot; &quot;117&quot; &quot;117&quot; &quot;858&quot; &quot;1242&quot; &quot;1255&quot; ... Character? Let’s try numeric instead… first_five &lt;- list.files(path=&quot;data/viral_structural_proteins&quot;, full.names=T, pattern=&quot;.*tsv&quot;) lengthz &lt;- vector(&quot;numeric&quot;,length(first_five)) for (i in seq_along(first_five) ) { lengthz[i] &lt;- nchar( read.delim(first_five[i], header=F,as.is=T,sep=&quot;\\t&quot;)$V3[[1]] ) } Base R histogram hist(lengthz,breaks=50) More fancy ggplot library(ggplot2) ggplot( data.frame(length=lengthz) )+theme_classic()+ aes(x=length)+geom_histogram(bins=50) summary(lengthz) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0 230.0 626.0 726.3 1088.5 3390.0 And that’s about how long the viral structural proteins in this database tend to be. How do we extend this analysis to add more files? How do we calculate new properties about the sequences? How do we store that? 4.1.3 Extending the workflow How do you think about/plan the workflow we’ve built? One way is to flatten out all the tasks, and to script each individual task every time it is done. This requires the author, user, and reader to understand a lot of complexity. graph TD classDef default line-height:12px; classDef three line-height:12px,fill:yellow; you[analyst] --> B[finding files] & C[making a vector] & D[looping] & E[reading files] & F[getting protein seq] & G[calculting and saving nchar] & H[plotting nchar] B:::three; C:::three; D:::three; E:::three; F:::three; G:::three; H:::three; Another way to approach this is to cluster them into a hierarchy of modules. graph TD classDef default line-height:12px; classDef one line-height:12px,fill:pink; classDef two line-height:12px,fill:cyan; classDef three line-height:12px,fill:yellow; you2[analyst] --> find & read & plot; find:::one --> B; B[finding files]:::three; read:::one --> looping & reading; looping:::two --> C & D; C[making a vector]:::three; D[looping]:::three; reading[reading and calc]:::two --> E & F & G; E[reading files]:::three; F[getting protein seq]:::three; G[calculting and saving nchar]:::three; plot:::one --> H; H[plotting nchar]:::three; In this organization, the analyst can operate at levels of steps, modules, and specific instructions, depending on what is needed. Organizing your workflows into composable modules lets you extend these to un-ancipiated complexity. You could imaging using these steps or modules: graph LR classDef default line-height:12px; vp[read viral proteins] --> cl[calculate lengths] --> hist[histogram] in new ways by composing the elements together, to analyze a different source of proteins, with a new analysis, with similar plots: graph LR classDef default line-height:12px; vp[read viral proteins] --> cl[calculate lengths] --> hist[histogram] hp[read human proteins] --> cl --> boxplot[boxplot] vp --> crrar[find a particular motif] --> gchist[histogram] hp --> crar --> gcboxplot[boxplot] Where maybe these are how the inputs/outputs are defined: reading_proteins: input: directory path of proteins to read output: protein sequences calculating lengths: input: protein sequences output: numeric vector, of lengths calculating gc_content: input: protein sequences output: numeric vector, of lengths 4.1.4 Tips for modular workflows Try to not “hardcode” things - if it’s a number, consider if it can be a parameter that is “passed in” as an argument. Group repeated code functions - some folks say you should never repeat code (but do what works for you!). Try to read inputs and outputs as general, flexible formats - strings of filenames, vectors of values Write a comment at the top of the function that says what it’s doing and what to expect, generally comment things. Consider, the list.files() and hist() functions are already built this way! How about a ggplot2 style boxplot? 4.1.4.1 Let’s write a plotting module/function Write a function that makes a ggplot2 boxplot for some numbers. What should the function take, what should it do? What is “some numbers”? What should it return? wrap it in `print(ggplot code)` or return the ggplot object as a variable 4.1.5 More complex workflow Let’s write a simulation of viral evolution. (could be a bad idea, considering….) More examples/exercise show a simulation of something…. genetic drift of a virus replicating? lineage G1312F exercise - wrap the entire analysis as a function talk about ease of calling ease of tweaking this exercise - break into subfunctions, generate and plot ease of changing models 4.1.6 Apply is a popular tool Apply is another common way of doing something over and over. It is a very compact way to take pieces of a list, vector, dataframe, or matrix and put them into a function. There are: apply - for 2D objects lapply - for lists and vectors sapply - is lapply but with simplified returns mapply - is for combinations of multiple variables replicate - calls a function multiple times Some people strongly prefer coding this way. Here is an example: list_of_protein_files &lt;- lapply( list.files(&quot;data/viral_structural_proteins/&quot;,full.names=T), read.delim, sep=&quot;\\t&quot;,header=F) These “apply” some “FUN” to some input variables, by taking each element or slices of elements from input variables and putting them in as arguments to the function being applied. They return odd things, often lists (unlist()). One crucial aspect missing is the actual bash commands used. I did not save them as a script, and I should go back, save them in a script, and re-run that to make sure I get the same result!↩ Often this is typed into computers and scripts as \", as the tab key will often not put a TAB character into where you’re typing.↩ "],["functions-as-modular-steps.html", "4.2 Functions as modular steps", " 4.2 Functions as modular steps The beauty and danger of any programming project is that there are many ways to organize equivalent functionality. While liberating, unorganized variety in your code is more difficult for to read and help you with, and is more difficult for future you to read and re-use. One way to clearly organize code is to use a “functional” approach. This approach uses functions, as you learned a bit about on Wednesday, to organize your code into “code blocks”. You “call” this function by giving it “arguments”, and the function will return “return values”. This approach allows you to focus your attention on making (and testing that) a function does one thing, and does it well. Then later you can use these larger pieces in novel combinations to flexibly adapt to new requirements in your analysis workflows. For example, in one of the workshop’s author’s recent papers, they defined a function that wrote out multiple ggplot formats at once so that they wouldn’t have to re-type that code ever again. Functions are handy. For example, let’s calculate the standard error of a sample of values: values &lt;- c(4,3,2,2,5,3,6,2,2,4) stderr(values) ## Error in stderr(values): unused argument (values) Er … what? What does stderr() do? Use the help functionality of ? to figure out what the stderr() function is built for, and if there are any other functions that calculate the standard error of the mean. It would appear that functionality is not built into base R. Perhaps they assume that everyone will write their own standard error function? Let’s write one. How do you calculate the standard error of the mean of a sample? 5 Does that look familiar from your stats classes? Let’s code it up. I recommend you first develop the code in your functions by playing with it interactively: sd(values)/sqrt(length(values)) ## [1] 0.4484541 To use this multiple times, we could just copy and paste it into each script or workflow we want to use it in. However, this leaves lots of complexity for the future user to have to handle. For example, we can copy and paste a new variable name into the code: values2 &lt;- c(10,30,20) sd(values2)/sqrt(length(values)) ## [1] 3.162278 This can lead to errors in writing, small incorrect parts. In the above example, I purposefully neglected to change the second values to values2. Did you catch that easily, or did you have to look for it? Additionally, how do we save it, document it, and share it? Copy and pasting from a file is okay, but it takes up a lot more work/space. We could email around a file of code chunks, or share on a website. How do we make sure it works? This is hard to do with copy-paste code chunks. Changing the chunk to work on a new input can be an opportunity for introducing errors, typos. 4.2.0.1 Write it as a function! This would make a useful function. What are the parameters? Are any defaults set? How does it calculate with the parameters? What does it return? Once it is a function, it can be copy and pasted, or source()’d from a .R file. As you develop it, you can test that it works and change inputs easily. Later, you could change the function to fix bugs, and you don’t have to then fix all the places you copied it to - it’s fixing the code inside the function. sez &lt;- function(x) {sd(x)/sqrt(length(x))} sez(values) ## [1] 0.4484541 sez(rnorm(10)) ## [1] 0.2502884 sez(rnorm(100)) ## [1] 0.0877486 sez(rpois(1e3,3)) ## [1] 0.05482595 I always forget and look at wikipedia…↩ "],["writing-reusable-code.html", "4.3 Writing (re)usable code", " 4.3 Writing (re)usable code Crack open your code - can you use it again? Can you adapt it to modify your question, feed in new data, and modify the scientifically-important bits easily? 4.3.0.1 Examples Let’s look at two chunks of code from a paper (lightly edited). The experiment is counting barcoded lineages of yeast cells to estimate “PPIs” (protein-protein interactions) 6 Here’s an example of one style of writing R script: # filter out bad barcode lineages ( &lt;= 2 time points counts &gt; 0, or maximum of each time point &lt;= 5 or total counts of a lineage &lt; 10) bad_index = rep(0, nrow(DBC_known_counts)) for(i in 1:length(bad_index)){ counts = as.numeric(DBC_known_counts[i, 4:8]) if (length(which(counts != 0)) &lt; 3 | max(counts) &lt; 5 | sum(counts) &lt; 10){ bad_index[i] = 1 } } length(which(bad_index == 1)) # 1447775 What is going on here? How do you feed in new data? How do you run this multiple times? How do you change the logic? Here’s another example from the same author: H202_Output &lt;- PPI_calling_sigmoid( PPI_multiple=dataFrameReader_T(&quot;/Volumes/zmliu_02/PPiseq_03/H2O2/counts/PPI_multiple_p.values.csv&quot;), specific_PPV=c(seq(0.5,0.58,by=0.02),seq(0.6,0.8,by=0.01),seq(0.82,0.9,by=0.02)), Fitness=fitness(0,1,0.01), p_value=seq(-4,0,0.1), Neg_number_PPI=6e4, Neg_ref_number=50, p_threshold=-4 p_loc=6, ) Same questions. What are some differences? What’s useful for you? What’s a lot of effort to do? 4.3.1 Style guides can be inspiring tidyverse style guide google-specific changes Jean Fan’s search for “R style guide” What style should you use? Be inconsistently consistent! Balance for yourself: How easy is it to write? How easy is it for you to read? How easy is it for others to read? How similar is it to what everyone else is doing (a very good thing)? But most importantly, use what folks around you are using. Be lazy, imitate others! How would you describe your code writing style? How do you name things? How good are your comments? What ideas would you like to incorporate? Ask Darach if you want details, or read the paper↩ "],["packages-install-and-use.html", "4.4 Packages - install and use", " 4.4 Packages - install and use Let’s take a look at your code from the last few days. How would you share this with the other people in the course? R, like other languages, is built on a package system - if you wrap up your code in particular expected ways and put it in particular expected places, it is very easy for others to get and use your code Packages are said to be easy to make, from scratch. There’s more comprehensive instruction here. Personally, the author has never had cause to make one, but I can report that they’re very easy to use! 4.4.1 CRAN is the canonical R package resource The Comprehensive R Archive Netowrk CRAN is an integral part of the R environment. This is a global and free resource for storing and distributing code as R packages - well documented and designed chunks of code. You can install packages using the RStudio IDE, but it’s most important to know how to do it in R - what’s happening just under the hood. Use library to load packages. You should be able to load a package you’ve already installed. Call library() function, with an argument of the library name, as a string. Try loading this one: library(&quot;stringdist&quot;) library(stringdist) Don’t have it? You can look for packages by using a search engine, or finding suggestions in the documentation, papers. Once you have a package name, use install.packages(&quot;stringdist&quot;) to install it. You may need to select a “mirror”. This is just where you are downloading from, usually just go with “0-Cloud” 7. What did this do? It retrieves and downloads the package’s code and documentation to what’s called your library, get the stringdist library, use it get a new viz layer, use it 4.4.2 Using packages without library()-ing them Some packages you don’t really want to have to go through the trouble of loading, you’ll just want one function. For example, this website is generated with one function, render_book() in the package bookdown. Instead of: library(bookdown) render_book() I can save a line by just using :: bookdown::render_book() This means I am looking in library bookdown for function render_book. The function is hidden again right afterwards, but it is useful if you just want to run one function from a library, once. 4.4.3 Github is a common place for sharing packges in development Installing from github requires some different programming - and there’s libraries with functions for this. library(&quot;remotes&quot;) This used to be part of the devtools package, but the authors have been “uncoupling” these to make each package simple and composable. 4.4.3.1 Let’s install a package off of github. Here’s a cool Ghibli color palette, so you can change sets of colors for your plots. Try installing it with: remotes::install_github(&quot;ewenme/ghibli&quot;) Then adapt one of your previous plots (that has colors) to use these new color palettes. Exercise - install a package from github and learn how to use it Sports field plotting Ridgeplots in ggplot2 problems with this - security, eh 4.4.4 Bioconductor is a great place for bio-related packages 😬 Bioconductor is a giant repository of just scientific, well bio-focused, packages. It’s where a lot of bioinformatics software gets published. However, it has it’s own package system and thus package manager. You will have to install it … from CRAN. Like so: install.packages(&quot;BiocManager&quot;) That installs a package with a function that then installs Bioconductor packages. It’s actually easy. Let’s give it a try by installing and using the limma package to analyze some gene expression data. If you’re recall from before, we can use :: to avoid having to actually library the whole package: BiocManager::install() How to install Bioconductor Scholars will know how to install Bioconductor package from CRAN Scholars will know how to find and install packages from Bioconductor As demo, install DESeq2 and start to work through a tutorial with one of their datasets. 4.4.5 What packages do you have already? explore the namespace for a package, ie ?packagename:: and TAB or ?packagename:: how to print out all the packages you have loaded 4.4.6 Finding more help and documentation Need a problem ?geom_line ?stringdist Objective X.X: Scholars will know how to get official docs ( ? ) and how to skim/read these Objective X.X: Scholars will see how easy it is to use a search engine to try and find other answers Objective X.X: Scholars will understand the intention of the Slack being a learning community, or something (set expectations about this) Source up problems from the class? Demo a weird plot idea, search for how to do it on stack overflow? Maybe how to change the color of facet labels in a ggplot facet graph, that’s a bit tricky and demonstrates why base R is important… I have no idea where this is, never heard of that country!↩ "],["more-rmarkdown-for-reproducibile-sharing.html", "4.5 More rmarkdown for reproducibile sharing", " 4.5 More rmarkdown for reproducibile sharing How are you saving your notes for this class? How do you usually save notes/code/instructions? How do you share this with others? 4.5.1 rmarkdown package 4.5.1.1 Background The rmarkdown package was created by Yihui Xie, extending from previous work and ideas in knitr and sweave. It’s a way of mixing writing with code, such that you can run the code and it makes a pretty doc (and more). video1 overview 4.5.1.2 Chunk options You can set chunk-level options in an Rmd, for each code chunk. Such as: ```{r, name_of_chunk, cache=T, fig.width=3, fig.height=2, error=F, warning=F, fig.align=&quot;right&quot;} x &lt;- 1:10 y &lt;- 1:10 plot(x,y) ``` x &lt;- 1:10 y &lt;- 1:10 plot(x,y) It is possible to enable figure captions. video2 chunk options 4.5.1.3 Document options You can set document-level options to. To do this, you create what’s called a YAML header, like so: --- title: &quot;Titled&quot; author: &quot;yours&quot; --- You need three hyphens to open and close it. Put it at the very beginning. You can define quite a few options, including themes, like so: --- title: &quot;Titled&quot; author: &quot;yours&quot; --- video3 head options 4.5.1.4 How it works, sort of… video4 how it does what it’s doing rmarkdown parses an Rmd file to extract out the R code. It runs this, using the chunk and other options to control this, then makes outputs from different chunks (text and plots). It then sticks the text and images into a markdown document, and uses a program called pandoc (included inside Rstudio) to turn that into HTML, PDF, and/or slides. 4.5.2 Sharing with others What does someone need to re-run your analyses? R (often Rstudio) what packages you’re using devtools::session_info() ## ─ Session info ─────────────────────────────────────────────────────────────── ## setting value ## version R version 3.6.3 (2020-02-29) ## os Ubuntu 20.04.2 LTS ## system x86_64, linux-gnu ## ui X11 ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz America/Los_Angeles ## date 2021-06-04 ## ## ─ Packages ─────────────────────────────────────────────────────────────────── ## package * version date lib source ## assertthat 0.2.1 2019-03-21 [1] CRAN (R 3.6.3) ## bookdown 0.22 2021-04-22 [1] CRAN (R 3.6.3) ## callr 3.5.1 2020-10-13 [1] CRAN (R 3.6.3) ## cli 2.2.0 2020-11-20 [1] CRAN (R 3.6.3) ## crayon 1.3.4 2017-09-16 [1] CRAN (R 3.6.3) ## desc 1.2.0 2018-05-01 [1] CRAN (R 3.6.3) ## devtools 2.2.2 2020-02-17 [3] CRAN (R 3.6.3) ## digest 0.6.27 2020-10-24 [1] CRAN (R 3.6.3) ## ellipsis 0.3.1 2020-05-15 [1] CRAN (R 3.6.3) ## evaluate 0.14 2019-05-28 [1] CRAN (R 3.6.3) ## fansi 0.4.1 2020-01-08 [1] CRAN (R 3.6.3) ## fs 1.5.0 2020-07-31 [1] CRAN (R 3.6.3) ## glue 1.4.2 2020-08-27 [1] CRAN (R 3.6.3) ## htmltools 0.5.0 2020-06-16 [1] CRAN (R 3.6.3) ## knitr 1.30 2020-09-22 [1] CRAN (R 3.6.3) ## magrittr 2.0.1 2020-11-17 [1] CRAN (R 3.6.3) ## memoise 1.1.0 2017-04-21 [3] CRAN (R 3.5.0) ## pkgbuild 1.2.0 2020-12-15 [1] CRAN (R 3.6.3) ## pkgload 1.1.0 2020-05-29 [1] CRAN (R 3.6.3) ## prettyunits 1.1.1 2020-01-24 [1] CRAN (R 3.6.3) ## processx 3.4.5 2020-11-30 [1] CRAN (R 3.6.3) ## ps 1.5.0 2020-12-05 [1] CRAN (R 3.6.3) ## R6 2.5.0 2020-10-28 [1] CRAN (R 3.6.3) ## remotes 2.1.1 2020-02-15 [3] CRAN (R 3.6.2) ## rlang 0.4.9 2020-11-26 [1] CRAN (R 3.6.3) ## rmarkdown 2.8 2021-05-07 [1] CRAN (R 3.6.3) ## rprojroot 2.0.2 2020-11-15 [1] CRAN (R 3.6.3) ## sessioninfo 1.1.1 2018-11-05 [3] CRAN (R 3.5.1) ## stringi 1.5.3 2020-09-09 [1] CRAN (R 3.6.3) ## stringr 1.4.0 2019-02-10 [1] CRAN (R 3.6.3) ## testthat 3.0.1 2020-12-17 [1] CRAN (R 3.6.3) ## usethis 1.5.1 2019-07-04 [3] CRAN (R 3.6.2) ## withr 2.3.0 2020-09-22 [1] CRAN (R 3.6.3) ## xfun 0.23 2021-05-15 [1] CRAN (R 3.6.3) ## yaml 2.2.1 2020-02-01 [1] CRAN (R 3.6.3) ## ## [1] /home/zed/R/x86_64-pc-linux-gnu-library/3.6 ## [2] /usr/local/lib/R/site-library ## [3] /usr/lib/R/site-library ## [4] /usr/lib/R/library why the hell we want others to use our code? Well I’ll tell you why - the general intellect think about this think about the next person coming along, or yourself in 3 months links on syllabus for more info all sorts of automated stuff you can do 4.5.3 Reproducibility and sharing think about this think about the next person coming along, or yourself in 3 months links on syllabus for more info all sorts of automated stuff you can do "],["troubleshooting-getting-help-dissecting-problems.html", "4.6 Troubleshooting, getting help, dissecting problems", " 4.6 Troubleshooting, getting help, dissecting problems You will have problems. How do you (1) get past these and (2) learn from them? What kinds of errors did we encounter in this class? How do we deal with these? 4.6.1 Places to look for help yourself - past notes, troubleshoot logically the people around you, research mentors Slack channel github/gitlab “Issues” page - search first the closed issues!!! StackOverflow - search first for similar problems!!! twitter (for fun problems) It helps to have a… 4.6.2 Minimum reproducible example video1 If you can’t find an easy answer, it is important to begin to approach the problem like you would a science problem. Reduce the problem, eliminate extraneous variables. Use or capture intermediate steps to identify where the issue is. Collect sessionInfo() and share it with folks. "],["conclusion.html", "4.7 Conclusion", " 4.7 Conclusion Today we covered: loops to repeat code aspects of nice (re)usable code writing code modularly with functions finding and using packages writing an Rmarkdown report finding help and tackling a complex problem We hope you are better oriented and prepared to repeat your analyses to build a complete story, reproduce these larger analyses workflows, extend from your work to do more complicated workflows, and share your analyses with others in a well-organized but complete way. You can extend on these ideas. Here’s a couple of areas that might be useful: Workflow / pipeline tools These are automation tools to smartly run different chunks of analyses. This is really handy when parts of your workflow take a long time, and you usually don’t want to be re-running those, but you do want it to be able to run the whole thing automatically. You can use old-school shell-like tools such as “Make/Makefiles”, newfangled but complex tools like “Nextflow” or “snakemake”, or R-specific tooks like “targets”. "],["appendix.html", "5 Appendix", " 5 Appendix Useful bits "],["additional-tutorials.html", "5.1 Additional tutorials", " 5.1 Additional tutorials a list of tutorials 5.1.1 sub sub section "],["more-challenges.html", "5.2 More challenges", " 5.2 More challenges Challenge yourself Extra datasets ere "],["figures-ideas.html", "5.3 Figures ideas", " 5.3 Figures ideas An interactive page showing different types of figures A R graph gallery "],["rmd-example-of-code-chunk.html", "5.4 Rmd example of code chunk", " 5.4 Rmd example of code chunk knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 5.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa "],["more-example-rmds.html", "5.5 More example Rmds", " 5.5 More example Rmds extraRmds/worksheet.Rmd "],["how-to-edit-this-document.html", "5.6 How to edit this document", " 5.6 How to edit this document Here is a too-long video about how to edit this website. Each level 1 header is its own heading the left TOC 5.6.1 Setup Clone the repo locally git clone https://github.com/darachm/dll-r 5.6.2 Edit/work on it git pull so you’re current! Error with what is in docs/? Do git checkout docs/ to remove the edits to those, then pull again. Make or edit the files whose name starts with two digits and ends with Rmd, Like 01-day3.Rmd. Feel free to run that in Rstudio or whatever. To test the full site, sun ./_build.sh to build the entire site. ( Can Rstudio run this as a terminal ??? There may be another way to do this in Rstudio - I believe they develop the bookdown package? ) The entire site is in docs/ because that is what github wants. When done, add and commit your changes. git add 02-day4.Rmd # or whatever you changed git commit -m &quot;nice commit message of what you have done&quot; git checkout docs/ # this prevents errors from docs not matching what&#39;s published git pull # to make sure current git push # to push it up to github 5.6.3 Publishing onto the main page, so it’s hosted at the URL git pull # to make sure current # if there are conflicts, then run git checkout docs/ # this prevents errors from docs not matching what&#39;s published # if not, you can skip this step! ./_build.sh # builds the site in docs/ git add docs/* # adds the entire site to the repo git commit -m &quot;updated site&quot; git push # to push it up to github "],["more-links.html", "5.7 More links", " 5.7 More links Claus Wilke’s dataviz bookdown "]]

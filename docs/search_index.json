[["index.html", "DLL 2021, R section 1 Workshop Introduction", " DLL 2021, R section mesako Margaret Samson Zac darachm Typeset on 2021-06-18 1 Workshop Introduction This short 3-day course in R aims to give you a basic framework and skills for working effectively with your research mentors. Together, we will get oriented with basic skills (e.g. using RStudio, documenting your process with R Markdown, reading data in, basic data analysis, and visualization) and concepts for how to organize your research workflows in R. We intend for this starting point to empower you to accomplish research-related tasks in R. R is a high-level data analysis scripting language 1. While it is very easy to write programs in this language, it is designed first as an environment that stitches together cutting edge research methods with flexible visualization and reporting frameworks. R has swept to be the de facto high-level language for data analysis because of the rich ecosystem of dispersed open-source developers. Here’s some examples of plots you generate in R. Here’s an example of the types of workflows and analyses you can generate in R (all the plots, and the website too). Heck, this website is generated by the R package bookdown from Rmd files, which you will learn to write. and also a “GNU” project, apparently!↩ "],["workshop-goals.html", "1.1 Workshop goals", " 1.1 Workshop goals We aim for all participants to be able to: use the Rstudio IDE (open source edition) know how to store and manipulate data in variables read in data from computer files in various formats process these with functions to generate statistical summaries turn these into various plots using the base graphics and ggplot2 library read in packages from various sources and know how to start using them do these steps in workflows that scale to analyzing many many files write all of this up as an Rmarkdown file to report your analysis and findings to collaborators "],["structure-and-resources.html", "1.2 Structure and resources", " 1.2 Structure and resources 1.2.1 Workshop schedule Each day will have a slightly different schedule, but you can expect a mix of synchronous and asynchronous work sessions, as well as two breaks in that day’s workshop. Please visit each day’s specific page for the exact schedule: Day 3 Schedule Day 4 Schedule Day 5 Schedule Each time segment lists who among the teaching team will be around to assist. We encourage you to poke the Slack channel (so below for info) with questions, but you can feel free to tag (@) or directly message the person responsible for the time slot/content you have questions about. 1.2.2 Asynchronous sessions Asynchronous here means self-paced learning that takes place off-Zoom. You will be expected to progress through this website during asynchronous work time in this 3-day period. We developed this website/document for your reference, as a living textbook and collection of “slides” and code snippets. As you go through this website during the asynchronous sessions, you should also complete the exercises provided in the accompanying worksheets. If you have questions and/or need help, you should reach out to us and your peers on Slack. We are also happy to jump into a Zoom call to work through issues with you. Tips: You can shrink the table of contents (left) by clicking the four lines icon in the top menu. You can click on footnotes 2. 1.2.2.1 Slack channel While you are working in asynchronous sessions, or if you just need help during the remainder of your program, there is a Slack channel available where you can go for ideas/help. The channel is called #learn-R and should be accessible to you on the SSRP Slack server. 1.2.3 Synchronous sessions Synchronous here means live, group learning that takes place on-Zoom. During synchronous meetings, you should plan to work directly with your peers and us on focused tasks. We will also be there to help with confusing or challenging topics that you want to discuss with someone live. During the schedule synchronous session (check each day’s schedule for timing), you should log on to the provided Zoom link. to read them, then go back by clicking the arrow↩ "],["workshop-expectations.html", "1.3 Workshop expectations", " 1.3 Workshop expectations Be respectful and compassionate. Teach one another, learn from one another. Aim for productive struggle. You will learn best if you make a good faith effort before seeking help. However, you should always seek help if you feel truly stuck. Create your own sense of challenge. Pick activities that you will learn and grow from. If you don’t find something challenging, make it challenging for yourself. "],["day-3-intoduction-to-r.html", "2 Day 3 - intoduction to R", " 2 Day 3 - intoduction to R Learning Goals: By the end of today’s session, students will be able to: Navigate the RStudio environment and move between the different panels. Open, save, and run an Rmd (R Markdown) file within RStudio. Apply internet searches, help functions, and documentation to learn to use the appropriate functions. Assign and manipulate variables within the current environment/session in R. Create, modify, and access into a vector (an ordered grouping of elements). Create, modify, and access into a dataframe (an ordered two-dimensional grouping of elements with rows and columns). Apply existing functions to accomplish a specific task in R. Preparing for Today: Please read through all parts of Section 1: Workshop Introduction. Before you start reading through Section 2 for today or taking part in synchronous activities, download and open today’s worksheet. On this Github page, please right-click the button that says “Raw” and click “Save Link As…” to get a local copy of this Rmd file on your computer and open it in Rstudio. Today’s Schedule: Start Time End Time Activity Description Facilitator 10:00 AM PDT 10:45 AM PDT Synchronous Welcome session on Zoom Melissa, Samson, Margaret, Zac, Darach 10:45 AM PDT 11:30 AM PDT Asynchronous Work through sections 2.1-2.3 Zac, Samson, Darach 11:30 AM PDT 12:15 PM PDT Synchronous Peer coding exercises on Zoom Melissa, Samson, Zac, Darach 12:15 PM PDT 12:45 PM PDT Lunch Break Take a computer break! N/A 12:45 PM PDT 1:30 PM PDT Asynchronous Work through sections 2.4-2.5 Margaret, Zac, Samson 1:30 PM PDT 2:30 PM PDT Synchronous Peer coding exercises on Zoom Melissa, Samson, Zac 2:30 PM PDT 3:00 PM PDT Snack Break Take a computer break! N/A 3:00 PM PDT 3:30 PM PDT Synchronous Wrap-up session on Zoom Samson, Margaret, Darach, Zac "],["using-rstudio.html", "2.1 Using RStudio", " 2.1 Using RStudio Hi everyone, welcome to the R section of the data learning labs! What is R? R is a programming language. Other programming languages include Python, Java, C, and more. Different programming languages are better for different tasks. R is commonly used for data analysis and visualizations. Many people use a tool called RStudio, which we will over the next few days. RStudio allows you to write code in R, run that code, and more! In this chapter, we walk through RStudio so you have an understanding of its main functionality and its relationship with the R programming language. If you have not yet installed R and RStudio, please refer to the Pre-Workshop Installation Guide and follow the instructions to install R and RStudio. Follow along with the steps below on your own computer. There is a video at the end that walks through most of the content as well. 2.1.1 R Scripts in RStudio When you first open RStudio, it will look something like this. RStudio Homepage Most of the time, you will be working with an R script or an R markdown file. In this section, we will cover R scripts. R markdown files will be covered in the next section. To open a new R script, click the button in the upper left hand corner shown below. RStudio Homepage When you click that button, you should see a dropdown menu like in the picture below. New script dropdown After clicking on “R Script” you will have created a new, blank R script that should pop up. Your RStudio should now look like this. RStudio with blankj script To start, let’s save our script so that if our computer crashes we don’t lose all our progress. You can do this by clicking File -&gt; Save, or Command + S, or Ctrl + S depending on if you are on a Mac or PC. Let’s name our file “Demo.R”. Including the .R at the end of the file name indicates to your computer that it is an R script. Now, let’s look at each of the sections of R Studio in more depth. First, let’s look at the top left, shown in the screenshot below. Zoom on blank script This is your R script. Here is where you can write blocks of R code to run. You will do a majority of your coding here. When you’ve written some code, a fully written script might look something like this. Full script example Don’t worry if it doesn’t make sense to you now - by the end of the week you’ll be able to write code just like it! You will see a few icons near the top of the screen. The arrows allow you to navigate between files if you have multiple files open. You can also click on the tabs that will appear on the top of the screen. There’s also a button to make your code pop up in a different window, if you prefer, and a save button. You will also see a checkbox next to text that says “Source on Save”. In R, source means run. If you check the box next to “Source on Save” your code will run every time you save it. There are a few other tools that can help you manipulate your code. The most important buttons are on the righthand side, “Run” and “Source”. If you click run, the line of code where your cursor is will run. If you have a block of code highlighted, that block will run. You can click source to run all your code in the script. If you click the arrow next to source, there’s a “Source with Echo” option that will print each line of code in the console as it runs. 2.1.2 Console and Terminal Now, we’ll move to the lower left corner, as shown in the figure below. Zoom in on console You will see there are three sections: Console, Terminal, and Jobs. The Console is a place where R is up and running, waiting for you to enter commands. As we progress through the course you will learn how to do increasingly complex things in R. For now, we can work with a simple functionality of R: adding numbers. Try typing 5+11 in the console and pressing enter. You should see 16 output below like in the figure. Console output It’s important to note that the console is not a good place to write lots of R code. It is useful for simple commands that only need to be run one time. You can also use it to help debug lines of code. For example, if you’re trying to figure out how to do something, you can try playing with it in the console until the output is what you expect. Then, you can add it to your script above. When you run code if there is any output (e.g. if you print something) it will show up in the console. Now, let’s look at the terminal. The terminal here is the same as the terminal you learned about in previous days! It’s just another way to access it, and run the same commands. Try playing around with some of the commands you learned about in the previous days! Lastly, there is a jobs tab. Just like with remote machines, you can run jobs locally in RStudio. You’ll see a broom in each of the Console, Terminal, and Jobs tabs. This simply clears what’s there so you can start fresh. 2.1.3 Environment and History Now, let’s move to the upper right corner. Zoom on environment This tab shows your environment. The environment contains any data that you may have saved as part of running your code. Your environment is probably empty right now. Try typing x &lt;- 7 in the console. Zoom on environment with variable This will store the value 7 in a variable called x. You should now see a variable named x pop up with the value 7! You can also store other types of data in variables. Try typing y &lt;- \"hello\" in your console. Now if you type y, you should see “hello” printed! If needed, you can save your current environment and all the variables in it (also called a workspace) by clicking the save button on top. Directly to the left of the save button is a button to load a pre-existing workspace. This can be useful if you have been doing analysis in the console instead of a script and want to save some of your variables to share with someone else. The history tab shows a history of commands you have run. If you double click on a command, it will put it in the console for you and you can click enter to run it. However, be careful that some environment variables may have changed! Try typing x in the console to see the value of x. You should 7 print out. Now try typing x &lt;- x+2 in the console. You should see the value 9. The line of code we just wrote took the value of x, added 2, and stored that new value in x. Because x was 7, the new value is 9. If you type x again, you’ll see the value 9 print out. Similarly, if you use the same command from your history, if any of the variables involved as changed, the output may be different. Once you overwrite a variable, you cannot recover its old value! You will probably not use these last two tabs. The connection tab allows you to connect to some kinds of external databases (though more commonly your mentors will provide you with some data files for you to load in a script). The tutorial tab can help walk you through various things in R. 2.1.4 Files, Plots, and Packages Lastly, we will move to the bottom right. The first tab, Files, is like a small version of Finder or the Windows File Browser. The Plots tab will show any plots you might make. The Packages tab shows all packages you have installed (like tidyverse, which we will use later!), with check marks indicating which ones are loaded. The Help tab has some resources you can use, though googling and searching for your problem on websites like Stack Overflow will also be useful. 2.1.5 Putting It All Together Let’s put what we’ve learned together. Copy the code below and paste it into a blank R script. x &lt;- c(1,2,3) print(x) ## [1] 1 2 3 print(sum(x)) ## [1] 6 x[1] &lt;- 0 print(x) ## [1] 0 2 3 print(sum(x)) ## [1] 5 The first line of code stores a vector of numbers into the variable x. The next line prints that list. The next line takes the sum of those numbers and prints that. A command that does something else is called a function. Here, sum is a function that takes a vector of numbers as its input, and outputs the sum of all the numbers. The next line of code changes the first value in the vector and replaces it with 3. We can see that change when we print the vector. We can also see the sum changed accordingly. In R, whenever we call a function we write the name of the number, then put the inputs (also called arguments) in parenthesis after the name of the function. There are other built in functions like mean() and min(). We can also write our own functions. We’ll work with functions more throughout the week. "],["using-rmds.html", "2.2 Using Rmds", " 2.2 Using Rmds Throughout the next few days we will be using another type of file that can run R code called and R Markdown file, or Rmd. To create a new Rmd file click the same box in the upper left corner as you did when creating a new R Script, but instead of R Script, click on R Markdown (3rd box from the top). Make the title of your file Demo, and click OK (keep the default output as HTML). A new file will be created that looks like this . Save the file, making sure to put .Rmd at the end of the file name so your computer knows it’s an R Markdown file. 2.2.1 Basic Structure An R Markdown file has two main parts: R code and Markdown code, hence the name R Markdown. Markdown is a language that was designed for writing text that converts easily to HTML format. People might use Markdown to write blog posts on their website, or to provide context to an R analysis! We can use R Markdown files to create a nice writeup that explains what we’re doing in our analysis. We can insert chunks of R code where needed, and display either the actual code, or the text / figures generated from the code. In fact, this tutorial is all written in R Markdown! The next few sections will discuss how to write in Markdown, how to insert a chunk of R code, and how to put it all together. 2.2.2 Overview of Markdown In Markdown, you can type text just like you would in a word document. If you need a new paragraph, just hit enter twice (so there is a gap between text). You can also press the space bar twice at the end of a sentence to create a new line. You can also create headers in Markdown using the #. You can type from 1-6 #s, followed by a space, and then the header. Click on “Knit” at the top of the page in your Rmd file. You should see a window pop up with the compiled file. “R Markdown” in big, bold letters, with text underneath, like below. Close the window with the compiled R Markdown. Change the number of # symbols in front of the text “R Markdown” on line 10. Now try clicking “Knit” again. You should see the text get smaller as you increase the number of #! You can also make text italicized by putting a single asterisk * on both sides of the text. Try italicizing some text in your demo file, and make sure you click “Knit” and check! You can bold text with ** on both sides. You can bold AND italicize with triple asterisks on both sides. More detail about markdown can be found here. Click the link and spend a few minutes playing around with Markdown! 2.2.3 Overview of how to write an R chunk The next part of an R Markdown file is the R. You can insert a chunk of R code by writing the following This will run the code in the block! Try adding that block of code somewhere in your demo Rmd file. In case you’re having trouble finding it, the character that’s repeated three times is the backtick, it’s usually on the upper left of your keyboard. You should now see output that looks like this in your R Markdown file; make sure you Knit the Rmd file to check! print(&#39;This is R code!&#39;) ## [1] &quot;This is R code!&quot; You’ll notice that there’s some R code chunks already present in your R Markdown file. Let’s talk about them! The first code chunk looks like this Let’s talk about {r setup, include=FALSE}. The first character r just indicates that this is an R code chunk. Then, there is a space followed by setup. This is a name for the code chunk and is option. It cannot contain any spaces. Then, following the comma, you’ll see include=FALSE. This means that the code is not visible in the Rmd file once compiled and neither is the output. However, the code chunk is still run. Create a new Rmd file, but click “Create empty document” instead of “Ok” on the pop up menu. Now, create a code chunk that looks like this When you click Knit, you’ll get an error because we haven’t defined x! Put a new line of code in that code chunk where you define x to be some number (remember, we set variables with the &lt;- operator, like x &lt;- 10). Now, let’s say that you only wanted to print the value of x, but didn’t want the code to show up. In the second code chunk where you are printing x, set echo=FALSE. Remember to include a comma after the r (or after the name of the chunk, if you gave it a name). Now, when you knit the file, you should only see the value of x printed, like this: Now, let’s say you wanted to show someone how they write some code, but you don’t actually want to run it. Change echo=FALSE to eval=FALSE. Now, when you knit the Rmd, you should see the code but not the printed output, like this: . If an Rmd file is taking a long time to knit, that could be because some of the code is taking a long time to run. The code is rerun every time you knit the file! If this is the case, you can add a cache=TRUE flag to a code chunk which will stop it from running every time you knit the Rmd file. It will only run if you change something. You can add multiple flags to a code chunk by using commas between them. 2.2.4 Final Thoughts R Markdown files tend to be useful for creating reports that integrate text, code, and figures. We will be using them throughout the next few days as an interface to practice writing chunks of R. "],["getting-started-in-r.html", "2.3 Getting started in R", " 2.3 Getting started in R Let’s get started programming in R! Our goal is to be able to interact with datasets like the built-in iris dataset in R. By the end of today, we aim to be able to pull out information from this dataset and modify it using R programming. head(iris, n = 10) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5.0 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa 2.3.1 Getting help in R If you run into any error while using R, it is a great idea to look up your error message in Google and read through forum posts on StackOverflow. You may find it also helps to know more about your session info such as the version of R you are using, and what packages you have currently loaded. sessionInfo() ## R version 3.6.3 (2020-02-29) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 20.04.2 LTS ## ## Matrix products: default ## BLAS: /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.9.0 ## LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.9.0 ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8 ## [5] LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## loaded via a namespace (and not attached): ## [1] compiler_3.6.3 magrittr_2.0.1 bookdown_0.22 htmltools_0.5.0 ## [5] tools_3.6.3 yaml_2.2.1 codetools_0.2-16 stringi_1.5.3 ## [9] rmarkdown_2.8 knitr_1.30 stringr_1.4.0 digest_0.6.27 ## [13] xfun_0.23 rlang_0.4.9 evaluate_0.14 You can look up what functions do using either a question mark ? or the help function. ?rm help(rm) You can try running the following command to look up variables that load in with base R or packages that we will use. For example, you can use the help function to read more details about the iris dataset (worksheet task 2.3.1A). ?iris 2.3.2 Using variables and data types Variables are a short-hand name or label that stores a piece of data. We can save information into variables and then call them by name (invoke the variable) to use that information when needed. You can assign variables using the = or &lt;- operators. We will use the &lt;- operator exclusively going forward. save.num &lt;- 7 save.num ## [1] 7 Here I have chosen the name save.num but you can give it a different name. Note that variable names must with a letter and cannot contain special characters (worksheet task 2.3.2A). Variables are mutable: you can overwrite the saved value of a variable with another value. save.num &lt;- 10 save.num ## [1] 10 You can see that save.num does not remember the value 7 anymore and instead returns 10. Variables can be saved as other values such as character strings or boolean values (TRUE or FALSE). These are examples of other datatypes in R. save.string &lt;- &quot;hello&quot; save.bool &lt;- TRUE You can check what variables you have assigned in your current working environment using the ls function. Try running ?ls to learn more about this function! ls() ## [1] &quot;save.bool&quot; &quot;save.num&quot; &quot;save.string&quot; You can also remove saved variables using the rm function (worksheet task 2.3.2B). rm(save.num) ls() ## [1] &quot;save.bool&quot; &quot;save.string&quot; You will notice that our session no longer remembers save.num. If you tried to call save.num after it was removed, you would get an error message in R. 2.3.3 Using functions We have already shared several functions with you, including the help function, the ls function, and the rm function. Even the command sessionInfo is an example of a function in R! Functions can be recognized by a string (letters) followed by parentheses (). Functions may take information inside the parentheses that are called arguments. ?help When you look at documentation for the help function, you will notice that there are many things you can provide inside the parentheses. The documentation section called “Arguments” describes one necessary input for help called topic that must be provided inside the parentheses. Arguments are often named and described in the documentation. When you provide an argument to a function, you can provide it by name explicitly or just let R figure it out based on the order you give it. help(topic = iris) help(iris) In the second line, we provide only one input iris and R assumes that we intend for topic = iris as the argument. One function we will use often is the c function which can be used to create a vector or ordered collection of pieces of information. ?c If you look up the documentation for the c function, you will notice that it accepts ... arguments. This can be confusing but it often means that it accepts multiple arguments, more than can be named or described. This makes sense for the c function because it will accept as many inputs as you give it, the number does not need to be consistent. save.data &lt;- c(1, 4, 6, 2, 3, 8, NA) Functions generally take the arguments in the parentheses as an input and then produce some output (worksheet task 2.3.3A). A clear example of this is using the mean function. ?mean As implied, the mean function will take a grouping of numbers and return the mean or the average. mean(save.data) ## [1] NA However, here we ran into an issue because one of the elements in our initial vector save.data is NA. NA means “not available” or in other words the data is missing. This is not the same as being zero. So R is not sure how to calculate the mean since that last element is essentially a question mark. We can make use of an additional argument that the mean function takes, namely the na.rm argument. This argument functions more like a setting, where you can provide a flag (i.e. TRUE or FALSE) or a distinct option (e.g. top, bottom, left, or right) that modifies how it produces the output. This argument has a default that is shown in the “Usage” section of the documentation. We are going to change that setting (worksheet task 2.3.3B). mean(save.data, na.rm = TRUE) ## [1] 4 Now you can see that R runs the function and decides to leave out or ignore the NA value and is able to return an average based on the other values in save.data (worksheet task 2.3.3C). "],["working-with-vectors.html", "2.4 Working with vectors", " 2.4 Working with vectors The power of using programming like R is to be able to process, analyze, and visualize large sets of data. We will build our way up to thinking about tabular data like that in the iris dataset starting first with vectors. Remember that you can create vectors (an ordered list of elements) by combining elements with the c function. 2.4.1 Building vectors Let us pretend that we measured and recorded the resting heart rates of several patients. We will create these vectors and assign them to some variables (worksheet task 2.4.1A). heart.rates &lt;- c(78, 68, 95, 82, 69, 63, 86, 74, 64, 62) more.heart.rates &lt;- c(86, 79, 64, 74, 80) heart.rates ## [1] 78 68 95 82 69 63 86 74 64 62 more.heart.rates ## [1] 86 79 64 74 80 We have used the c function to combine several individual elements that we typed out, but you can combine vectors together to make an even bigger vector (worksheet task 2.4.1B). all.heart.rates &lt;- c(heart.rates, more.heart.rates) all.heart.rates ## [1] 78 68 95 82 69 63 86 74 64 62 86 79 64 74 80 Vectors can contain different datatypes besides numbers. We can also provide several character strings in a vector, like the names of our patients. patient.names &lt;- c(&quot;oakley&quot;, &quot;rashmi&quot;, &quot;kiran&quot;, &quot;eun&quot;, &quot;sasha&quot;, &quot;mattie&quot;) However, note that vectors cannot handle multiple different datatypes at once. If you try to provide a vector with multiple datatypes: mixed.data &lt;- c(&quot;oakley&quot;, 18, TRUE, &quot;eun&quot;, NA, 50) mixed.data ## [1] &quot;oakley&quot; &quot;18&quot; &quot;TRUE&quot; &quot;eun&quot; NA &quot;50&quot; You can see this displays these elements enclosed in quotes, to indicate that R has converted them all to the same datatype (i.e. character string). There are several helpful functions and an operator that can speed up your ability to generate vectors. The : operator quickly creates a numeric vector for a range of values. 1:4 ## [1] 1 2 3 4 The seq function can help you quickly create numeric vectors. The rep function can be used to build either numeric or character string vectors. Reading the documentation will tell you which arguments these functions take (worksheet tasks 2.4.1C and 2.4.1D). ?seq ?rep Here are examples of how they can be used: z &lt;- seq(0, 100, by = 20) z ## [1] 0 20 40 60 80 100 z &lt;- rep(&quot;A&quot;, times = 5) z ## [1] &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; 2.4.2 Indexing and subsetting vectors Indexing is a way to access into a vector (or a matrix or a data frame) and pull out certain elements. There are multiple ways to index into a vector, one of the easiest ways is to pull out an element based on its order/position in the grouping (its index). We use [] immediately after the name of the grouping (in this case a vector) to access into it. all.heart.rates[2] # pull out second heart rate measurement ## [1] 68 patient.names[3] # pull out third patient name ## [1] &quot;kiran&quot; You can also pull out several elements at a time. To do this, you provide a vector of numeric values within the parentheses. patient.names[2:3] # pull out 2nd and 3rd patient name ## [1] &quot;rashmi&quot; &quot;kiran&quot; patient.names[c(1, 3)] # pull out 1st and 3rd patient name ## [1] &quot;oakley&quot; &quot;kiran&quot; You can remove elements of a vector by using the same syntax of indexing, but instead put a negative sign in front of the index number (worksheet tasks 2.4.2A through 2.4.2C). patient.names[-1] ## [1] &quot;rashmi&quot; &quot;kiran&quot; &quot;eun&quot; &quot;sasha&quot; &quot;mattie&quot; There are times where we will want to pull out certain elements of a vector using something other than the position. What if we do not know where they are located, especially if the vector is very long? 2.4.3 Using logic and logicals You can also access elements in a vector that meet certain criteria using what we will call conditional logic. Logic will be a recurring idea in programming. Logic looks like evaluating whether something meets your criteria and evaluating it as either TRUE or FALSE (remember these words are special and represent a logical datatype). Let’s explore what it looks like to evaluate a statement. save.num &lt;- 7 save.num &lt; 8 ## [1] TRUE save.num &gt; 8 ## [1] FALSE save.num != 8 ## [1] TRUE save.num == 8 ## [1] FALSE You can use the standard comparison operators like &gt; or &lt; to check greater than or less than. You can also use == to check for equality or != to check that values are not equal. We can do the same thing with vectors, and it will be performed in a vectorized manner. That is, by default, R will evaluate each element in the vector to see if it meets the criteria (worksheet task 2.4.3A). For example, we can evaluate each heart rate in our original vector to see whether or not the value is less than 80 (worksheet tasks 2.4.3B and 2.4.3C). all.heart.rates &lt; 80 ## [1] TRUE TRUE FALSE FALSE TRUE TRUE FALSE TRUE TRUE TRUE FALSE TRUE ## [13] TRUE TRUE FALSE This series of flags with TRUE and FALSE can be used to access into a vector and will only return the elements where there is a TRUE (worksheet tasks 2.4.3D through 2.4.3F). all.heart.rates[all.heart.rates &lt; 80] ## [1] 78 68 69 63 74 64 62 79 64 74 In the context of bigger data analysis, we can use conditional logic to make choices between executing different sets of code. You can pair these conditional statements with an if/else statement that breaks up the code and only executes parts of the code where conditions are met. test.num &lt;- 30 if (test.num &lt; 10) { print(&quot;small&quot;) } else { print(&quot;big&quot;) } ## [1] &quot;big&quot; Here, the code within the brackets only runs if the condition is met. Since the statement next to the if evaluates as FALSE, it then only executes the code within the brackets of the else clause. 2.4.4 Modifying vectors Let’s look into how we change an existing vector. We can save data into a vector, access this data, but we will likely want to process it too! We can investigate an unknown vector using several functions, like the length function. As the name implies, it returns how many elements your vector contains. x &lt;- c(1, 3, 2) y &lt;- c(5, 4, 6) length(x) ## [1] 3 length(y) ## [1] 3 Let’s try modifying the values in an existing vector. Note that when you try to add values to a vector, it can be done in a pairwise manner or uniformly across the entire vector. Let’s see what it looks like when we have pairwise addition of two vectors of the same length. x + y ## [1] 6 7 8 Here, you can see that the first element of x is added to the first element of y to create a new value in the first element. The same happens with the second element and so forth (worksheet task 2.4.4A). You can perform mathematical operations on an entire numeric vector all at once. Here we take the vector of our heart rate data and add 1 to it, which will add 1 to each individual element in the entire vector (worksheet tasks 2.4.4B and 2.4.4C). all.heart.rates + 1 ## [1] 79 69 96 83 70 64 87 75 65 63 87 80 65 75 81 This is another example of a vectorized operation, where we perform a step automatically to all items in a collection of items (e.g. a vector). This works with operators like + or -, but also with certain functions. ?log For example, the log function calculates the log value of a number. If you provide it with a vector, it will do that calculation for each element, returning a vector of the same length. log(all.heart.rates) ## [1] 4.356709 4.219508 4.553877 4.406719 4.234107 4.143135 4.454347 4.304065 ## [9] 4.158883 4.127134 4.454347 4.369448 4.158883 4.304065 4.382027 This demonstrates how one can quickly transform or scale values in a dataset. For example, you might have taken the temperature of an experiment in Fahrenheit and using the power of vectorization, you can apply the same arithmetic steps to all of your measurements simultaneously to get the values in Celsius. Here is an additional example using the signif function, which returns your numeric values with the number of significant digits that you specify (worksheet task 2.4.4D). ?signif Here we can specify one significant figure to get only the tens digit (or the hundreds digit if there is a value over 100). signif(all.heart.rates, digits = 1) ## [1] 80 70 100 80 70 60 90 70 60 60 90 80 60 70 80 Any of these results can also be saved into a new vector name (e.g. heart.rates2) or we can save over the original variable name if we are fixing the data. "],["working-with-dataframes.html", "2.5 Working with dataframes", " 2.5 Working with dataframes We can now work our way from vectors to dataframes. The most common data format we will deal with in research is a dataframe format. A dataframe has data is stored in a tabular format with the rows generally referring to individual measurements (single patients, samples, cells, etc.) and the columns referring to parameters (genes, proteins, etc.) measured in each individual. Essentially a dataframe can be thought of a bunch of vectors lined up in columns or lined up in rows. We work with a dataframe instead of a matrix (another datatype in R) because dataframes can tolerate different datatypes in the same table. As seen below, a matrix will easily accept data all of the same datatype, but do unexpected things when you provide multiple datatypes. matrix1 &lt;- matrix(data = c(1, 2, 3, 4, 5, 6, 7, 8, 9), nrow = 3, ncol = 3) matrix1 ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 matrix2 &lt;- matrix(data = c(1, &quot;apple&quot;, 3, TRUE, &quot;cat&quot;, 6, 7, NA, FALSE), nrow = 3, ncol = 3) matrix2 ## [,1] [,2] [,3] ## [1,] &quot;1&quot; &quot;TRUE&quot; &quot;7&quot; ## [2,] &quot;apple&quot; &quot;cat&quot; NA ## [3,] &quot;3&quot; &quot;6&quot; &quot;FALSE&quot; Now matrix1 looks normal, but you may notice that matrix2 has quotation marks around its elements, including the numbers and the TRUE/FALSE values. This means these elements are all being treated like character strings because we included elements like apple and R wants them to be one consistent type. While we are working mainly with dataframes, tibbles will pop up as we work in tidyverse. Tibbles are very similar to dataframes in their ability to handle different data types across their different columns. You can think of them as very similar entities. We will first explore the built-in iris dataset. If you look up its documentation, you will notice that it is described as a dataframe and does contain both numeric values and character strings for the names of species. ?iris For the sake of this exercise, we will assign a new variable called iris.temp that is a shorter version of the original dataset. The head function returns just the first few rows and here we use an argument to request the first 10 (worksheet task 2.5.1A). iris.temp &lt;- head(iris, n = 10) iris.temp ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5.0 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa 2.5.1 Indexing and subsetting dataframes You can index into a dataframe and pull out one or more cells within the dataframe. Note that we are providing two coordinates to explain which row (the first number before the comma) and which column (the second number after the comma) to find the exact element (or cell in the table) (worksheet task 2.5.1B). iris.temp[1, 3] ## [1] 1.4 You can pull out multiple elements at a time, specifying which row and column they reside in. iris.temp[c(1, 2), c(2, 3)] # gives us the 2nd and 3rd columns of the 1st and 2nd rows ## Sepal.Width Petal.Length ## 1 3.5 1.4 ## 2 3.0 1.4 iris.temp[1:3, 3:5] # gives us the 3rd through 5th columns of the 1st through 3rd rows ## Petal.Length Petal.Width Species ## 1 1.4 0.2 setosa ## 2 1.4 0.2 setosa ## 3 1.3 0.2 setosa If you provide the row and not the column, or vice versa, by default R will pull out all of the available columns and rows respectively. iris.temp[1:2, ] # pulls out the first two rows and all columns ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa iris.temp[, 1:2] # pulls out the first two columns and all rows ## Sepal.Length Sepal.Width ## 1 5.1 3.5 ## 2 4.9 3.0 ## 3 4.7 3.2 ## 4 4.6 3.1 ## 5 5.0 3.6 ## 6 5.4 3.9 ## 7 4.6 3.4 ## 8 5.0 3.4 ## 9 4.4 2.9 ## 10 4.9 3.1 It’s possible to also remove elements in a dataframe using the negative sign (worksheet tasks 2.5.1C and 2.5.1D). iris.temp2 &lt;- iris.temp[-c(1, 3), ] # removes the first and third rows head(iris.temp) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa head(iris.temp2) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 2 4.9 3.0 1.4 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5.0 3.4 1.5 0.2 setosa We saved the shorter dataset into a new variable and when we preview iris.temp against iris.temp2 you may be able to see the missing rows (worksheet task 2.5.1E). 2.5.2 Exploring dataframes When you are first presented with a dataframe, for example data that was collected in your research lab that you are tasked with analyzing, you will want to learn more about it. There are a few different functions you can use to investigate a dataframe, the size of it, and other aspects. The dim function, short for dimensions, returns the number of rows and columns (worksheet task 2.5.2A). dim(iris) ## [1] 150 5 dim(iris.temp) ## [1] 10 5 This shows you that iris.temp is in fact just the first 10 rows of iris. You can use the following functions: str for structure, colnames for column names, and summary to investigate aspects of a given dataset (worksheet task 2.5.2B). colnames(iris) ## [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; &quot;Species&quot; str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## The str function reveals that the iris dataframe contains different data types. Specifcally, it mostly contains columns of numbers as well as a column of factors or categorical data referring to which species the iris belonged to. The summary function tries to tell us more info about each column. For numerical data, it summarizes the min and max values, the quartiles, and the center values (e.g. median or mean). For categorical data like the Species column, this function shows how many rows belong to each category (worksheet tasks 2.5.2C and 2.5.3D). We can try to look at the beginning of a specific column in this dataset to get a better understanding for it. Dataframes that have names for their columns allow you to index into the columns specifically by name using the $ operator as shown below. head(iris$Species) ## [1] setosa setosa setosa setosa setosa setosa ## Levels: setosa versicolor virginica Remember the head function lets us preview a longer set of data, either showing the first few elements of a vector or the first few rows of a dataframe. 2.5.3 Building and modifying dataframes We can also generate our own dataframes from vectors that we put together into a table. Revisiting our heart rate measurement example, let’s build a dataframe of patient data. patient.data &lt;- data.frame(name = c(&quot;oakley&quot;, &quot;rashmi&quot;, &quot;kiran&quot;), heart_rate = c(78, 68, 95), disease_status = c(FALSE, FALSE, TRUE)) patient.data ## name heart_rate disease_status ## 1 oakley 78 FALSE ## 2 rashmi 68 FALSE ## 3 kiran 95 TRUE We can inspect the dataframe we have created using the same functions. str(patient.data) ## &#39;data.frame&#39;: 3 obs. of 3 variables: ## $ name : Factor w/ 3 levels &quot;kiran&quot;,&quot;oakley&quot;,..: 2 3 1 ## $ heart_rate : num 78 68 95 ## $ disease_status: logi FALSE FALSE TRUE summary(patient.data) ## name heart_rate disease_status ## kiran :1 Min. :68.00 Mode :logical ## oakley:1 1st Qu.:73.00 FALSE:2 ## rashmi:1 Median :78.00 TRUE :1 ## Mean :80.33 ## 3rd Qu.:86.50 ## Max. :95.00 You can add new rows and columns using the rbind and cbind functions. Let’s pretend that we had collected additional information about our patients, such as their self-reported gender. We can add this as a new column (cbind short for bind column) (worksheet task 2.5.3A). patient.data &lt;- cbind(patient.data, gender = c(&quot;M&quot;, &quot;F&quot;, NA)) patient.data ## name heart_rate disease_status gender ## 1 oakley 78 FALSE M ## 2 rashmi 68 FALSE F ## 3 kiran 95 TRUE &lt;NA&gt; You can merge two dataframes together using the rbind function assuming that their columns match up correctly. Let’s pretend that we had another day at the clinic and collected additional patient measurements (worksheet task 2.5.3B). more.patients &lt;- data.frame(name = c(&quot;eun&quot;, &quot;sasha&quot;, &quot;mattie&quot;), heart_rate = c(86, 79, 64), disease_status = c(TRUE, TRUE, FALSE), gender = c(NA, &quot;M&quot;, &quot;F&quot;)) more.patients ## name heart_rate disease_status gender ## 1 eun 86 TRUE &lt;NA&gt; ## 2 sasha 79 TRUE M ## 3 mattie 64 FALSE F Let’s use rbind, short for binding rows, to add these additional rows to the bottom of our first dataframe patient.data (worksheet tasks 2.5.3C and 2.5.3D). all.patients &lt;- rbind(patient.data, more.patients) all.patients ## name heart_rate disease_status gender ## 1 oakley 78 FALSE M ## 2 rashmi 68 FALSE F ## 3 kiran 95 TRUE &lt;NA&gt; ## 4 eun 86 TRUE &lt;NA&gt; ## 5 sasha 79 TRUE M ## 6 mattie 64 FALSE F You can also remove rows and columns using the trick with a negative index. patient.data[, -1] # removes first column ## heart_rate disease_status gender ## 1 78 FALSE M ## 2 68 FALSE F ## 3 95 TRUE &lt;NA&gt; patient.data[-1, ] # removes first row ## name heart_rate disease_status gender ## 2 rashmi 68 FALSE F ## 3 kiran 95 TRUE &lt;NA&gt; 2.5.4 Handling datatypes in dataframes One of the great strengths of a dataframe is that it can handle each column containing different datatypes. Our patient data has columns of character strings, logicals, and numerical values. However, you should take care that sometimes unexpected behavior may arise when a column in your dataframe is one datatype and you add data that is not compatible with that datatype. Remember that vectors can only contain one datatype at a time? Each column in the dataframe is essentially a vector. We briefly discussed factors as categorical variables. Let’s pretend that for our analysis we wanted to treat gender as a categorical variable. Factors are a special datatype that deals with categorical data and can be handy for certain manipulations or visualizations. To do this, we can coerce data into a different datatype using functions like as.factor (worksheet tasks 2.5.4A and 2.5.4B). head(all.patients$gender) ## [1] M F &lt;NA&gt; &lt;NA&gt; M F ## Levels: F M all.patients$gender &lt;- as.factor(all.patients$gender) head(all.patients$gender) ## [1] M F &lt;NA&gt; &lt;NA&gt; M F ## Levels: F M This vector of factors shows which category each element belongs to, and then summarizes what the possible categories are down at the bottom where it prints the Levels of this factor. This can cause issues if we introduce data that does not match these categories. Let’s try adding a new patient’s data. all.patients &lt;- rbind(all.patients, c(name = &quot;lupe&quot;, heart_rate = 72, disease_status = FALSE, gender = &quot;NB&quot;)) ## Warning in `[&lt;-.factor`(`*tmp*`, ri, value = &quot;lupe&quot;): invalid factor level, NA ## generated ## Warning in `[&lt;-.factor`(`*tmp*`, ri, value = &quot;lupe&quot;): invalid factor level, NA ## generated all.patients ## name heart_rate disease_status gender ## 1 oakley 78 FALSE M ## 2 rashmi 68 FALSE F ## 3 kiran 95 TRUE &lt;NA&gt; ## 4 eun 86 TRUE &lt;NA&gt; ## 5 sasha 79 TRUE M ## 6 mattie 64 FALSE F ## 7 &lt;NA&gt; 72 FALSE &lt;NA&gt; What has happened with this new addition? If you tried to add a new patient to the dataframe that had a gender that wasn’t already represented in the data, chances are you had a warning and that gender was turned to NA. all.patients$gender ## [1] M F &lt;NA&gt; &lt;NA&gt; M F &lt;NA&gt; ## Levels: F M Once a factor is created, it doesn’t let you easily add new categories that were not in the original set. We will not get issues though if we add a new patient whose gender is represented as one of the levels in our gender factor. all.patients &lt;- rbind(all.patients, c(name = &quot;chihiro&quot;, heart_rate = 101, disease_status = TRUE, gender = &quot;M&quot;)) ## Warning in `[&lt;-.factor`(`*tmp*`, ri, value = &quot;chihiro&quot;): invalid factor level, ## NA generated all.patients$gender ## [1] M F &lt;NA&gt; &lt;NA&gt; M F &lt;NA&gt; M ## Levels: F M So how do we fix this? The easiest way around this is to treat the column as characters instead of as factors (worksheet task 2.5.4C). all.patients$gender &lt;- as.character(all.patients$gender) all.patients &lt;- rbind(all.patients, c(name = &quot;ayodele&quot;, heart_rate = 101, disease_status = TRUE, gender = &quot;NB&quot;)) ## Warning in `[&lt;-.factor`(`*tmp*`, ri, value = &quot;ayodele&quot;): invalid factor level, ## NA generated all.patients$gender ## [1] &quot;M&quot; &quot;F&quot; NA NA &quot;M&quot; &quot;F&quot; NA &quot;M&quot; &quot;NB&quot; There will be times that we want to treat certain columns in our data as a factor, but take care that you add compatible data to each column of your existing dataset. "],["day-4-tidyverse-and-visualizations.html", "3 Day 4 - tidyverse and visualizations", " 3 Day 4 - tidyverse and visualizations Learning Goals: By the end of today’s session, students will be able to: Read in and process data starting from a local saved file. Transform, scale, filter, and convert values within a dataset. Manipulate data frames using tidyverse piping and functions. Reshape a data frame so that it is “tidy” and appropriate for a given analysis. Describe the roles of data, aesthetics, and geoms in ggplot functions. Choose the correct aesthetics and alter the geom parameters for a scatter plot, histogram, or box plot. Customize plot scales, titles, subtitles, themes, fonts, layout, and orientation. Layer multiple geoms in a single plot, including error bars for data analysis. Save a ggplot to a local file. Analyze dataset using a variety of statistical approaches. Preparing for Today: Please make sure you have completed all readings and activities from Section 2: Day 3 - introduction to R. You will also have some time at the start of the day to do the first sections 3.1-3.2. Before you start reading through Section 3 for today or taking part in synchronous activities, download and open today’s worksheet. On this Github page, please right-click the button that says “Raw” and click “Save Link As…” to get a local copy of this Rmd file on your computer and open it in Rstudio. Today’s Schedule: Start Time End Time Activity Description Facilitator 10:00 AM PDT 10:45 AM PDT Asynchronous Work through sections 3.1-3.2 Margaret, Darach 10:45 AM PDT 11:30 AM PDT Synchronous Peer coding exercises on Zoom Margaret, Melissa, Samson, Zac, Darach 11:30 AM PDT 12:15 PM PDT Asynchronous Work through sections 3.4-3.5, 3.4.5 is optional Melissa, Margaret, Darach 12:15 PM PDT 12:45 PM PDT Lunch Break Take a computer break! N/A 12:45 PM PDT 1:30 PM PDT Synchronous Peer coding exercises on Zoom Melissa, Margaret, Zac 1:30 PM PDT 2:00 PM PDT Asynchronous Work through sections 3.6 Margaret, Samson, Darach 2:00 PM PDT 2:30 PM PDT Snack Break Take a computer break! N/A 2:30 PM PDT 3:30 PM PDT Synchronous Wrap-up session on Zoom Samson, Margaret, Zac "],["reading-and-processing-data.html", "3.1 Reading and processing data", " 3.1 Reading and processing data So far, we have largely used data that was already provided to us in base R or through the tidyverse packages. In reality, you will be working with data that comes from a file containing measurements collected by a researcher. 3.1.1 Loading a file into R There are multiple file types that can be read into R. The most common one is the csv file extension, though txt files also work. You can read in Excel files using additional packages such as the readxl library. Let’s practice reading in data using the read.csv function. ?read.csv The repository for this site has a folder called data that contains a file called iris.csv, which is simply the original iris dataset with some modifications. We read in this data and save it to a variable named irisz (worksheet task 3.1.1A). irisz &lt;- read.csv(&quot;data/iris.csv&quot;) head(irisz) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species Location ## 1 5.1 3.5 1.4 0.2 setosa Korea ## 2 4.9 3 1.4 0.2 setosa China ## 3 4.7 3.2 1.3 0.2 setosa Korea ## 4 4.6 3.1 1.5 0.2 setosa China ## 5 5 3.6 1.4 0.2 setosa China ## 6 5.4 1.7 0.4 setosa Canada You can already see there are some differences compared to the original iris dataset, such as missing data and an additional column of information. You can see there is also a difference in how certain columns are handled. head(iris$Species) ## [1] setosa setosa setosa setosa setosa setosa ## Levels: setosa versicolor virginica head(irisz$Species) ## [1] setosa setosa setosa setosa setosa setosa ## Levels: setosa versicolor virginica For example, here it is clear that our data we read from a csv file is not treating species as categorical information. We can toggle a setting called stringsAsFactors that will turn these character string inputs into factors when the file is loaded into R. irisz &lt;- read.csv(&quot;data/iris.csv&quot;, stringsAsFactors = TRUE) head(irisz$Species) ## [1] setosa setosa setosa setosa setosa setosa ## Levels: setosa versicolor virginica head(irisz$Location) ## [1] Korea China Korea China China Canada ## Levels: Canada China Japan Korea Russia USA If we did not want to reload our data from a file, we can also transform columns (i.e. vectors) in our dataframe to be different types. For example, we could use irisz$Species &lt;- as.factor(irisz$Species) to turn it into a factor after it has already been loaded. There are similar functions like as.numeric, as.character, and as.logical that you may use to transform data types. Be aware though that certain conversions may lead to issues and loss of data! Now both of these columns are being treated as categorical data. There are other settings we can change, such as providing different names for the columns (worksheet task 3.1.1B). irisz &lt;- read.csv(&quot;data/iris.csv&quot;, col.names = c(&quot;sep_len&quot;, &quot;sep_wid&quot;, &quot;pet_len&quot;, &quot;pet_wid&quot;, &quot;species&quot;, &quot;loc&quot;)) head(irisz) ## sep_len sep_wid pet_len pet_wid species loc ## 1 5.1 3.5 1.4 0.2 setosa Korea ## 2 4.9 3 1.4 0.2 setosa China ## 3 4.7 3.2 1.3 0.2 setosa Korea ## 4 4.6 3.1 1.5 0.2 setosa China ## 5 5 3.6 1.4 0.2 setosa China ## 6 5.4 1.7 0.4 setosa Canada 3.1.2 Handling issues in data Now that we have loaded in the data, we should preview it and look for any possible issues. You never know what can go wrong in the data pipeline: data can be recorded incorrectly or corrupted at any point in the process. str(irisz) ## &#39;data.frame&#39;: 150 obs. of 6 variables: ## $ sep_len: Factor w/ 36 levels &quot; &quot;,&quot;4.3&quot;,&quot;4.4&quot;,..: 10 8 6 5 9 13 5 9 3 8 ... ## $ sep_wid: Factor w/ 27 levels &quot;&quot;,&quot; &quot;,&quot;0.35&quot;,..: 18 13 15 14 19 2 17 17 12 14 ... ## $ pet_len: Factor w/ 46 levels &quot;&quot;,&quot; &quot;,&quot;1&quot;,&quot;1.1&quot;,..: 7 7 6 8 7 10 7 8 7 8 ... ## $ pet_wid: Factor w/ 25 levels &quot;&quot;,&quot;0.1&quot;,&quot;0.2&quot;,..: 3 3 3 3 3 5 4 3 3 2 ... ## $ species: Factor w/ 4 levels &quot;&quot;,&quot;setosa&quot;,&quot;versicolor&quot;,..: 2 2 2 2 2 2 2 1 2 2 ... ## $ loc : Factor w/ 7 levels &quot;&quot;,&quot;Canada&quot;,&quot;China&quot;,..: 5 3 5 3 3 2 3 3 6 4 ... summary(irisz) ## sep_len sep_wid pet_len pet_wid species ## 5 : 9 3 :26 1.4 :13 0.2 :29 : 2 ## 5.1 : 9 2.8 :14 1.5 :13 1.3 :13 setosa :49 ## 6.3 : 9 3.2 :13 4.5 : 8 1.5 :12 versicolor:50 ## 5.7 : 8 3.4 :12 5.1 : 8 1.8 :12 virginica :49 ## 6.7 : 8 3.1 :11 1.3 : 7 1.4 : 8 ## 5.5 : 7 2.7 : 9 1.6 : 6 0.3 : 7 ## (Other):100 (Other):65 (Other):95 (Other):69 ## loc ## :14 ## Canada:35 ## China : 6 ## Japan :11 ## Korea :10 ## Russia:11 ## USA :63 One immediate issue you can spot here is that all variables are being treated as character strings. We would expect that sepal length/width and petal length/width would be numeric variables. To inspect what might be happening, we can look at the unique elements in one of these columns using the unique function. unique(irisz$sep_len) ## [1] 5.1 4.9 4.7 4.6 5 5.4 4.4 4.8 4.3 5.8 5.7 5.2 5.5 4.5 n/a 5.3 7 6.4 6.9 ## [20] 6.5 6.3 6.6 5.9 6 6.1 5.6 6.7 6.2 6.8 7.1 7.6 7.2 7.7 7.4 7.9 ## 36 Levels: 4.3 4.4 4.5 4.6 4.7 4.8 4.9 5 5.1 5.2 5.3 5.4 5.5 5.6 5.7 ... n/a Looking at this, we can find the culprit for why these values are treated as a string. There are some entries that are simply a blank space \"\" and there are missing data encoded as \"n/a\" in this dataset. The read.csv function has a nice way to deal with this, specifically the argument na.strings that takes options for elements that might be present in the loaded data that should be treated as an NA (worksheet tasks 3.1.2A through worksheet task 3.1.2D). irisz &lt;- read.csv(&quot;data/iris.csv&quot;, na.strings = c(&quot;&quot;, &quot;n/a&quot;), stringsAsFactors = TRUE) str(irisz) ## &#39;data.frame&#39;: 150 obs. of 6 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 NA 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 NA 1 1 ... ## $ Location : Factor w/ 6 levels &quot;Canada&quot;,&quot;China&quot;,..: 4 2 4 2 2 1 2 2 5 3 ... summary(irisz) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :0.350 Min. :1.000 Min. : 0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.550 1st Qu.: 0.300 ## Median :5.800 Median :3.000 Median :4.300 Median : 1.300 ## Mean :5.839 Mean :3.028 Mean :3.752 Mean : 1.331 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.: 1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :23.000 ## NA&#39;s :2 NA&#39;s :3 NA&#39;s :3 NA&#39;s :2 ## Species Location ## setosa :49 Canada:35 ## versicolor:50 China : 6 ## virginica :49 Japan :11 ## NA&#39;s : 2 Korea :10 ## Russia:11 ## USA :63 ## NA&#39;s :14 Now we can see that the first columns are being properly treated as numeric vectors. Our next step would be to double-check our data for anything that looks anamolous. Let’s first take a look at the distributions of our numerical data using the base R hist function, which plots a simple histogram (worksheet task 3.1.2E). hist(irisz$Sepal.Length) hist(irisz$Sepal.Width) From these figures, we can spot that there is one very low value for sepal width that we may want to investigate. hist(irisz$Petal.Length) hist(irisz$Petal.Width) From these figures, we see there is a very high value for petal width that we may want to investigate. It is possible that these values somehow got changed or corrupted during the recording process. We can filter out these anomalous pieces of data using conditional logic that we learned about when indexing and subsetting into a vector. Let’s start by filtering out rows that have a petal width more than 5. dim(irisz) ## [1] 150 6 irisz &lt;- irisz[irisz$Petal.Width &lt; 5, ] dim(irisz) ## [1] 149 6 Note that we used the statement irisz$Petal.Width &lt; 5 that evaluates as TRUE or FALSE for every row. We then indexed into the irisz dataframe using brackets [] and selected all rows that evaluate as TRUE. This led to one row being removed. We can repeat this with the anamolous value for sepal width (worksheet task 3.1.2F). dim(irisz) ## [1] 149 6 irisz &lt;- irisz[irisz$Sepal.Width &gt; 1, ] dim(irisz) ## [1] 148 6 Now that we have removed observations that looked strange by eye inspection, we may have only some final pre-processing steps to do. Depending on what downstream analysis we want to conduct, we may want to standardize or normalize our numeric values. R comes with an existing scale function that we can use to standardize between values with different ranges and orders of magnitude. ?scale The scale function will by default scale and center a numeric vector, but you can choose to do only one of these steps using argument settings. x &lt;- 1:10 scale(x) ## [,1] ## [1,] -1.4863011 ## [2,] -1.1560120 ## [3,] -0.8257228 ## [4,] -0.4954337 ## [5,] -0.1651446 ## [6,] 0.1651446 ## [7,] 0.4954337 ## [8,] 0.8257228 ## [9,] 1.1560120 ## [10,] 1.4863011 ## attr(,&quot;scaled:center&quot;) ## [1] 5.5 ## attr(,&quot;scaled:scale&quot;) ## [1] 3.02765 scale(x, center = FALSE) ## [,1] ## [1,] 0.1528942 ## [2,] 0.3057883 ## [3,] 0.4586825 ## [4,] 0.6115766 ## [5,] 0.7644708 ## [6,] 0.9173649 ## [7,] 1.0702591 ## [8,] 1.2231533 ## [9,] 1.3760474 ## [10,] 1.5289416 ## attr(,&quot;scaled:scale&quot;) ## [1] 6.540472 scale(x, scale = FALSE) ## [,1] ## [1,] -4.5 ## [2,] -3.5 ## [3,] -2.5 ## [4,] -1.5 ## [5,] -0.5 ## [6,] 0.5 ## [7,] 1.5 ## [8,] 2.5 ## [9,] 3.5 ## [10,] 4.5 ## attr(,&quot;scaled:center&quot;) ## [1] 5.5 We can scale the columns in our dataset and create a more standardized range of values for each column. # before scaling hist(irisz$Sepal.Length) hist(irisz$Petal.Width) # after scaling hist(scale(irisz$Sepal.Length)) hist(scale(irisz$Petal.Width)) The shapes of our distributions for these measurements is not fundamentally different, but if you look along the x-axis, you will notice that the values (especially the highest and lowest) are more similar between these two columns after they are both scaled. You may also consider rescaling values or normalizing them to all be between 0 and 1. There is a function for this called rescale that comes with the scales library. library(scales) rescale(x) ## [1] 0.0000000 0.1111111 0.2222222 0.3333333 0.4444444 0.5555556 0.6666667 ## [8] 0.7777778 0.8888889 1.0000000 "],["tidyverse-12-functions-pipes.html", "3.2 Tidyverse 1/2: functions &amp; pipes", " 3.2 Tidyverse 1/2: functions &amp; pipes Goal: Understand the tidyverse framework of pipes and functions, as well as how to apply it to tidy data frame cleaning and manipulation. 3.2.1 Welcome to the tidyverse! Tidyverse is a suite of R packages that are very useful for data science when we need to manipulate data frames and visualize data. The most important packages in tidyverse for our workshop will be dplyr and ggplot2. It builds on the base R functions and data types we’ve studied so far. It just provides a different design framework for working with data in R. The main two features of tidyverse are A new suite of very useful, intuitive functions The pipe %&gt;% which will allow us to go send data from one function to the next without using intermediate steps or nesting functions together 3.2.1.1 Preview: new functions in tidyverse Let’s say we have a dataframe, df, which has the name and age (in years) of everyone in a class. We want to create a new variable of age in months. In base R, we would do it this way: df$age_months &lt;- df$age_years * 12 In tidyverse, we can use the function mutate() to create a new variable. As with most tidyverse functions, mutate() takes the dataset as the first argument and the operation as the second. Here the operation is to create the new variable. mutate(df, age_months = age_years * 12) As you will see with more complex examples, these functions can make dataset operations more readable, especially when strung together with a pipe. 3.2.1.2 Preview: the pipe in tidyverse Recall the head() function used in the previous section. If you have a data frame, df, instead of peaking at the first few lines with head(df), you can pipe it into the head() function like this: df %&gt;% head(). This might seem like more work than just calling the function around the data frame object, but when you need to run a bunch of functions on a data frame, it becomes much TIDIER! In the previous example, we could call the function using a pipe like so: df %&gt;% mutate(age_months = age_years * 12) Feeling motivated to accelerate data cleaning and analysis using the tidyverse? 3.2.2 Load tidyverse Load the tidyverse library library(tidyverse) If tidyverse is already installed, loading the library should not return an error message. It is ok if it says there are package conflicts - this happens when different packages that are loaded have functions with the same name (but probably do different things). Just take note of the conflicts. If you use those other packages/functions then you’ll have to specify which package the function you’re using comes from. For example if mutate() was also in another package, to use the tidyverse version I would write tidyr::mutate() instead of just mutate() If loading the library returns an error message saying it’s not installed, then go to the SSRP Pre-installation guide and follow the instructions for installing tidyverse 3.2.3 How to pipe %&gt;% The tidyverse pipe can be used with most base R functions and with all tidyverse-specific functions, which we’ll learn more about soon. Although we’ll focus on using tidyverse pipe and functions on data frames, the pipe can also be used on vectors and even scalars. 3.2.3.0.1 Pipes with dataframes Let’s read in the iris dataset used in the previous section. Peak at it with head() to check that the columns look intact. # Load the dataset and peak at it with the head() function irisz &lt;- read.csv(&quot;data/iris.csv&quot;, na.strings = c(&quot;&quot;, &quot;n/a&quot;), stringsAsFactors = TRUE) head(irisz) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species Location ## 1 5.1 3.5 1.4 0.2 setosa Korea ## 2 4.9 3.0 1.4 0.2 setosa China ## 3 4.7 3.2 1.3 0.2 setosa Korea ## 4 4.6 3.1 1.5 0.2 setosa China ## 5 5.0 3.6 1.4 0.2 setosa China ## 6 5.4 NA 1.7 0.4 setosa Canada Now let’s try this using the pipe %&gt;%. irisz %&gt;% head() ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species Location ## 1 5.1 3.5 1.4 0.2 setosa Korea ## 2 4.9 3.0 1.4 0.2 setosa China ## 3 4.7 3.2 1.3 0.2 setosa Korea ## 4 4.6 3.1 1.5 0.2 setosa China ## 5 5.0 3.6 1.4 0.2 setosa China ## 6 5.4 NA 1.7 0.4 setosa Canada Practice this with other functions we’ve used so far: summary(), unique(), str(), dim(), table(), and even hist()!. Do it with and without the pipe. Pretty simple right? Notice how the %&gt;% pipe is sending the output from before it to the function after it. This %&gt;% is similar to the | we use on the command line to send data from one operation to the next: sort data.txt | uniq -c (sort a data file and count the number of unique lines) Pipes can string together functions In the previous section, we learned how to read in a file with read.csv(), get unique values with unique(), and get the dimensions of a data frame with dim(). Let’s try these operations using the pipe instead of performing them as separate operations. irisz %&gt;% unique() %&gt;% dim() ## [1] 150 6 Notice how each operation is on one line so it’s easy to read the sequence of operations. Pipes with scalars To take the square root of a number x and round it to y digits, we would use the sqrt() and round() functions Here’s what this looks like with and without the pipe: x = 112 y = 2 # Two steps without the pipe z &lt;- sqrt(x) round(z, digits = y) ## [1] 10.58 # One step without the pipe round(sqrt(x), digits = y) ## [1] 10.58 # One step with the pipe sqrt(x) %&gt;% round(digits = y) ## [1] 10.58 Pipes with vectors We can apply the same piping method to vectors. Think about the vector that is the column Sepal.Length in the dataframe irisz which we have been using. We’ll use pipes to string together the following operations starting with the vector irisz$Sepal.Length: sort it, make the values numeric, and make a histogram. (These three functions were also introduced in the previous section.) # One step with the pipe irisz$Sepal.Length %&gt;% as.numeric() %&gt;% sort() %&gt;% hist() # One step without the pipe requires lots of parentheses hist(sort(as.numeric(irisz$Sepal.Length))) Hopefully by now you have a sense for how the pipe in tidyverse works and how it can work with functions we already know and can be applied to data frames, vectors, and scalars. Another big contribution tidyverse makes to the data science in R is the addition of new functions! 3.2.4 Essential tidyverse functions Let’s start with the essential functions of tidyverse. All of these are from the sub-package dplyr select(): subset columns filter(): subset rows on conditions arrange(): sort results mutate(): create new columns by using information from other columns group_by() and summarize(): create summary statistics on grouped data count(): count discrete values Reminder: if you ever want to know more about these functions, just use the help function - ? followed by the function of interest. For example: ?select 3.2.4.1 Selecting, deleting, reordering columns Selecting columns In base R, if we wanted to select certain columns (e.g. column names c1, c2, c6, c7) from a dataframe, df, we would do something like this: df[c(\"c1\", \"c2\", \"c6\", \"c7\")]. With tidyverse, we can use the select(c1, c2, c6, c7) function (from dplyr). Deleting columns To delete a column, just add a - before the column name inside of select(). Reordering columns Select can also be used to reorder columns. Just specify the order of columns in select(). Selecting and deselecting in the irisz dataset # Let&#39;s select just the location select(irisz, Location) # Remove the Location column select(irisz, -Location) Selecting multiple columns Let’s select both the Location and Species columns and see what unique combinations are present in the dataset unique(select(irisz, c(Location, Species))) ## Location Species ## 1 Korea setosa ## 2 China setosa ## 6 Canada setosa ## 8 China &lt;NA&gt; ## 9 Russia setosa ## 10 Japan setosa ## 46 &lt;NA&gt; setosa ## 51 &lt;NA&gt; versicolor ## 54 USA versicolor ## 61 Canada versicolor ## 101 USA virginica ## 107 &lt;NA&gt; virginica ## 143 &lt;NA&gt; &lt;NA&gt; # Note that the order returned is the order specified in select() # Originally Location came after Species, but now they are reordered. We can also do this in with the pipe. irisz %&gt;% select(Location, Species) %&gt;% unique() ## Location Species ## 1 Korea setosa ## 2 China setosa ## 6 Canada setosa ## 8 China &lt;NA&gt; ## 9 Russia setosa ## 10 Japan setosa ## 46 &lt;NA&gt; setosa ## 51 &lt;NA&gt; versicolor ## 54 USA versicolor ## 61 Canada versicolor ## 101 USA virginica ## 107 &lt;NA&gt; virginica ## 143 &lt;NA&gt; &lt;NA&gt; Note that select() subsets the dataframe to the specified columns. The output is a data frame (not a vector), even if only one column is selected What if we want to extract one column as a vector? In base R, we would use the $ operation to select a single column as a vector. With tidyverse, we can use the pull() function (from dplyr). Notice that pull() will only accept a single column name and will return a vector # Base R irisz$Location # With the pipe and tidyverse function pull() irisz %&gt;% pull(Location) 3.2.4.2 Filtering dataframes The basics of filtering data frames in tidyverse are the filter() function and boolean operators (&gt;, &lt;, ==) learned in section 2.4.3. Let’s try two examples: # Filter for Petal.Length greater than x x = 7 irisz %&gt;% filter(as.numeric(Sepal.Length) &gt; x) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species Location ## 1 7.1 3.0 5.9 2.1 virginica USA ## 2 7.6 3.0 6.6 2.1 virginica USA ## 3 7.2 3.6 6.1 2.5 virginica USA ## 4 7.7 3.8 6.7 2.2 virginica USA ## 5 7.7 2.6 6.9 23.0 virginica USA ## 6 7.7 2.8 6.7 2.0 virginica USA ## 7 7.2 3.2 6.0 1.8 virginica USA ## 8 7.2 3.0 5.8 1.6 virginica USA ## 9 7.4 2.8 6.1 1.9 virginica USA ## 10 7.9 3.8 6.4 2.0 virginica USA ## 11 7.7 3.0 6.1 2.3 virginica USA # Filter to get observations of setosa species from Korea irisz %&gt;% filter(Species == &quot;setosa&quot;, Location == &quot;Korea&quot;) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species Location ## 1 5.1 3.50 1.4 0.2 setosa Korea ## 2 4.7 3.20 1.3 0.2 setosa Korea ## 3 4.8 3.00 1.4 0.1 setosa Korea ## 4 5.4 3.90 1.3 0.4 setosa Korea ## 5 5.1 3.50 1.4 0.3 setosa Korea ## 6 5.1 3.80 1.5 0.3 setosa Korea ## 7 5.1 3.70 1.5 0.4 setosa Korea ## 8 5.5 0.35 1.3 0.2 setosa Korea ## 9 4.4 3.00 1.3 0.2 setosa Korea ## 10 NA NA 1.6 0.6 setosa Korea 3.2.4.3 Creating and renaming columns Creating columns in tidyverse is simple with the mutate() function. Let’s say we wanted to create a new column for the irisz dataset which has the row number. In base R, we would do something like irisz$row_number &lt;- seq(1,nrow(irisz)). Here, seq() creates a sequence of numbers from 1 to the number of rows in irisz. In tidyverse we can do it using the mutate() and row_number() functions. So it doesn’t output the whole data frame, we’ll pipe it directly into head() irisz %&gt;% mutate(row_number = row_number()) %&gt;% head() ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species Location row_number ## 1 5.1 3.5 1.4 0.2 setosa Korea 1 ## 2 4.9 3.0 1.4 0.2 setosa China 2 ## 3 4.7 3.2 1.3 0.2 setosa Korea 3 ## 4 4.6 3.1 1.5 0.2 setosa China 4 ## 5 5.0 3.6 1.4 0.2 setosa China 5 ## 6 5.4 NA 1.7 0.4 setosa Canada 6 See the row number column? Mutate can also be used to update a column without creating a new one. # Change sepal length to be 10x what it currently is irisz %&gt;% mutate(Sepal.Length = 10*Sepal.Length ) %&gt;% head() ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species Location ## 1 51 3.5 1.4 0.2 setosa Korea ## 2 49 3.0 1.4 0.2 setosa China ## 3 47 3.2 1.3 0.2 setosa Korea ## 4 46 3.1 1.5 0.2 setosa China ## 5 50 3.6 1.4 0.2 setosa China ## 6 54 NA 1.7 0.4 setosa Canada What if all the setosa measurements were actually supposed to be the virginica species? Let’s use the ifelse() function which accepts a conditional test, value if true, value if false. irisz %&gt;% # If Species is setosa, change it to virginica, otherwise use the value in Species mutate(Species = ifelse(Species == &quot;setosa&quot;, &quot;virginica&quot;, Species )) %&gt;% head() ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species Location ## 1 5.1 3.5 1.4 0.2 virginica Korea ## 2 4.9 3.0 1.4 0.2 virginica China ## 3 4.7 3.2 1.3 0.2 virginica Korea ## 4 4.6 3.1 1.5 0.2 virginica China ## 5 5.0 3.6 1.4 0.2 virginica China ## 6 5.4 NA 1.7 0.4 virginica Canada Renaming columns Sometimes, we just need to rename an existing column. Use rename() to specify the new and old column names, rename(new_name = old_name). Let’s rename the location column to be “Country”. Let’s just pipe it directly into colnames() which will give us the column names of the data frame. irisz %&gt;% rename(Country = Location) %&gt;% colnames() ## [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; &quot;Species&quot; ## [6] &quot;Country&quot; Want to change row names into a column or a column into a row name? Check out rownames_to_column() and column_to_rownames(). 3.2.4.4 Summarizing data Summarizing data is very important in data analysis. It allows you to get a big picture overview of a new dataset: type, range of values, etc. These are also types of summaries to think about including when in the exploratory stages or a project: before running analyses on a dataset, create a summary of the dataset. Present this during your update meetings for your summer project and keep these summaries on hand in case you or others have questions about the dataset throughout the course of your project. For summarizing data in tidyverse, let’s explore the functions count() group_by() summarize() The function count() does what it sounds like - it counts the number of unique observations for the specified column(s) Count on one variable in the data frame irisz %&gt;% count(Location) ## Location n ## 1 Canada 35 ## 2 China 6 ## 3 Japan 11 ## 4 Korea 10 ## 5 Russia 11 ## 6 USA 63 ## 7 &lt;NA&gt; 14 Count on more than one variable in the data frame # Count the number of observations for each combination # of location and species irisz %&gt;% count(Location, Species) ## Location Species n ## 1 Canada setosa 11 ## 2 Canada versicolor 24 ## 3 China setosa 5 ## 4 China &lt;NA&gt; 1 ## 5 Japan setosa 11 ## 6 Korea setosa 10 ## 7 Russia setosa 11 ## 8 USA versicolor 20 ## 9 USA virginica 43 ## 10 &lt;NA&gt; setosa 1 ## 11 &lt;NA&gt; versicolor 6 ## 12 &lt;NA&gt; virginica 6 ## 13 &lt;NA&gt; &lt;NA&gt; 1 Can you think of another (perhaps longer) way to count using other functions, such as select(), unique(), n(), pull(), or table()? Summarizing WITHIN groups What if we want to do more than just count the number of occurrences for a given group? group_by takes a variable and groups the data by that variable. Anything that happens after that is only calculated within the group variable. To turn off grouping, use ungroup(). It’s always good practice to ungroup() after you’re done grouping. Although it doesn’t matter if you won’t be doing any more operations on the dataset. # Here&#39;s an example that produces a similar result # as in the count() example above irisz %&gt;% group_by(Location) %&gt;% summarize(n = n()) %&gt;% ungroup() ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 7 x 2 ## Location n ## &lt;fct&gt; &lt;int&gt; ## 1 Canada 35 ## 2 China 6 ## 3 Japan 11 ## 4 Korea 10 ## 5 Russia 11 ## 6 USA 63 ## 7 &lt;NA&gt; 14 Try taking away the ungroup() and see if it changes the result. Let’s build on this. We also want to calculate the mean() and sd() for a new variable called size for each location. Size is going to be length * width of the sepal. Our process is: Create a new column for size using mutate() Group the data by Location using group_by() Summarize the mean and standard deviation of the group using summarize() # Since mean() and sd() require numeric inputs, # We&#39;ll also tell mean() and sd() to ignore NAs in the calculation irisz %&gt;% mutate(size = as.numeric(Sepal.Length) * as.numeric(Sepal.Width)) %&gt;% group_by(Location) %&gt;% summarize(n = n(), mean_size = mean(Petal.Length, na.rm = T), mean_sd = sd(Petal.Length, na.rm = T)) %&gt;% ungroup() ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 7 x 4 ## Location n mean_size mean_sd ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Canada 35 3.39 1.30 ## 2 China 6 1.52 0.194 ## 3 Japan 11 1.43 0.205 ## 4 Korea 10 1.4 0.105 ## 5 Russia 11 1.48 0.218 ## 6 USA 63 5.21 0.826 ## 7 &lt;NA&gt; 14 4.51 0.990 For a further challenge, let’s clean up this table. Arrange the data so the countries with largest number of observations are at the top. Also, remove observations with no location. 3.2.4.5 Recap: Tidyverse vs Base R To review, tidyverse is just another way to code in R with new functions and a framework based on the pipe %&gt;%. You can always use a combination of both base R functions and tidyverse functions. There are some great resources on tidyverse functions. Here are a couple: Beginner’s tidyverse cheat sheet. The dplyr section includes the functions we focused on today. The ggplot2 section is what we’ll go over in the next section. Introduction to tidyverse includes a more comprehensive overview of tidyverse, from reading in data to applying essential functions (what we did today) to more advanced topics such as reshaping data frames. "],["tidyverse-22-reshaping-data.html", "3.3 Tidyverse 2/2: reshaping data", " 3.3 Tidyverse 2/2: reshaping data Goal: Understand when reshaping a dataframe is necessary and how to do it using tidyverse functions gather() and spread() 3.3.1 Why tidy data So far we’ve used two datasets: irisz and msleep. Even though there is some missing data in these datasets, for the most part, they are quite neat. Notice how each line represents a single observation and each column represents a single variable. There is no redundancy in the rows or columns. Sometimes datasets will have columns with columns that mean the same thing but apply to only certain subsets of the data. Columns like this can be collapsed. On the other hand, sometimes data will be recorded together in the same column, when they are in fact two different types of data. Imagine if conservation status in the msleep dataset were appended to the name. If we wanted to perform an analysis based on conservation status, we would need to tidy that up by making sure it’s in a separate columns. 3.3.2 How to tidy data The reality is that datasets don’t always come in neat format. Before and during data analysis, we will often need to “tidy” datasets. tidyverse has 4 important functions for tidying data: gather(): change data from a wide to long format - i.e. reduce extra columns spread(): change data from a long to wide format - i.e. expand a column into multiple columns separate(): spearate a column into 2 or more columns based on a pattern or delimeter unite(): unite multiple columns into 1 In lieu of re-inventing the wheel, here’s an excellent explanation and illustration of the concept of tidy data and how to apply these functions to make data tidy. Read this excerpt from Data Science in R by Garret Grolemand https://garrettgman.github.io/tidying/ For the asynchronous portion, let’s apply these principles and functions to a new dataset. I’ll set up a few example datasets based on the original. Answer the questions starting from the appropriate example dataset (e.g. df_1, df_2, df_3). 3.3.3 Data exploration with tidyverse The dataset: New York Times COVID-19 cases Even though we’re just using this dataset to learn about tidyverse, it’s always important to read about the dataset. Downloading data can be difficult and we don’t want to dive into that if the data is not what we want. So always read about it first! Read the introduction on Github. Based on the description of the dataset, try to answer these questions: (no need to look at the actual data, the description should be sufficient) Why was this data collected? When was the data last updated? What kind of data is it? Is it genetic data, imaging data, text data? Is it descriptive, numeric, categorical? How was the data collected? One or many sources? Are there imperfections in the data? Limitations? E.g. missing data, different data meanings? We’ll read in the data straight from github. This is a lot of data (every county in the U.S.) so to make it more manageable downstream, we’ll filter the dataset to only include: A few states: California, Washington, Illinois, Nebraska, and Florida. Feel free to add your own state! The last total count of cases and deaths ## ── Attaching packages ───────────────────────────────────────────────────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.3.2 ✔ purrr 0.3.4 ## ✔ tibble 3.0.4 ✔ dplyr 1.0.2 ## ✔ tidyr 1.1.2 ✔ stringr 1.4.0 ## ✔ readr 1.4.0 ✔ forcats 0.5.0 ## ── Conflicts ──────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() data_url = &quot;https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv&quot; favorite_states = c(&quot;California&quot;, &quot;Washington&quot;, &quot;Illinois&quot;, &quot;Nebraska&quot;, &quot;Florida&quot;) df &lt;- read.csv(data_url, header = T) %&gt;% filter(state %in% favorite_states) %&gt;% mutate(date = as.Date(date,format=&quot;%Y-%m-%d&quot;)) %&gt;% filter(date == max(date)) Test your knowledge! Use functions we learned in the last section to answer these questions: What is the last date for recorded cases? Is it the same for all states/counties? Are all of the states listed in favorite_states in the dataset? How many counties are there for each of the states? 3.3.3.1 Example datasets Use the following three datasets to complete the coding exercises in today’s worksheet. The datasets are based on the New York Times Data, which is loaded above. Example Dataset #1:df_1 Three columns: county, cases, deaths df_1 &lt;- df %&gt;% tidyr::gather(key = &quot;metric&quot;, value = &quot;count&quot;, c(cases, deaths)) %&gt;% arrange(state,county) head(df_1) ## date county state fips metric count ## 1 2021-06-13 Alameda California 6001 cases 89408 ## 2 2021-06-13 Alameda California 6001 deaths 1719 ## 3 2021-06-13 Alpine California 6003 cases 89 ## 4 2021-06-13 Alpine California 6003 deaths 0 ## 5 2021-06-13 Amador California 6005 cases 3720 ## 6 2021-06-13 Amador California 6005 deaths 47 Example Dataset #2:df_2 Five columns: date, fips, metric corresponding to count, count, location (county, state) df_2 &lt;- df %&gt;% tidyr::gather(key = &quot;metric&quot;, value = &quot;count&quot;, c(cases, deaths)) %&gt;% mutate(county_state_fips = paste0(county,&quot;,&quot;, state, &quot;,&quot;, fips)) %&gt;% select(-county,-state) head(df_2) ## date fips metric count county_state_fips ## 1 2021-06-13 6001 cases 89408 Alameda,California,6001 ## 2 2021-06-13 6003 cases 89 Alpine,California,6003 ## 3 2021-06-13 6005 cases 3720 Amador,California,6005 ## 4 2021-06-13 6007 cases 12576 Butte,California,6007 ## 5 2021-06-13 6009 cases 2195 Calaveras,California,6009 ## 6 2021-06-13 6011 cases 2276 Colusa,California,6011 Example Dataset #3:df_3 A column with the county name, followed by columns for each state with the number of cases for that state/county pair. df_3 &lt;- df %&gt;% select(county,state,cases) %&gt;% tidyr::spread(key = &quot;state&quot;, value = &quot;cases&quot;) head(df_3) ## county California Florida Illinois Nebraska Washington ## 1 Adams NA NA 8654 3181 2212 ## 2 Alachua NA 25293 NA NA NA ## 3 Alameda 89408 NA NA NA NA ## 4 Alexander NA NA 473 NA NA ## 5 Alpine 89 NA NA NA NA ## 6 Amador 3720 NA NA NA NA Use these three datasets to complete the coding exercises in today’s worksheet "],["making-plots-with-ggplot2.html", "3.4 Making plots with ggplot2", " 3.4 Making plots with ggplot2 We will primarily be working in ggplot2 as it has the greatest degree of customization for visualization and offers many additional features over the basic plotting in R. library(ggplot2) library(tidyverse) 3.4.1 Getting started with a ggplot Most ggplot calls to create a figure take the following form (you can read more using help(ggplot)): ggplot(data = &lt;DATA&gt;, mapping = aes(&lt;MAPPINGS&gt;)) + &lt;GEOM_FUNCTION&gt;() We will practice using our mammalian sleep dataset. You can look up more info about this dataset using the help function and the dataset name, msleep. head(msleep) ## # A tibble: 6 x 11 ## name genus vore order conservation sleep_total sleep_rem sleep_cycle awake ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Chee… Acin… carni Carn… lc 12.1 NA NA 11.9 ## 2 Owl … Aotus omni Prim… &lt;NA&gt; 17 1.8 NA 7 ## 3 Moun… Aplo… herbi Rode… nt 14.4 2.4 NA 9.6 ## 4 Grea… Blar… omni Sori… lc 14.9 2.3 0.133 9.1 ## 5 Cow Bos herbi Arti… domesticated 4 0.7 0.667 20 ## 6 Thre… Brad… herbi Pilo… &lt;NA&gt; 14.4 2.2 0.767 9.6 ## # … with 2 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt; You will first use the ggplot() function and bind the plot to a specific data frame using the data argument. ggplot(data = msleep) You will next need to define a mapping (using the aesthetic or aes function), by selecting the variables to be plotted and specifying how to present them in the graph, e.g. as x/y positions or characteristics such as size, shape, color, etc. ggplot(data = msleep, aes(x = brainwt, y = sleep_rem)) You can then add “geoms” or graphical representations of the data in the plot (points, lines, bars). ggplot2 offers many different geoms including: geom_point() for scatter plots, dot plots, etc. geom_boxplot() for, well, boxplots! geom_line() for trend lines, time series, etc. To add a geom to the plot use the + operator. To plot using two continuous variables, we will use geom_point() first. To save your work-in-progress, you can assign the plot to a variable. First we establish our coordinate system. my.plot &lt;- ggplot(data = msleep, aes(x = brainwt, y = sleep_rem)) We can now draw the plot as a scatterplot with points to represent each mammal’s measurements from the msleep dataset (worksheet task 3.4.1A). my.plot + geom_point() ## Warning: Removed 35 rows containing missing values (geom_point). You might notice that all of the points are squished against the y-axis since many of the mammals in this dataset have low brain weights. summary(msleep$brainwt) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.00014 0.00290 0.01240 0.28158 0.12550 5.71200 27 As you can see with the summary function, the minimum and median values are very low, but there are a few mammals with high brainwt as you can see by the much larger maximum value in this vector. To make more useful plots, we can transform this value using log-scaling. While we will have to note that the new values do not exactly match the real-world measurements anymore, patterns we see (i.e. something that correlates with higher brain weights) will still hold true. msleep2 &lt;- msleep %&gt;% mutate(brainwt_log = log(brainwt)) ggplot(msleep2, aes(x = brainwt_log, y = sleep_rem)) + geom_point() ## Warning: Removed 35 rows containing missing values (geom_point). Here we use the mutate function to make a new variable called brainwt_log in our dataset (technically a new dataset copy that we have saved as msleep2). Plotting this variable as our x variable (i.e. independent variable), makes it easier to look for patterns (worksheet tasks 3.4.1B and 3.4.1C). 3.4.2 Changing plot aesthetics We can modify the appearance of the plot in two ways: either uniformly changing the appearance or having the appearance vary depending on information present in our data. Let’s explore how to modify our plots uniformly. We can change aspects of the points we plot such as transparency (“alpha”) and color by supplying them as arguments in the geom_point function. ggplot(data = msleep2, aes(x = brainwt_log, y = sleep_rem)) + geom_point(alpha = 0.5, color = &quot;blue&quot;) ## Warning: Removed 35 rows containing missing values (geom_point). Here we have made the plots semi-transparent and colored blue. You can try varying these values (e.g. change blue to a different color). You can also supply other arguments such as shape to use something other than a dot (worksheet task 3.4.2A). However, it is also possible to scale the color of the points by some variable present in the data. This approach means we can create a scatterplot that conveys more than just two variables’ worth of information (x-axis and y-axis) by having the color reflect a third variable. To do this, we specify the color inside the aesthetic mapping aes within the initial ggplot function. Same as how we told R to use a specific column by name for x or y coordinates, we specify which column to use for color (worksheet task 3.4.2B). ggplot(msleep2, aes(x = brainwt_log, y = sleep_rem, color = vore)) + geom_point() ## Warning: Removed 35 rows containing missing values (geom_point). This plot conveys not only the relationship between brainwt_log and sleep_rem, but each plot representing a different mammal now conveys what the feeding behavior of that mammal is. When generating visualizations, it is important to annotate the figure with meaningful labels on the axes to make them accessible for the viewer. For that, we can use the labs function (worksheet task 3.4.2C). ggplot(msleep2, aes(x = brainwt_log, y = sleep_rem, color = vore)) + geom_point() + labs(x = &quot;Brain Weight (log)&quot;, y = &quot;Duration of REM Sleep&quot;, color = &quot;Feeding Behavior&quot;) ## Warning: Removed 35 rows containing missing values (geom_point). 3.4.3 Exploring simple plots Let’s consider how we make other plots besides a scatterplot. Scatterplots are a great way to look at two quantitative (numerical) values at the same time to observe patterns (i.e. correlations) between the variables or to identify interesting outliers. However, other plots may be more useful to look at differing numbers of variables (i.e. one quantitative variable) or different types of variables (i.e. qualitative or categorical data). Here, we discuss two types of single variable plots that look at either a quantitative variable (histogram) or a categorical variable (barplot). We can create histograms in ggplot2 that are more aesthetically pleasing than the default hist function. This shows the distribution of one quantitative variable (worksheet tasks 3.4.3A and 3.4.3B). ggplot(msleep, aes(x = sleep_total)) + geom_histogram(bins = 10) We can look at how many individuals in the dataset fall into each category, such as how many mammals have each kind of feeding behavior (worksheet task 3.4.3C). ggplot(data = msleep, aes(x = vore)) + geom_bar() As you can see where we map the aesthetic, we only tell the ggplot function to refer to a single column in our dataset for our x-axis. 3.4.4 Visualizing between groups Let’s return to iris dataset to explore how we can visualize differences between groups/categories. These groups are often represented in our data as a factor. We can look at how the distributions of Sepal.Length differ depending on which species each iris belongs to. One plot that can do this easily is the geom_boxplot function (worksheet task 3.4.4A). ggplot(data = iris, aes(x = Species, y = Sepal.Length)) + geom_boxplot() By adding a different parameter to fill in the aes we define throgh the ggplot function, we can separate out histograms according to different groupings. Here, we use Species to determine the color of the fill. ggplot(iris, aes(x = Sepal.Length, fill = Species)) + geom_histogram(bins = 10) While we can sort of see the trends on this plot, it may be helpful to separate out each histogram for each individual species. There is an easy way to do this in ggplot2 using facetting or the facet_wrap function. This function splits the figure into separate panel where the data has been filtered by the category (i.e. Species) (worksheet task 3.4.4B). ggplot(iris, aes(x = Sepal.Length, fill = Species)) + geom_histogram(bins = 10) + facet_wrap( ~ Species) This matches what we already saw in the boxplot, showing that there are different sepal lengths depending on which iris species we look at. We will explore in the next section how we know if these differences are significant. 3.4.5 Generating heatmaps (OPTIONAL) If you are already familiar with ggplot2 or progress quickly through that material, you may consider reading this subsection. Otherwise, feel free to skip ahead! Heatmaps are a useful way to show the values of multiple samples across many measurements. You can visualize a heatmap by thinking of your dataframe, this tabular data, if it had each cell colorcoded based on how high or low the value is. The base R heatmap function meets many needs while the ggplot2 equivalent (geom_tile) can be confusing so we will recommend that you not use ggplot2 for heatmaps. Let’s go back to msleep dataset to visualize trends between the different measurements taken on each mammal. We will first create a simplified dataset from msleep where we take the log value of both brainwt and bodywt (worksheet task 3.4.5A). temp.data &lt;- cbind(log(msleep$brainwt), log(msleep$bodywt)) head(temp.data) ## [,1] [,2] ## [1,] NA 3.9120230 ## [2,] -4.1669153 -0.7339692 ## [3,] NA 0.3001046 ## [4,] -8.1456296 -3.9633163 ## [5,] -0.8603831 6.3969297 ## [6,] NA 1.3480731 Next, we feed these numeric values into the heatmap function along with some arguments that specify settings for displaying the figure. We use labCol to specify how to label these columns and cexCol to control the text size of these labels. We set labRow to be the names of each species from the msleep dataset (worksheet task 3.4.5B). heatmap(temp.data, labRow = msleep$name, labCol = c(&quot;brainwt&quot;, &quot;bodywt&quot;), cexCol = 1) This first heatmap will look strange because it colors each box by its magnitude, but body weight of a mammal is always greater than its brain weight. We want to scale within each column so that the depth of the color reflects whether the mammal has a high brain weight or high body weight relative to the other mammals (worksheet task 3.4.5C). heatmap(temp.data, scale = &quot;col&quot;, labRow = msleep$name, labCol = c(&quot;brainwt&quot;, &quot;bodywt&quot;), cexCol = 1) Using the scale argument which we set to \"col\", now the color of the columns is more meaningful. For example, it makes sense that the measurements taken on an Asian elephant are much higher than those from a mole rat, so the color of those cells is deeper. Let’s add some more data to our heatmap visualization (worksheet task 3.4.5D and 3.4.5E). temp.data2 &lt;- cbind(temp.data, msleep$sleep_total, msleep$sleep_rem) heatmap(temp.data2, scale = &quot;col&quot;, labRow = msleep$name, labCol = c(&quot;brainwt&quot;, &quot;bodywt&quot;, &quot;total_sleep&quot;, &quot;rem_sleep&quot;), cexCol = 1) You may have noticed the weird diagrams along the top and left hand side of this heatmap. These strange line diagrams are trees that show how our samples cluster together. Mammals that have similar patterns of values across these four measurements are placed near each other in the diagram. "],["making-scientific-figures.html", "3.5 Making scientific figures", " 3.5 Making scientific figures Making plots can be a great way to develop an intuition for your dataset, though to derive and communicate scientific insights, we need to have an idea of the uncertainty in our interpretations. Uncertainty describes ideas such as: are the values between two groups different enough, that it is unlikely that the differences are due to chance? Is the correlation between these variables strong enough that one can predict the other, with some level of confidence? How statistically significant are the patterns we see? 3.5.1 Plotting error bars When we compare measurements taken from two samples (i.e. two groups), we might want to see if the two groups have very different values for that specific measurement. If we have multiple observations within each group, we can take a summary statistic such as the mean or median and plot those against each other. ggplot(msleep2, aes(x = vore, y = awake)) + geom_bar(stat = &quot;summary&quot;, fun = &quot;mean&quot;) For example, here we have asked our geom_bar function to plot a summary, specifically the mean of each group, instead of plotting identity which usually means the value as is. Looking at this figure, we can’t guess if the groups are significantly different without an idea of the uncertainty in our measurements through something like error bars. Here is the convention for plotting error bars in ggplot2, as you can see it is just another kind of geom that we can add to our plot: ggplot(data = &lt;SUMMARY DATA&gt;, mapping = aes(&lt;SUMMARY MAPPINGS&gt;) + geom_bar(stat = \"identity\") + geom_errorbar(aes(&lt;ERROR MAPPINGS&gt;)) This method is straightforward, but you need to have pre-calculated the summary statistic for each group and the amount of error (i.e. standard error) from your data. That “aggregated” dataframe becomes the data that you provide to ggplot, instead of the original dataset (worksheet task 3.5.1A). feeding.data &lt;- msleep2 %&gt;% group_by(vore) %&gt;% summarize(mean_se(awake)) ## `summarise()` ungrouping output (override with `.groups` argument) feeding.data ## # A tibble: 5 x 4 ## vore y ymin ymax ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 carni 13.6 12.6 14.7 ## 2 herbi 14.5 13.6 15.4 ## 3 insecti 9.06 6.41 11.7 ## 4 omni 13.1 12.4 13.7 ## 5 &lt;NA&gt; 13.8 12.7 14.9 What does mean_se do? We can check. ?mean_se This function returns three values, y, ymin, and ymax which correspond to the mean, the mean minus one standard error, and the mean plus one standard error. The mean value will be the height of each bar on the barplot, while ymin corresponds to the bottom of the error bar and ymax to the top of the error bar. Let’s first plot the height of the bars using this new feeding.data dataset and mapping y to our new y column generated from the mean_se function. Here we create the same plot as before from this aggregated dataset, just showing the mean value in each group (worksheet task 3.5.1B). my.plot &lt;- ggplot(feeding.data, aes(x = vore, y = y)) + geom_bar(stat = &quot;identity&quot;) my.plot Now we add the error bars, mapping the ymin and ymax values to the arguments that happen to have the same name in the geom_errorbar function. We add in the width setting just for aesthetics (worksheet tasks 3.5.1C and 3.5.1D). my.plot &lt;- my.plot + geom_errorbar(aes(ymin = ymin, ymax = ymax), width = 0.2) my.plot Now it is more clear that insectivores have significantly less time awake compared to other kinds of mammals. 3.5.2 Showing trends in data Lots of these different figures summarize or aggregate the data. We may want to display the data with the individual points, but still show the overall trend across the data. Some good plots that do this are geom_density_2d which provides a contour plot. Let’s check out the trend in our msleep data between body weight and brain weight. We will create a new column where we take the log value of body weight, like we did with brain weight. msleep3 &lt;- msleep2 %&gt;% mutate(bodywt_log = log(bodywt)) ggplot(data = msleep3, mapping = aes(x = brainwt_log, y = bodywt_log)) + geom_density_2d() + geom_point() As you can see the density of points almost look like they fit a line. As brain weight increases then body weight increases, or vice versa. We can add a trendline to this plot with the geom_smooth function (worksheet tasks 3.5.2A and 3.5.2B). ggplot(msleep3, aes(x = brainwt_log, y = bodywt_log)) + geom_point(alpha = 0.5) + geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; We can also do this with trendlines that summarize only certain subsets of the data, such as those that belong to a specific category. We will flip back to our iris dataset to look at how sepal length compares to sepal width between different species of iris. ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) + geom_point() + geom_smooth(aes(color = Species)) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; We create a seaprate trendline for each by specifiying the aes with color mapping to Species within the geom_smooth function. The above plot may make it hard to see the data that is contributing to each trend line. Using facet_wrap again, we can split the figure into separate panel where the data has been filtered by the category (i.e. species) (worksheet task 3.5.2C). ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) + geom_point() + geom_smooth(aes(color = Species)) + facet_wrap( ~ Species) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Now it is clear that there is a positive correlation (as one goes up, the other goes up) in each species of iris, but some species have a sharper upward trend than others. 3.5.3 Saving figures locally As you produce analysis in your research, you may want to create high-quality images of your figures to then use in presentations or publications. There are two easy ways to save images as an individual file on your computer (worksheet tasks 3.5.3A and 3.5.3B), The first method uses ggsave to save the most recent ggplot figure you generated (worksheet task 3.5.3C). ggplot(msleep2, aes(x = brainwt_log, y = sleep_rem)) + geom_point() + geom_smooth(method = &quot;lm&quot;) ggsave(&quot;plot.png&quot;, width = 5, height = 5) This function will save the figure just produced by this code wherever your directory is currently. You can check your current directory with getwd() and change with setwd(&lt;FOLDER NAME&gt;). You can also provide a precise file path in the new file name. ggsave(&quot;~/Downloads/plot.png&quot;, width = 5, height = 5) Here is an alternative method for saving your figures: pdf(&quot;plot.pdf&quot;) # creates the file # png() also works if you want a different file format ggplot(msleep2, aes(x = brainwt_log, y = sleep_rem)) + geom_point() + geom_smooth(method = &quot;lm&quot;) dev.off() # finishes editing the file Any changes to the figure that are contained between the initial creation of the figure (i.e. the pdf command) and the dev.off command will be included in the final saved image (worksheet task 3.5.3D). However, the figure is being printed directly to the file it is writing and won’t appear elsewhere (worksheet task 3.5.3E). "],["applying-basic-stats.html", "3.6 Applying basic stats", " 3.6 Applying basic stats Following up on our analysis using visualizations, we will review different types of statistical tests that we can perform on data. We will again revisit the mammalian sleep dataset msleep. head(msleep) ## # A tibble: 6 x 11 ## name genus vore order conservation sleep_total sleep_rem sleep_cycle awake ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Chee… Acin… carni Carn… lc 12.1 NA NA 11.9 ## 2 Owl … Aotus omni Prim… &lt;NA&gt; 17 1.8 NA 7 ## 3 Moun… Aplo… herbi Rode… nt 14.4 2.4 NA 9.6 ## 4 Grea… Blar… omni Sori… lc 14.9 2.3 0.133 9.1 ## 5 Cow Bos herbi Arti… domesticated 4 0.7 0.667 20 ## 6 Thre… Brad… herbi Pilo… &lt;NA&gt; 14.4 2.2 0.767 9.6 ## # … with 2 more variables: brainwt &lt;dbl&gt;, bodywt &lt;dbl&gt; 3.6.1 Describing quantitative variables There are several ways to describe quantitative measurements. We might first look at the range of values using the quantile function, which returns the min, max, and median values as well as the 25th and 75th percentiles. quantile(msleep$brainwt, na.rm = TRUE) ## 0% 25% 50% 75% 100% ## 0.00014 0.00290 0.01240 0.12550 5.71200 Note that we are using the setting na.rm = TRUE as an argument to these functions so that it ignores NA values in the data. You can also directly isolate some of these values using the functions min or max. We might describe the center of our data, that is the mean or the median values using those respective function names. median(msleep$brainwt, na.rm = TRUE) ## [1] 0.0124 mean(msleep$brainwt, na.rm = TRUE) ## [1] 0.2815814 We might also want to describe the spread in the data, such as the standard deviation or the interquartile range. sd(msleep$brainwt, na.rm = TRUE) ## [1] 0.9764137 IQR(msleep$brainwt, na.rm = TRUE) ## [1] 0.1226 Visualizations are a great way to see this information in one picture. We have learned to create a fancier histogram, but the hist function always works for quick viewing (worksheet tasks 3.6.1A and 3.6.1B). hist(msleep$brainwt) It is clear from this image that the data is skewed towards approaching 0 with a few high outliers. 3.6.2 Finding differences in data For the following statistical tests, we want to determine if measurements of a quantitative variable taken on a certain group of samples are different from similar measurements taken from a different group of samples. To know if these differences are statistically significant, we need to be aware of the uncertainty in our data. Even if we have collect many observations, there is generally noisiness and error in those measurements. Thus, we have uncertainty if the mean value we calculate for a group is in fact the true mean. We can calculate a confidence interval to describe our guess for the mean in our data. That is, we can identify the mean based on our samples, but also provide an upper or lower bound for where the mean might be (worksheet task 3.6.2A). t.test(msleep$brainwt) ## ## One Sample t-test ## ## data: msleep$brainwt ## t = 2.1581, df = 55, p-value = 0.03531 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 0.02009613 0.54306673 ## sample estimates: ## mean of x ## 0.2815814 We use this t.test function because we assume that our data is normally distributed (which is not necessarily always the case) and it has a small sample size. You can see that while the mean for brain weight from this popular is 0.28, the range is fairly large (between 0.2 and 0.54) because of how big the variability is in the data. If we want to determine if two populations have a difference in their mean values that is statistically significant, we can calculate the t-test between two sets of observations. Let’s look at whether the average time spent awake by mammals who are insectivores is significantly less than those who are not (worksheet task 3.6.2B). insectivores &lt;- msleep %&gt;% filter(vore == &quot;insecti&quot;) %&gt;% select(awake) other.mammals &lt;- msleep %&gt;% filter(vore != &quot;insecti&quot;) %&gt;% select(awake) head(insectivores) ## # A tibble: 5 x 1 ## awake ## &lt;dbl&gt; ## 1 4.3 ## 2 4.1 ## 3 5.9 ## 4 15.6 ## 5 15.4 head(other.mammals) ## # A tibble: 6 x 1 ## awake ## &lt;dbl&gt; ## 1 11.9 ## 2 7 ## 3 9.6 ## 4 9.1 ## 5 20 ## 6 9.6 At first, we use tidyverse functions to isolate only the mammals that match the feeding behavior and select only the relevant column, before feeding this data into the t.test function (worksheet tasks 3.6.2C and 3.6.2D). t.test(insectivores, other.mammals, alternative = &quot;less&quot;) ## ## Welch Two Sample t-test ## ## data: insectivores and other.mammals ## t = -1.7796, df = 4.3091, p-value = 0.0723 ## alternative hypothesis: true difference in means is less than 0 ## 95 percent confidence interval: ## -Inf 0.8336811 ## sample estimates: ## mean of x mean of y ## 9.06000 13.86056 These results show a p-value of 0.07, which is not the usual level of significance that most scientists accept (p-value &lt; 0.05). However, there are numerous feeding behaviors besides insectivore and we may be interested in comparing the multiple categories against each other. We can use ANOVA (analysis of variance) to see how our quantitative variable is affected by the different kinds of feeding behavior (worksheet tasks 3.6.2E and 3.6.2F). anova.msleep &lt;- aov(awake ~ vore, data = msleep) summary(anova.msleep) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## vore 3 133.7 44.58 2.234 0.0916 . ## Residuals 72 1437.0 19.96 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## 7 observations deleted due to missingness The p-value of 0.09 suggests that there may not be a strongly significant difference in the average time spent awake between these groups. 3.6.3 Identifying correlations When we are examining the relationship between two quantitative variables, we might be interested in whether they correlate with one another. A positive correlation between x and y means that as x increases so does y. A negative correlation between x and y means that as x increases, y decreases. We can use the base R cor function to calculate the correlation between two numeric vectors. cor(msleep$brainwt, msleep$bodywt) ## [1] NA Notice that we get NA instead of an actual numeric value. Unfortunately there is not an easy setting to have the cor function ignore any NA values. Thus, we need to temporarily remove these values. Let’s first find them with the summary function. summary(msleep$bodywt) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.005 0.174 1.670 166.136 41.750 6654.000 summary(msleep$brainwt) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.00014 0.00290 0.01240 0.28158 0.12550 5.71200 27 There are no NA values in the bodywt column, but there are 27 NA values in the brainwt column. Let’s remove those using a helpful function in tidyverse and save the more complete dataset as msleep2 (worksheet task 3.6.3A). msleep2 &lt;- msleep %&gt;% drop_na(brainwt) dim(msleep) ## [1] 83 11 dim(msleep2) ## [1] 56 11 You can see that we dropped 27 rows, corresponding to the mammals for whom brain weight was not measured or available. Now we can try to use the cor function. cor(msleep2$brainwt, msleep2$bodywt) ## [1] 0.9337822 We can see that the correlation is 0.93, a value very close to 1. This suggests that these variabilities are almost perfectly correlated with each other, which we saw when we plotted these two variables against each other in the prior section. There are multiple types of correlations. The cor function uses a Pearson correlation by default and can use different methods like Spearman. Let’s explore the difference between these correlations by looking at the relationship between the log value of brain weight and length of sleep cycle. msleep3 &lt;- msleep %&gt;% mutate(brainwt_log = log(brainwt)) %&gt;% drop_na(brainwt_log, sleep_cycle) ggplot(msleep3, aes(x = brainwt_log, y = sleep_cycle)) + geom_point() + geom_smooth(method = &quot;lm&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; When we plot these variables against each other, including a fitted linear trend line, we can see that there is a relationship between these variables (worksheet task 3.6.3B). However, the points do not quite fit a straight line if we look at the edges and center of this fitted line. This will reflect in our calculation of two different correlations (worksheet task 3.6.3C). cor(msleep3$brainwt_log, msleep3$sleep_cycle, method = &quot;pearson&quot;) ## [1] 0.8331014 cor(msleep3$brainwt_log, msleep3$sleep_cycle, method = &quot;spearman&quot;) ## [1] 0.8726893 Pearson correlations are higher when the data fits to a straight line like a trend line. However, Spearman correlations do not require a single slope: as long as one value goes up and the other goes up, these values are considered highly correlated. 3.6.4 Producing linear models You may be interested in studying the relationship between one or more variables and some outcome that you care about. To determine if this set of one or more variables are strong predictors of an outcome (some quantitative variable), you can fit a linear model to this data using the lm function. Here we train a linear model to try to predict brain weight from body weight in the msleep dataset. Linear models are established with a formula with the format of outcome ~ predictor where we are trying to determine if the predictor is able to help us accurately estimate an outcome (worksheet task 3.6.4A). my.mod &lt;- lm(formula = brainwt ~ bodywt, data = msleep2) summary(my.mod) ## ## Call: ## lm(formula = brainwt ~ bodywt, data = msleep2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.20686 -0.07565 -0.07184 -0.03565 1.18663 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.0761566 0.0479290 1.589 0.12330 ## bodywt 0.0009228 0.0003162 2.919 0.00686 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2459 on 28 degrees of freedom ## Multiple R-squared: 0.2333, Adjusted R-squared: 0.2059 ## F-statistic: 8.52 on 1 and 28 DF, p-value: 0.006859 In the summary of the results of this modeling, we see that bodywt is a significant predictor of brainwt in the msleep dataset. We can provide more predictors into our formula in the format: outcome ~ predictor1 + predictor2 + ... (worksheet tasks 3.6.4B and 3.6.4C). my.mod2 &lt;- lm(formula = sleep_total ~ bodywt + brainwt, data = msleep2) summary(my.mod2) ## ## Call: ## lm(formula = sleep_total ~ bodywt + brainwt, data = msleep2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.3940 -2.2591 -0.1205 1.3586 7.5917 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 12.309310 0.675518 18.222 &lt; 2e-16 *** ## bodywt -0.014261 0.004874 -2.926 0.00688 ** ## brainwt -3.290750 2.551012 -1.290 0.20800 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.319 on 27 degrees of freedom ## Multiple R-squared: 0.4012, Adjusted R-squared: 0.3569 ## F-statistic: 9.046 on 2 and 27 DF, p-value: 0.0009843 These results suggest that neither bodywt nor brainwt are significant predictors of sleep_total. That means that the value of either of those measurements does not give us information that helps us guess the total sleep duration for a mammal accurately. "],["day-5-workflows-loops-packages-and-help.html", "4 Day 5 - workflows, loops, packages, and help", " 4 Day 5 - workflows, loops, packages, and help Important links: The Syllabus / Notes - you are here! The Worksheet for Day 5 - save it, and open it from inside RStudio ( control/combo + o key, usually). You can also go here and then click “Raw”, and save that as a file. Open that inside Rstudio. Today we’re going to learn about applying R skills to repeat, reproduce, extend, and share your analyses. Here’s some broad questions to reflect on the previous two days: Crack open your code - can you use it again? Can you adapt it to modify your question, feed in new data, and modify the scientifically-important bits easily? Can you share it with someone and they follow along? Our goals are to learn: Why and how to organize work in a folder and rmarkdown files How look at character strings, to look for patterns and calculate lengths of strings Why and how to write loops to do many analyses in a scalable way What packages are for, how to find them, and how to install them How to find help and tackle new problems with a learning community Preparing for Today: Please make sure you have completed all readings and activities from the previous days. If you have not found the time to do so, go ahead and join us to start on this material today. This site will stay up for at least several months (if not years), so these notes and worksheets (and us!) will be available long after. Today’s Schedule: Start Time End Time Activity Description Facilitator 10:00 AM PDT 10:15 AM PDT Synchronous - lecture 4.1 organizing workflows Darach, Melissa, Samson, Margaret, Zac 10:15 AM PDT 10:45 AM PDT Synchronous - breakout rooms, then lecture 4.1 exercises Darach, Melissa, Samson, Margaret, Zac 10:45 AM PDT 12:15 PM PDT Asynchronous - video, slack, etc 4.2 Project - analyze viral protein sequences Darach, Samson, Margaret, Zac 12:15 PM PDT 12:30 PM PDT Synchronous - lecture 4.2 Modular, extendable workflows Darach, Melissa, Samson, Margaret 12:30 PM PDT 01:00 PM PDT Break Lunch. Take a computer break! N/A 01:00 PM PDT 01:15 PM PDT Synchronous - lecture 4.3 Packages - why and how Darach, Samson, Margaret, Zac 01:15 PM PDT 02:00 PM PDT Asynchronous - video, slack, etc 4.3 exericses Darach, Samson, Margaret, Zac 02:00 PM PDT 02:15 PM PDT Synchronous - lecture 4.3 wrapup Darach, Samson, Margaret, Zac 02:15 PM PDT 02:30 PM PDT Break Take a computer break! N/A 02:30 PM PDT 02:45 PM PDT Synchronous - lecture 4.4 Demo/discuss some places to find help Darach, Samson, Margaret 02:45 PM PDT 03:15 PM PDT Synchronous - groups 4.4 projects - attempt ambitious projects Darach, Samson, Margaret, Zac 03:15 PM PDT 03:30 PM PDT Synchronous - lecture 4.5 Wrapping up Darach, Samson, Margaret, Zac "],["what-is-a-workflow.html", "4.1 What is a workflow?", " 4.1 What is a workflow? What does it take to do an analysis? When you embark on some project, it’s helpful (but not necessary) to plan where you are going. It’s best to think about factors like: What data am I analyzing? What format is it in, and is that going to change? Will there be a lot more of this data that I want to process in the future? What kind of analyses do I want to do? What major steps do I have in my process? What work from others can I re-use and build upon? What do I want to generate? What is the desired outcome, and for whom is this intended? In the author’s experience, bioinformatic workflows often involve reading in big (or many) flat files, processing the data into a useful form, sometimes using extra packages to analyze the data, then generating plots and statistical summaries of what you found - usually in an Rmarkdown-generated report. So let’s start building up some good habits and skills! 4.1.1 Organizing files Everyone’s got their own personal organizational systems, but usually everyone is operating on computers organized in a hierarchical file system. This paradigm organizes files into hierarchically nesting directories. In at least this author’s experience, it is common practice to: use one folder per major chunk of work use subfolders such as data scripts tmp output write a README explaining what the project is and how to use it There are many opinions on how to do this. The correct style is whatever works for you and your research community. Ask your supervisor. –&gt; Let’s go ahead and make an organized folder structure to work in &lt;– You can change files and such using RStudio. Go to the panel on the bottom right, and switch it to “Files” tab. You might see something like this: a mostly empty directory Go ahead and make a new folder, maybe call it “project”: making new folder Then, make folders to hold different types of files. What folders should we make? data, scripts, output is a good start. Then, put a copy of the day5 worksheet in scripts folder. You can move it however you like, using a folder, bash, or save-as. Put that in the scripts folder. 4.1.2 Use a notebook to generate reports Self-documented code means code that can be understood without an external explanation. You should probably do this with a code notebook. rmarkdown is the premier tool for typesetting static notebooks, and it’s simpler to setup, use, and distribute results from, and is most commonly used by the R community. 4.1.2.1 Background about rmarkdown The rmarkdown package was created by Yihui Xie, extending from previous work and ideas in knitr and sweave. It’s a way of mixing writing with code, such that you can run the code and it makes a pretty doc (and more). Basically, it does this: Parses .Rmd files to run the R code chunks, with appropriate chunk options, and saves the output of the R code along with the original text in a .md file. Passes the .md file to pandoc to turn this into an appropriate output, usually a HTML file. rmarkdown is maintained by the RStudio company. They generate a lot of helpful “cheatsheets” too. 4.1.2.2 Chunk options You can set chunk-level options in an Rmd, for each code chunk. Such as: ```{r, name_of_chunk, cache=T, fig.width=3, fig.height=2, error=F, warning=F, fig.align=&quot;right&quot;} x &lt;- 1:10 y &lt;- 1:10 plot(x,y) ``` x &lt;- 1:10 y &lt;- 1:10 plot(x,y) This includes options like: - fig.width - fig.height - cache - echo - results - warning, message, and error There’s a nice reminder list at the bottom of this doc. One critically important one is cache. This saves the outputs of code chunks in a file, so that each time you re-render the document, it will check if the chunk is new. If not, it’ll just load the outputs from last time. I would recommend you use cache=T for any chunks with long computations. Don’t use it for generated outputs while you’re modifying upstream variables, as they won’t re-generate each time you run the notebook. Similarly, I recommend you don’t cache loading libraries - these may not load to be able to run the rest of the code. ```{r, name_of_chunk, fig.align=&quot;right&quot;} x &lt;- 1:10 y &lt;- 1:10 plot(x,y) ``` 4.1.2.3 Document options You can set document-level options to. To do this, you create what’s called a YAML header, like so: --- title: &quot;Titled&quot; author: &quot;yours&quot; --- You need three hyphens to open and close it. Put it at the very beginning. You can define quite a few options, including themes, like so: --- title: &quot;Titled&quot; author: &quot;yours&quot; output: html_document: theme: yeti --- Note that it (theme: yeti) has to be indented after html_document:, indented after output:. For more info and practice about YAML, see this YAML parser to see what YAML you are actually writing. See more themes, and options, here. 4.1.3 Sharing with others You’ll need to share your work with others. Think about the next person coming along to the project, or yourself in 3 months to a year from now. What do you need to understand what you did and what you learned, and how to build off of it? This means all of what was done, the numbers and graphs made, and also what you were thinking and what you now think. In science, failure is totally an option, and is useful if it is paired with explanations of what didn’t work. “Success” is fine too, I guess, but you aren’t probably going to learn as much. Either way, prepare to share! Keep notes and write up text about what you were thinking, what you were trying to do, and how it turned out. You can do that in between code chunks in an Rmarkdown file. Ask your supervisor for details about what you’re going to produce and share with folks. "],["analyzing-viral-protein-sequences.html", "4.2 Analyzing viral protein sequences", " 4.2 Analyzing viral protein sequences Let’s practice with starting a project. Viral structural proteins are dramatically more important to the general public than they were 2 years ago - and under much more public attention. Let’s take a cursory look at some of these. What kind of data are we using for this? We’ll retrieve some determined protein sequences from NCBI databases. We’ve already done that work for you already. Here’s a short writeup of what we did, amd we would recommend that you always document where the data came from and how it was retrieved/processed: The NCBI Identical Protein Groups database was queried for “structural” with “Division” restricted to “Viruses” and “Source database” restricted to “UniProtKB/Swiss-Prot”. 243 results were retrieved as FASTA files and converted using bash commands into separate tab-delimited files. 3 What format are these in? These files are in the data/viral_structural_proteins folder, they end in .tsv, are tab-delimited three fields, and look like this: Q8V433.1 Membrane protein Bovine respiratory coronavirus (strain 98TXSF-110-LUN) MSSVTTPAPVYTWTA... This is not a standard format, because I’d like to tell y’all about reading tab-delimited files. 4 One trick to look at files using R is using readLines() function. This just reads lines, up to the argument n= number of lines. Try something like: readLines(&quot;data/viral_structural_proteins/viral_proteins_242.tsv&quot;,n=5) ## [1] &quot;YP_009724393.1 membrane glycoprotein \\tSevere acute respiratory syndrome coronavirus 2\\tMADSNGTITVEELKKLLEQWNLVIGFLFLTWICLLQFAYANRNRFLYIIKLIFLWLLWPVTLACFVLAAVYRINWITGGIAIAMACLVGLMWLSYFIASFRLFARTRSMWSFNPETNILLNVPLHGTILTRPLLESELVIGAVILRGHLRIAGHHLGRCDIKDLPKEITVATSRTLSYYKLGASQRVAGDSGFAAYSRYRIGNYKLNTDHSSSSDNIALLVQ&quot; Whenever using these commands, put your cursor in the quotes and then hit TAB, to use autocomplete. Let’s do two things. Let’s count how long the protein sequences are, so we can look at that distribution. Let’s also look for particular combinations of amino acids. Some folks are quite interested in the insertion of an amino-acid sequence of PRRA, relative to other related coronaviruses, in the SARS-CoV-2 genome. I don’t know hardly anything about furin-cleavage sites or viral evolution, but since this is an introductory R class, we can use the tools we have to look for this sequence in the files. Let’s do that5 How? 4.2.1 Reading in tab-delimited file Let’s read one of them into R. The records are “delimited” by tabs, so each field is tab-separated. We’ll need to use the read.delim function and specify a tab separator. 4.2.1.1 Review of looking up documentation You can look up the documentation for any named function or package by using the ?function syntax. Sometimes, you need to use extra backticks to make it work, like ?`+` If you don’t know what you’re looking for, you can search with ??. Extra backticks don’t hurt, and are necessary when you have spaces in the query. Once the documentation is open, you can search for text. If you are using the R console, you can use / to open a search bar and enter-key to search for it. 4.2.1.2 Back to the file-reading… We can use read.delim to read in one of the files. You will have to specify the delimiter/separator. \" is usually how you specify a TAB character in character strings, like you would set as that argument. viral_protein_data &lt;- read.delim(&quot;data/viral_structural_proteins/viral_proteins_100.tsv&quot;, sep=&quot;\\t&quot;,header=F) viral_protein_data ## V1 ## 1 Q8V433.1 Membrane protein ## V2 ## 1 Bovine respiratory coronavirus (strain 98TXSF-110-LUN) ## V3 ## 1 MSSVTTPAPVYTWTADEAIKFLKEWNFSLGIILLFITVILQFGYTSRSMFVYVIKMIILWLMWPLTIILTIFNCVYALNNVYLGFSIVFTIVAIIMWIVYFVNSIRLFIRTGSWWSFNPETNNLMCIDMKGRMYVRPIIEDYHTLTVTIIRGHLYMQGIKLGTGYSLSDLPAYVTVAKVSHLLTYKRGFLDKIGDTSGFAVYVKSKVGNYRLPSTQKGSGLDTALLRNNI What type is that third column? is(viral_protein_data$V3) ## [1] &quot;factor&quot; &quot;integer&quot; &quot;oldClass&quot; ## [4] &quot;double&quot; &quot;numeric&quot; &quot;vector&quot; ## [7] &quot;data.frameRowLabels&quot; Huh, looks like it’ll convert it to a factor automatically. Let’s tell it to not do that, to leave it as.is. viral_protein_data &lt;- read.delim(&quot;data/viral_structural_proteins/viral_proteins_100.tsv&quot;, sep=&quot;\\t&quot;,header=F,as.is=T) viral_protein_data ## V1 ## 1 Q8V433.1 Membrane protein ## V2 ## 1 Bovine respiratory coronavirus (strain 98TXSF-110-LUN) ## V3 ## 1 MSSVTTPAPVYTWTADEAIKFLKEWNFSLGIILLFITVILQFGYTSRSMFVYVIKMIILWLMWPLTIILTIFNCVYALNNVYLGFSIVFTIVAIIMWIVYFVNSIRLFIRTGSWWSFNPETNNLMCIDMKGRMYVRPIIEDYHTLTVTIIRGHLYMQGIKLGTGYSLSDLPAYVTVAKVSHLLTYKRGFLDKIGDTSGFAVYVKSKVGNYRLPSTQKGSGLDTALLRNNI colnames(viral_protein_data) ## [1] &quot;V1&quot; &quot;V2&quot; &quot;V3&quot; is(viral_protein_data$V3) ## [1] &quot;character&quot; &quot;vector&quot; &quot;data.frameRowLabels&quot; ## [4] &quot;SuperClassMethod&quot; Note that you can find the column names, or names of columns, using colnames. Or names. And you can inspect the type in several different ways. This is how you do it in base R. How do you do this in tidyverse? You can use read_delim or read_tsv, with different arguments, such as col_names=F to prevent it from reading the first line as a header. Also, no factor conversion :) 4.2.2 String stuff Once we have that, we can test how we will process the protein sequence. For now, let’s just calculate the length of the protein sequence. Character strings are a common type of data, and there’s lots of ways to cut, dice, extract, and recognize elements to help your work. Here’s some tools/ideas: 4.2.2.1 Base R tools Let’s start with one of those above sentences. You can make a string by putting characters in between single or double quotes: stringz &lt;- &quot;Character strings are a common type of data, and there&#39;s lots of ways to&quot; stringz ## [1] &quot;Character strings are a common type of data, and there&#39;s lots of ways to&quot; is(stringz) ## [1] &quot;character&quot; &quot;vector&quot; &quot;data.frameRowLabels&quot; ## [4] &quot;SuperClassMethod&quot; str(stringz) ## chr &quot;Character strings are a common type of data, and there&#39;s lots of ways to&quot; grep and grepl are good for searching for patterns (like grep in bash). The first returns position, the second returns TRUE or FALSE. I only ever use grepl anymore. It searches for a pattern string in a character vector x, and returns a logical vector. With a logical vector, you can do a lot, like indexing/slicing or sum-ing. grepl(pattern=&quot;common type&quot;,x=stringz) ## [1] TRUE grepl(&quot;rare type&quot;,stringz) ## [1] FALSE grepl(&quot; [comn]* type&quot;,stringz) ## [1] TRUE So see that last one with the []* stuff? Called a regular expression. You can use complex regular expressions to specify more complex patterns. These can get really complex and really powerful. We’re not going to explain these here, except just for the example below. Ask on Slack later if you’d like more help with something specific. Also of use is sub and gsub. These substitute patterns with replacements, for the first occurrence (sub) or globally (gsub). These are really handy for modifying data-tables. For example, let’s say you’d like to split up filenames by dates: datar &lt;- data.frame(stringsAsFactors=F, filenames=c( &quot;210607_pilot_transformation_works.jpeg&quot;, &quot;210609_transformation_seems_to_work.jpeg&quot;, &quot;210610_failed_gel_images.jpeg&quot; ) ) datar$dates &lt;- gsub(pattern=&quot;(\\\\d\\\\d\\\\d\\\\d\\\\d\\\\d)_(.*)\\\\.jpeg&quot;, replacement=&quot;\\\\1&quot;,x=datar$filenames) datar$names &lt;- gsub(pattern=&quot;(\\\\d\\\\d\\\\d\\\\d\\\\d\\\\d)_(.*)\\\\.jpeg&quot;, replacement=&quot;\\\\2&quot;,x=datar$filenames) datar ## filenames dates names ## 1 210607_pilot_transformation_works.jpeg 210607 pilot_transformation_works ## 2 210609_transformation_seems_to_work.jpeg 210609 transformation_seems_to_work ## 3 210610_failed_gel_images.jpeg 210610 failed_gel_images The above uses regular expressions, where \\\\d is matching any digit, .* matches anything, and () denote “groups” to “capture”. In the “replacement”, these “groups” are referred to by \\\\1 or \\\\2. These “capture groups” are super useful! You can use them to take out matches and re-arrange them. So, (\\\\d\\\\d\\\\d\\\\d\\\\d\\\\d)_ matches six digits followed by an underscore, such as 210618_, and saves it as \\\\1. So gsub(pattern=&quot;(\\\\d\\\\d\\\\d\\\\d\\\\d\\\\d)_(.*)&quot;,replacement=&quot;\\\\1&quot;,x=&quot;210616_after&quot;) ## [1] &quot;210616&quot; gsub(pattern=&quot;(\\\\d\\\\d\\\\d\\\\d\\\\d\\\\d)_(.*)&quot;,replacement=&quot;\\\\2&quot;,x=&quot;210616_after&quot;) ## [1] &quot;after&quot; There are better (cleaner) ways of doing this with tidyverse, basics are important. See a bit of tidyverse/stringr stuff in the end of this 4.2 section, if you’re curious. You should now be able to manipulate within character strings a bit, identify and replace patterns. 4.2.3 Loops - doing a similar task multiple times You will want to repeat this analysis for all the files. The simplest way of doing this is to copy and paste it, and change the filename. viral_protein_data &lt;- read.delim(&quot;data/viral_structural_proteins/viral_proteins_002.tsv&quot;, sep=&quot;\\t&quot;,header=F, as.is=T) nchar(viral_protein_data$V3) ## [1] 177 viral_protein_data &lt;- read.delim(&quot;data/viral_structural_proteins/viral_proteins_003.tsv&quot;, sep=&quot;\\t&quot;,header=F, as.is=T) nchar(viral_protein_data[1,&quot;V3&quot;]) ## [1] 137 Go ahead and do this for all 242 proteins … just kidding. This is a lot of work, and each time we do this we can introduce errors. If we ever have more files, we have to copy and paste more. If we ever want to change an argument for all of them, we have to do each one. Instead, we can work with a list of all the files available: list.files(path=&quot;data/viral_structural_proteins&quot;)[1:5] ## [1] &quot;viral_proteins_000.tsv&quot; &quot;viral_proteins_001.tsv&quot; &quot;viral_proteins_002.tsv&quot; ## [4] &quot;viral_proteins_003.tsv&quot; &quot;viral_proteins_004.tsv&quot; Note that I put a [1:5] to limit it to the first 5. You could also use head(). It’s a good idea to work with a small subset of files while you are iterating through development, then scale it up to the entirety. But watch out… tail(list.files(path=&quot;data/viral_structural_proteins&quot;)) ## [1] &quot;viral_proteins_238.tsv&quot; &quot;viral_proteins_239.tsv&quot; ## [3] &quot;viral_proteins_240.tsv&quot; &quot;viral_proteins_241.tsv&quot; ## [5] &quot;viral_proteins_242.tsv&quot; &quot;viral_structural_proteins.fasta&quot; At the very end is another different file, a FASTA file. So let’s use a pattern argument in list.files to specify what we want to list. And we’ll get back to the FASTA later… tail(list.files(path=&quot;data/viral_structural_proteins&quot;,pattern=&quot;.*tsv&quot;)) ## [1] &quot;viral_proteins_237.tsv&quot; &quot;viral_proteins_238.tsv&quot; &quot;viral_proteins_239.tsv&quot; ## [4] &quot;viral_proteins_240.tsv&quot; &quot;viral_proteins_241.tsv&quot; &quot;viral_proteins_242.tsv&quot; .* stands for 0 or more (*) of anything (.). For more details, look up regular-expressions. 4.2.3.1 What are loops? Loops are for running a “code block” as many times as the “condition” determines. A “code block” is either one line of code, or multiple lines of code surrounded by curly brackets - {} { code &lt;- &quot;in a block&quot; with &lt;- &quot;multiple lines&quot; } The code just runs. Yep. It’s that simple. A “condition” is an expression of code that can either evaluate to either TRUE or FALSE, or set a variable for each time running the code block. This is often just before the code block, in () parentheses. The most common form of these kind of “control statements” is a for loop. Other “control statements” or “flow control statements” are while, repeat, and if. Let’s look up what they do, with ?`for` Note the back ticks! These are a trick in R to make anything be interpreted as literally what you type, and not any special characters. Like ?`+`, or ?`?` Here’s an example loop: for (i in 1:4) { print(i) } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 The pieces: (i in 1:4) is what is being looped over - 1:4 is a vector of 1 through 4 that is created, and it is put one at a time into i (a new variable). You need the parentheses. { and } denote the opening and closing brackets, specify the “code block” that is run each time. inside this “code block” is print(i) - it prints the variable i, which is set to a value of 1, 2, 3, or 4 for each loop More about loops for (i in list.files(path=&quot;data/viral_structural_proteins&quot;, pattern=&quot;.*tsv&quot;)[1:5] ) { print(i) } ## [1] &quot;viral_proteins_000.tsv&quot; ## [1] &quot;viral_proteins_001.tsv&quot; ## [1] &quot;viral_proteins_002.tsv&quot; ## [1] &quot;viral_proteins_003.tsv&quot; ## [1] &quot;viral_proteins_004.tsv&quot; Inside the code block we can do more than print, like looking for a motif. Here, we’ll try to read the file and take a look at the sequence … for (i in list.files(path=&quot;data/viral_structural_proteins&quot;, pattern=&quot;.*tsv&quot;)[1:5] ) { print(i) print(read.delim(i)$V3) } ## [1] &quot;viral_proteins_000.tsv&quot; ## Warning in file(file, &quot;rt&quot;): cannot open file &#39;viral_proteins_000.tsv&#39;: No such ## file or directory ## Error in file(file, &quot;rt&quot;): cannot open the connection Error! It is looking for a file viral_proteins_000.tsv, but it is looking in this directory. It is actually in data/viral_structural_proteins. Look up the list.files documentation, and find how to get it to return the full name of the file. Next, this should work… for (i in list.files(path=&quot;data/viral_structural_proteins&quot;, pattern=&quot;.*tsv&quot;,full.names=T)[1:5] ) { print(read.delim(i,sep=&quot;\\t&quot;,header=F,as.is=T)$V3) } ## [1] &quot;MEFIPTQTFYNRRYQPRPWTPRPTIQVIRPRPRPQRQAGQLAQLISAVNKLTMRAVPQQKPRRNRKNKKQKQKQQAPQNNTNQKKQPPKKKPAQKKKKPGRRERMCMKIENDCIFEVKHEGKVTGYACLVGDKVMKPAHVKGTIDNADLAKLAFKRSSKYDLECAQIPVHMKSDASKFTHEKPEGYYNWHHGAVQYSGGRFTIPTGAGKPGDSGRPIFDNKGRVVAIVLGGANEGARTALSVVTWNKDIVTKITPEGAEEWSLAIPVMCLLANTTFPCSQPPCIPCCYEKEPEETLRMLEDNVMRPGYYQLLQASLTCSPHRQRRSTKDNFNVYKATRPYLAHCPDCGEGHSCHSPVALERIRNEATDGTLKIQVSLQIGIGTDDSHDWTKLRYMDNHIPADAGRAGLFVRTSAPCTITGTMGHFILARCPKGETLTVGFTDSRKISHSCTHPFHHDPPVIGREKFHSRPQHGKELPCSTYVQSNAATAEEIEVHMPPDTPDRTLLSQQSGNVKITVNGRTVRYKCNCGGSNEGLITTDKVINNCKVDQCHAAVTNHKKWQYNSPLVPRNAELGDRKGKIHIPFPLANVTCMVPKARNPTVTYGKNQVIMLLYPDHPTLLSYRSMGEEPNYQEEWVTHKKEVVLTVPTEGLEVTWGNNEPYKYWPQLSANGTAHGHPHEIILYYYELYPTMTVVVVSVASFILLSMVGMAVGMCMCARRRCITPYELTPGATVPFLLSLICCIRTAKAATYQEAAVYLWNEQQPLFWLQALIPLAALIVLCNCLRLLPCCCKTLAFLAVMSIGAHTVSAYEHVTVIPNTVGVPYKTLVNRPGYSPMVLEMELLSVTLEPTLSLDYITCEYKTVIPSPYVKCCGTAECKDKNLPDYSCKVFTGVYPFMWGGAYCFCDAENTQLSEAHVEKSESCKTEFASAYRAHTASASAKLRVLYQGNNITVTAYANGDHAVTVKDAKFIVGPMSSAWTPFDNKIVVYKGDVYNMDYPPFGAGRPGQFGDIQSRTPESKDVYANTQLVLQRPAAGTVHVPYSQAPSGFKYWLKERGASLQHTAPFGCQIATNPVRAMNCAVGNMPISIDIPDAAFTRVVDAPSLTDMSCEVPACTHSSDFGGVAIIKYAVSKKGKCAVHSMTNAVTIREAEIEVEGNSQLQISFSTALASAEFRVQVCSTQVHCAAECHPPKDHIVNYPASHTTLGVQDISATAMSWVQKITGGVGLVVAVAALILIVVLCVSFSRH&quot; ## [1] &quot;MFPFQPMYPMQPMPYRNPFAAPRRPWFPRTDPFLAMQVQELTRSMANLTFKQRRGAPPEGPPAKKSKREAPQKQRGGQRKKKKNEGKKKAKTGPPNLKTQNGNKKKTNKKPGKRQRMVMKLESDKTFPIMLEGKINGYACVVGGKLFRPMHVEGKIDNDVLAALKTKKASKYDLEYADVPQNMRADTFKYTHEKPQGYYSWHHGAVQYENGRFTVPRGVGARGDSGRPILDNQGRVVAIVLGGVNEGSRTALSVVMWNEKGVTVKYTPENCEQWSLVTTMCLLANVTFPCAQPPICYDRKPAETLAMLSANVDNPGYDELLKAAVTCPGRKRRSTEELFKEYKLTRPYMARCVRCAVGSCHSPIAIEAVKSDGHDGYVRLQTSSQYGLDPSGNLKSRTMRYNMYGTIEEIPLHQVSLHTSRPCHIVDGHGYFLLARCPAGDSITMEFKKDSVTHSCSVPYEVKFNPVGRELYTHPPEHGAEQACQVYAHDAQNRGAYVEMHLPGSEVDSSLVSLSSGLVSVTPPAGTSALVECECSGTTISKTINKTKQFSQCTKKEQCRAYRLQNDKWVYNSDKLPKAAGATLKGKLHVPFLLADGKCTVPLAPEPMITFGFRSVSLKLHPKYPTYLTTRELADEPHYTHELISEPSVRNFSVTAKGWEFVWGNHPPKRFWAQETAPGNPHGLPHEVIVHYYHRYPMSTITGLSICAAIVAVSIAASTWLLCRSRASCLTPYRLTPNAKMPLCLAVLCCARSARAETTWESLDHLWNNNQQMFWTQLLIPLAALIVVTRLLKCMCCVVPFLVVAGAAGAGAYEHATTMPNQAGISYNTIVNRAGYAPLPISITPTKIKLIPTVNLEYVTCHYKTGMDSPTIKCCGSQECTPTYRPDEQCKVFAGVYPFMWGGAYCFCDTENTQISKAYVMKSEDCLADHAAAYKAHTASVQALLNITVGEHSTVTTVYVNGETPVNFNGVKLTAGPLSTAWTPFDRKIVQYAGEIYNYDFPEYGAGQPGAFGDIQLRTVSSSDLYANTNLVLQRPKAGAIHVPYTQAPSGFEQWKKDKAPSLKFTAPFGCEIYTNPIRAENCAVGSIPLAFDIPDALFTRVSETPTLSAAECTLNECVYSSDFGGIATVKYSASKSGKCAVHVPSGTATLKEASVELAEQGSVTIHFSTANIHPEFRLQICTSFVTCKGDCHPPKDHIVTHPQYHAQTFTAAVSKTAWTWLTSLLGGSAVIIIIGLVLATLVAMYVLTNQKHN&quot; ## [1] &quot;MFNIKMTISTLLIALIILVIIILVVFLYYKKQQPPKKVCKVDKDCGSGEHCVRGTCSTLSCLDAVKMDKRNIKIDSKISSCEFTPNFYRFTDTAADEQQEFGKTRHPIKITPSPSESHSPQEVCEKYCSWGTDDCTGWEYVGDEKEGTCYVYNNPHHPVLKYGKDHIIALPRNHKHA&quot; ## [1] &quot;MEAVLTKLDQEEKKALQNFHRCAWEETKNIINDFLEIPEERCTYKFNSYTKKMELLFTPEFHTAWHEVPECREFILNFLRLISGHRVVLKGPTFVFTKETKNLGIPSTINVDFQANIENMDDLQKGNLIGKMNIKEG&quot; ## [1] &quot;MAFLMSEFIGLGLAGAGVLSNALLRRQELQLQKQALENGLVLKADQLGRLGFNPNEVKNVIVGNSFSSNVRLSNMHNDASVVNAYNVYNPASNGIRKKIKSLNNSVKIYNTTGESSV&quot; And yep, we have protein sequences. Now looking for some amino acids: for (i in list.files(path=&quot;data/viral_structural_proteins&quot;, pattern=&quot;.*tsv&quot;,full.names=T)[1:5] ) { print(grepl(&quot;QQ&quot;,read.delim(i,sep=&quot;\\t&quot;,header=F,as.is=T)$V3)) } ## [1] TRUE ## [1] TRUE ## [1] TRUE ## [1] FALSE ## [1] FALSE 4.2.4 Storing values from a loop First, how can we store the filenames and use them later for the loop? We need to turn out list of files into indicies. We’ll save it first so we can count how many there are. seq_along is handy function to create a number sequence along a vector, otherwise use something like seq(1,length(x)). # listing files first_five &lt;- list.files(path=&quot;data/viral_structural_proteins&quot;,pattern=&quot;.*tsv&quot;)[1:5] length(first_five) ## [1] 5 # looping through files for (i in seq_along(first_five) ) { print(i) print(first_five[i]) } ## [1] 1 ## [1] &quot;viral_proteins_000.tsv&quot; ## [1] 2 ## [1] &quot;viral_proteins_001.tsv&quot; ## [1] 3 ## [1] &quot;viral_proteins_002.tsv&quot; ## [1] 4 ## [1] &quot;viral_proteins_003.tsv&quot; ## [1] 5 ## [1] &quot;viral_proteins_004.tsv&quot; Okay, but how do we store values for later analysis? In other languages, “append”. But R is not built that way. It’ll work (using something like append, or x &lt;- c(x, new_value)), but it’s inefficient. The “R-way” to store values from a loop is to define a vector of the right length, then put each element in it. Here’s some example vectors that we can create. vector(&quot;character&quot;,10) ## [1] &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; vector(mode=&quot;numeric&quot;,length=5) ## [1] 0 0 0 0 0 vector(&quot;logical&quot;,2) ## [1] FALSE FALSE You can save these to a variable, and thus have a character vector, numeric vector, or logical vector, of different sizes. Putting these together, we can create and save a vector of file names: # listing files first_five &lt;- list.files(path=&quot;data/viral_structural_proteins&quot;,pattern=&quot;.*tsv&quot;)[1:5] # initializing vectors filenamez &lt;- vector(&quot;character&quot;,length(first_five)) # looping through files for (i in seq_along(first_five) ) { filenamez[i] &lt;- first_five[i] } filenamez ## [1] &quot;viral_proteins_000.tsv&quot; &quot;viral_proteins_001.tsv&quot; &quot;viral_proteins_002.tsv&quot; ## [4] &quot;viral_proteins_003.tsv&quot; &quot;viral_proteins_004.tsv&quot; And finally calculate the length of each protein: And # listing files first_five &lt;- list.files(path=&quot;data/viral_structural_proteins&quot;, full.names=T, pattern=&quot;.*tsv&quot;)[1:5] # initializing vectors has_qq &lt;- vector(&quot;logical&quot;,length(first_five)) # looping through files for (i in seq_along(first_five) ) { # calculating length and storing it has_qq[i] &lt;- grepl( &quot;QQ&quot;, read.delim(first_five[i], header=F,as.is=T,sep=&quot;\\t&quot;)$V3 ) } has_qq ## [1] TRUE TRUE TRUE FALSE FALSE Now we can take off the [1:5] limiter, and do the whole set: # listing files filez &lt;- list.files(path=&quot;data/viral_structural_proteins&quot;, full.names=T, pattern=&quot;.*tsv&quot;) # initializing vectors has_qq &lt;- vector(&quot;logical&quot;,length(filez)) # looping through files for (i in seq_along(filez) ) { # calculating length and storing it has_qq[i] &lt;- grepl( &quot;QQ&quot;, read.delim(filez[i], header=F,as.is=T,sep=&quot;\\t&quot;)$V3 ) } has_qq[1:10] ## [1] TRUE TRUE TRUE FALSE FALSE TRUE FALSE FALSE TRUE FALSE How many proteins have “QQ”? Cool trick for logical vectors, sum. sum(has_qq) ## [1] 148 table(has_qq) ## has_qq ## FALSE TRUE ## 95 148 Can also ggplot it, for fun: library(ggplot2) ggplot( data.frame(qq_status=has_qq) )+ theme_classic()+ aes(x=qq_status)+ geom_bar() 4.2.5 Extending the workflow How do you think about/plan the workflow we’ve built? How is it organized? One way is to flatten out all the tasks, and to script each individual task every time it is done. This requires the author, user, and reader to understand a lot of complexity. graph TD classDef default line-height:12px; classDef three line-height:12px,fill:yellow; you[analyst] --> B[finding files] & C[making a vector] & D[looping] & E[reading files] & F[getting protein seq] & G[calculting and saving nchar] & H[plotting nchar] B:::three; C:::three; D:::three; E:::three; F:::three; G:::three; H:::three; Another way to approach this is to cluster them into a hierarchy of modules. graph TD classDef default line-height:12px; classDef one line-height:12px,fill:pink; classDef two line-height:12px,fill:cyan; classDef three line-height:12px,fill:yellow; you2[analyst] --> find & read & plot; find:::one --> B; B[finding files]:::three; read:::one --> looping & reading; looping:::two --> C & D; C[making a vector]:::three; D[looping]:::three; reading[reading and calc]:::two --> E & F & G; E[reading files]:::three; F[getting protein seq]:::three; G[calculting and saving nchar]:::three; plot:::one --> H; H[plotting nchar]:::three; In this organization, the analyst can operate at levels of steps, modules, and specific instructions, depending on what is needed. Organizing your workflows into composable modules lets you extend these to un-ancipiated complexity. You could imaging using these steps or modules: graph LR classDef default line-height:12px; vp[read viral proteins] --> cl[calculate lengths] --> hist[histogram] in new ways by composing the elements together, to analyze a different source of proteins, with a new analysis, with similar plots: graph LR classDef default line-height:12px; vp[read viral proteins] --> cl[calculate lengths] --> hist[histogram] hp[read human proteins] --> cl --> boxplot[boxplot] vp --> cmotif[find a particular motif] --> motifhist[histogram] hp --> cmotif --> motifboxplot[boxplot] Where maybe these are how the inputs/outputs are defined: reading_proteins: input: directory path of proteins to read output: protein sequences calculating lengths: input: protein sequences output: numeric vector, of lengths finding a motif: input: protein sequences output: numeric vector, of presence/absence 4.2.6 Conclusion Write this up! Edit your rmarkdown file to use 4.2.6.1 headers italics bold statements , reference previous figures, or make additional plots, or list out filenames to support your claim. Maybe just spend five minutes for now, but get used to writing your notes down so you save them. Ideas: - You can analyze strings using functions like nchar or grepl. - You can use loops to organize repetitive tasks, such as working through a large list of files and reading them. - Modular design of workflows, using functions or just organized and commented code, can help you extend your analysis to handle new questions as they arise 4.2.7 Totally optional supplements, for your edification 4.2.7.1 stringr and the tidyverse The tidyverse includes a package stringr. This cleans up some of these basic string operations, makes them more standardized, and gives them nice standardized names. It’s worth using, but you should know about the other base R functions! library(stringr) For example the above, datar &lt;- data.frame(stringsAsFactors=F, filenames=c( &quot;210607_pilot_transformation_works.jpeg&quot;, &quot;210609_transformation_seems_to_work.jpeg&quot;, &quot;210610_failed_gel_images.jpeg&quot; ) ) datar$dates &lt;- str_extract(string=datar$filenames,pattern=&quot;\\\\d\\\\d\\\\d\\\\d\\\\d\\\\d_&quot;) datar ## filenames dates ## 1 210607_pilot_transformation_works.jpeg 210607_ ## 2 210609_transformation_seems_to_work.jpeg 210609_ ## 3 210610_failed_gel_images.jpeg 210610_ There are many options, type str_ and then TAB. For much much more about strings, check out Wickham’s R4DS book. Especially for the special characters part Or even cleaner using tidyverse verbs: library(tidyverse) ## ── Attaching packages ───────────────────────────────────────────────────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ tibble 3.0.4 ✔ purrr 0.3.4 ## ✔ tidyr 1.1.2 ✔ dplyr 1.0.2 ## ✔ readr 1.4.0 ✔ forcats 0.5.0 ## ── Conflicts ──────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() tibble(filenames=c( &quot;210607_pilot_transformation_works.jpeg&quot;, &quot;210609_transformation_seems_to_work.jpeg&quot;, &quot;210610_failed_gel_images.jpeg&quot; ) ) %&gt;% separate(col=filenames,sep=&quot;_&quot;,into=c(&quot;date&quot;,&quot;rest&quot;),extra=&quot;merge&quot;) %&gt;% separate(col=rest,sep=&quot;\\\\.&quot;,into=c(&quot;name&quot;,&quot;type&quot;)) ## # A tibble: 3 x 3 ## date name type ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 210607 pilot_transformation_works jpeg ## 2 210609 transformation_seems_to_work jpeg ## 3 210610 failed_gel_images jpeg I’d recommend you use the tidyverse-style manipulations - they’re powerful. But you should be familiar with base R, to fix bugs. 4.2.7.2 More tidyverse demo tidyverse is pretty neat at doing these things in simpler, more readable code. Here’s how you might approach the motif-finding task in that paradigm: 1st - list files, but set it in a tibble: tibble( filenames=list.files(path=&quot;data/viral_structural_proteins&quot;, full.names=T, pattern=&quot;.*tsv&quot;) ) ## # A tibble: 243 x 1 ## filenames ## &lt;chr&gt; ## 1 data/viral_structural_proteins/viral_proteins_000.tsv ## 2 data/viral_structural_proteins/viral_proteins_001.tsv ## 3 data/viral_structural_proteins/viral_proteins_002.tsv ## 4 data/viral_structural_proteins/viral_proteins_003.tsv ## 5 data/viral_structural_proteins/viral_proteins_004.tsv ## 6 data/viral_structural_proteins/viral_proteins_005.tsv ## 7 data/viral_structural_proteins/viral_proteins_006.tsv ## 8 data/viral_structural_proteins/viral_proteins_007.tsv ## 9 data/viral_structural_proteins/viral_proteins_008.tsv ## 10 data/viral_structural_proteins/viral_proteins_009.tsv ## # … with 233 more rows 2nd - pipe that into a mutate call, where you use map on filenames. map is like lapply. It takes whatever you have on the left, and puts it one by one into the function on the right, along with optional arguments that you list after it. Here, we set col_names to false, analogous to header in read.delim. tibble( filenames=list.files(path=&quot;data/viral_structural_proteins&quot;, full.names=T, pattern=&quot;.*tsv&quot;) ) %&gt;% mutate(rawfiles=map(filenames,read_tsv,col_names=F)) ## # A tibble: 243 x 2 ## filenames rawfiles ## &lt;chr&gt; &lt;list&gt; ## 1 data/viral_structural_proteins/viral_proteins_000.tsv &lt;tibble [1 × 3]&gt; ## 2 data/viral_structural_proteins/viral_proteins_001.tsv &lt;tibble [1 × 3]&gt; ## 3 data/viral_structural_proteins/viral_proteins_002.tsv &lt;tibble [1 × 3]&gt; ## 4 data/viral_structural_proteins/viral_proteins_003.tsv &lt;tibble [1 × 3]&gt; ## 5 data/viral_structural_proteins/viral_proteins_004.tsv &lt;tibble [1 × 3]&gt; ## 6 data/viral_structural_proteins/viral_proteins_005.tsv &lt;tibble [1 × 3]&gt; ## 7 data/viral_structural_proteins/viral_proteins_006.tsv &lt;tibble [1 × 3]&gt; ## 8 data/viral_structural_proteins/viral_proteins_007.tsv &lt;tibble [1 × 3]&gt; ## 9 data/viral_structural_proteins/viral_proteins_008.tsv &lt;tibble [1 × 3]&gt; ## 10 data/viral_structural_proteins/viral_proteins_009.tsv &lt;tibble [1 × 3]&gt; ## # … with 233 more rows 3rd - we then unnest the rawfiles. This takes a column of tibbles and makes it magically just line up with the top level tibble. Now we’ll also save it as variable datar. datar &lt;- tibble( filenames=list.files(path=&quot;data/viral_structural_proteins&quot;, full.names=T, pattern=&quot;.*tsv&quot;) ) %&gt;% mutate(rawfiles=map(filenames,read_tsv,col_names=F)) %&gt;% unnest(rawfiles) Then we can process on datar. Note the use of mutate, map, and wrapping map in unlist to just return the lengths instead of a list of lengths. datar %&gt;% mutate( protein_length=unlist(map(X3,nchar)), qq_present=unlist(map(X3,grepl,pattern=&quot;QQ&quot;)), ) ## # A tibble: 243 x 6 ## filenames X1 X2 X3 protein_length qq_present ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;lgl&gt; ## 1 data/viral_str… Q8JUX5.3 … Chikungun… MEFIPTQTFYNR… 1248 TRUE ## 2 data/viral_str… YP_009507… Everglade… MFPFQPMYPMQP… 1254 TRUE ## 3 data/viral_str… NP_042704… African s… MFNIKMTISTLL… 177 TRUE ## 4 data/viral_str… NP_042736… African s… MEAVLTKLDQEE… 137 FALSE ## 5 data/viral_str… P27413.1 … Rabbit he… MAFLMSEFIGLG… 117 FALSE ## 6 data/viral_str… NP_042800… African s… MDTETSPLLSHN… 117 TRUE ## 7 data/viral_str… NP_050192… Human bet… MDLKAQSIPFAW… 858 FALSE ## 8 data/viral_str… Q306W5.1 … Eastern e… MFPYPTLNYSPM… 1242 FALSE ## 9 data/viral_str… NP_040824… Venezuela… MFPFQPMYPMQP… 1255 TRUE ## 10 data/viral_str… YP_003987… Acanthamo… MNYSLEDLPNSG… 234 FALSE ## # … with 233 more rows And then we can pipe it into a ggplot: datar %&gt;% mutate( protein_length=unlist(map(X3,nchar)), qq_present=unlist(map(X3,grepl,pattern=&quot;QQ&quot;)), ) %&gt;% ggplot()+ aes(x=qq_present)+ geom_bar() And the entire code: tibble( filenames=list.files(path=&quot;data/viral_structural_proteins&quot;, full.names=T, pattern=&quot;.*tsv&quot;) ) %&gt;% mutate(rawfiles=map(filenames,read_tsv,col_names=F)) %&gt;% unnest(rawfiles) %&gt;% mutate( protein_length=unlist(map(X3,nchar)), qq_present=unlist(map(X3,grepl,pattern=&quot;QQ&quot;)), ) %&gt;% ggplot()+ aes(x=qq_present)+ geom_bar() I think that’s tidy. One crucial aspect missing is the actual bash commands used. I did not save them as a script, and I should go back, save them in a script, and re-run that to make sure I get the same result!↩ The formats you’re likely going to have to deal with are CSV, TSV, FASTQ, FASTA, SAM - at least. ↩ …since we’re just demonstrating R coding and not representing ourselves as being familiar enough with the field to properly interpret these results. I have no idea what occurance of this would mean or not mean, or are we doing any sort of statistics or comparisons to null expectations.↩ "],["packages-install-and-use.html", "4.3 Packages - install and use", " 4.3 Packages - install and use Think about sharing your code from the last few days. How would you share this with the other people in the course? Can they use it easily and flexibly to handle lots of different problems? R, like other languages, is built on a package system - if you wrap up your code in particular expected ways and put it in particular expected places, it is very easy for others to get and use your code Packages are said to be easy to make, from scratch. There’s more comprehensive instruction here. Personally, the author has never had cause to make one, but I can report that they’re very easy to use! 4.3.1 What packages do you have already? You have a few. Here’s a list: installed.packages() But which are loaded? sessionInfo() The packages are in a directory, the location is stored in this: .libPaths() I’m not running that one, as it’ll be different for each user. 4.3.2 CRAN is the canonical R package resource The Comprehensive R Archive Netowrk CRAN is an integral part of the R environment. This is a global and free resource for storing and distributing code as R packages - well documented and designed chunks of code. You can install packages using the RStudio IDE, but it’s most important to know how to do it in R - what’s happening just under the hood. Use library to load packages. You should be able to load a package you’ve already installed. Call library() function, with an argument of the library name, as a string. For example: library(&quot;stringdist&quot;) library(stringdist) Don’t have it? You can look for packages by using a search engine, or finding suggestions in the documentation, papers. Once you have a package name, use install.packages(&quot;stringdist&quot;) to install it. You may need to select a “mirror”. This is just where you are downloading from, usually just go with “0-Cloud” 6. What did this do? It retrieves and downloads the package’s code and documentation to what’s called your library. That’s a folder on your computer where the compiled versions of the packages are located, ready to be loaded when you want to use them. Try it out. Want to try it again? You can uninstall packages by removing them: remove.packages(&quot;stringdist&quot;) Each package has a page on CRAN with lots more information. The reference manual is a very specific reference document with per function level documentation. Vingettes are write ups that are more explaining how to do things - usually. 4.3.2.1 ggrepel Sometimes points are way to close, and you can’t label them properly. For example, library(tidyverse) ## ── Attaching packages ───────────────────────────────────────────────────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.3.2 ✔ purrr 0.3.4 ## ✔ tibble 3.0.4 ✔ dplyr 1.0.2 ## ✔ tidyr 1.1.2 ✔ stringr 1.4.0 ## ✔ readr 1.4.0 ✔ forcats 0.5.0 ## ── Conflicts ──────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() set.seed(1234) # this is just to get the same results each time we run read_csv(&quot;data/iris.csv&quot;,col_types=&#39;nnnncc&#39;) %&gt;% slice_sample(n=20) %&gt;% ggplot()+ aes(x=Sepal.Length,y=Sepal.Width,col=Species,label=Species)+ geom_label()+ geom_point() ## Warning: 4 parsing failures. ## row col expected actual file ## 44 Sepal.Length a number n/a &#39;data/iris.csv&#39; ## 44 Sepal.Width a number n/a &#39;data/iris.csv&#39; ## 148 Petal.Length a number n/a &#39;data/iris.csv&#39; ## 148 Petal.Width a number n/a &#39;data/iris.csv&#39; ## Warning: Removed 1 rows containing missing values (geom_label). The ggrepel library, from CRAN, is a handy way to achieve this: library(ggrepel) set.seed(1234) # this is just to get the same results each time we run read_csv(&quot;data/iris.csv&quot;,col_types=&#39;nnnncc&#39;) %&gt;% slice_sample(n=20) %&gt;% ggplot()+ aes(x=Sepal.Length,y=Sepal.Width,col=Species,label=Species)+ geom_label_repel()+ geom_point() ## Warning: 4 parsing failures. ## row col expected actual file ## 44 Sepal.Length a number n/a &#39;data/iris.csv&#39; ## 44 Sepal.Width a number n/a &#39;data/iris.csv&#39; ## 148 Petal.Length a number n/a &#39;data/iris.csv&#39; ## 148 Petal.Width a number n/a &#39;data/iris.csv&#39; ## Warning: Removed 1 rows containing missing values (geom_label_repel). 4.3.3 Using packages without library()-ing them Some packages you don’t really want to have to go through the trouble of loading, you’ll just want one function. For example, this website is generated with one function, render_book() in the package bookdown. Instead of: library(bookdown) render_book() I can save a line by just using :: bookdown::render_book() This means I am looking in library bookdown for function render_book. The function is hidden again right afterwards, but it is useful if you just want to run one function from a library, once. This is super handy when wanting to typeset (render) a Rmd file: rmarkdown::render() I never load the rmarkdown package, I just type the above. A lot. 7 4.3.4 Github is a common place for sharing packges in development Installing from github requires some different programming - and there’s libraries with functions for this. library(&quot;remotes&quot;) This used to be part of the devtools package, but the authors have been “uncoupling” these to make each package simple and composable. 4.3.4.1 Let’s install a package off of github. Here’s a cool Ghibli color palette, so you can change sets of colors for your plots. Try installing it with: remotes::install_github(&quot;ewenme/ghibli&quot;) Then adapt one of your previous plots (that has colors) to use these new color palettes. 4.3.4.2 Let’s install another one, to check the previous Colorblind folks can’t distinguish between certain colors so well. Designing figures so that colorblind folks can readily distinguish the relevant differences is a good idea. It makes for a better figure, just like increasing the font size and cleaning up distracting artifacts. The ghibli package above has been claimed to be somewhat good for plotting for universal visibility. Is that true? Let’s install this package to check: remotes::install_github(&quot;nowosad/colorblindcheck&quot;,upgrade=&quot;never&quot;) Then library(ghibli) library(colorblindcheck) palette_check(ghibli_palettes$PonyoMedium,plot=T) ## name n tolerance ncp ndcp min_dist mean_dist max_dist ## 1 normal 7 9.939857 21 21 9.939857 36.11891 59.92427 ## 2 deuteranopia 7 9.939857 21 19 7.044647 30.14324 52.02502 ## 3 protanopia 7 9.939857 21 20 8.068943 26.94418 50.93756 ## 4 tritanopia 7 9.939857 21 20 4.587519 35.34529 60.29190 Other packages on there: Sports field plotting Ridgeplots in ggplot2 what else? 4.3.5 Bioconductor is a great place for bio-related packages Bioconductor is a giant repository of just scientific, well bio-focused, packages. It’s where a lot of bioinformatics software gets published. However, it has it’s own package system and thus package manager. You will have to install it … from CRAN. Like so: install.packages(&quot;BiocManager&quot;) That installs a package with a function that then installs Bioconductor packages. It’s actually easy. Let’s give it a try by installing and using the limma package to analyze some gene expression data. If you’re recall from before, we can use :: to avoid having to actually library the whole package: BiocManager::install() And so we can use this to install important packages like DESeq2. BiocManager::install(&#39;DESeq2&#39;) Hold off on installing this quite yet. It’d take a while. In Bioconductor, I always say to update ‘none’ (press ‘n’ when prompted). There’s lots of good documentation on each package’s webpage, like vingettes. 4.3.6 Major problems with package installations Somethings can go wrong. You’ll want to pay attention to what is output from the installation attempt. Look for errors. Scroll up to find warnings and errors. These can arise from multiple ways. Here’s a few ideas: the package name you put in is misspelled the package is in another repo (CRAN, Bioconductor, etc) your internet is down (this is more of a Stanford WiFi problem) your R package library directory is not writable (you don’t have permission) the package is not available for this version of R the package relies on the system having programs or software or libraries installed that are not installed (less of a problem with RStudio) the package actually has a bug Let’s next talk about how to find help for problems. I have no idea where this is, never heard of that country!↩ Or, I use the one-liner in bash of Rscript -e 'rmarkdown::render(\"notebook.Rmd\")' ↩ "],["getting-help-troubleshooting.html", "4.4 Getting help - troubleshooting", " 4.4 Getting help - troubleshooting You will have problems. Virtually all of these problems will have been encountered by someone before, and most of these workarounds will be findable online. How do you (1) find a way past these problems and (2) learn from them? What kinds of errors did we encounter in this class? How do we deal with these? General help - problems of not knowing where to start, what tool is even possible to use Specific debugging - problems of not knowing how to do something, use a tool correctly Generally, approach these problems as you would a scientist. Treat them (and others) with respect and an open mind. Reduce the problem, measure what is going on before/around the error, develop a theory about what is going on, and test that idea. Ask for help and ideas from friends and colleagues. And if it’s a problem that doesn’t have a readily available solution, consider writing it up on a blog or site like StackOverflow! 4.4.1 Where to start? General help For the first, you can ask for ideas in a few places: Typing your question into a text search engine (like google) will likely give you many ideas and ways forward. As you become more experienced and better understand the programming concepts behind this work, you will better able to ask more specific and powerful questions (and ask questions that get past the basic search-engine optimized commercial sites). If you don’t know how to ask, or wonder if there’s a better or alternative approach, then asking friends, advisors, mentors, or just about anyone can help. You can also just blast it out on twitter - that actually works sometimes. Someone may know the answer, or know a tangentially related fact that may be relavant, or simply the process of explaining your problem often helps you sharpen your thoughts and think about things in a new way. 4.4.2 Debugging specific problems You will often be trying to do something, and get specific errors as your implementation of your idea fails to work. Getting help in this way is best approached by debugging. Debugging is a term that means troubleshooting specific errors in your programming project. These are valuable opportunities. Never waste an error message, if you can afford the time to. Cherish the bugs, because these moments of surprise are like mutants - the strange and unexpected behavior, if appreciated, can reveal hidden processes. The process of gaining understanding will make you wiser and faster 8. If you can’t find an easy answer, it is important to begin to approach the problem like you would a science problem. Reduce the problem, eliminate extraneous variables. Use or capture intermediate steps to identify where the issue is (try putting lots of print(x) functions in your code to print out what variables are inbetween functions/calls/loops). There are several things to try: First, read the error message. As you become more experienced and familiar with programming concepts, you will learn to be able to understand what is actually being said. For example, if it complains that a value is numeric instead of character, you may have saved it as a numeric value. If it is in the context of some characters getting put into a data.frame, you may have forgotten to use as.is=T or stringsAsFactors=F to prevent character strings from being converted to factors, which get converted to integers. Error messages may be more complex, where you can’t understand them. One approach is to simply copy and paste portions or entire error messages into text search enginges, like Google or DuckDuckGo. Often, this will find places online where others have asked for help. There can be solutions to the problem, or there can be more information that may help you debug the problem. If you can’t find a way to solve a specific problem in this way, it makes a lot of sense to “get unstuck” and bug your friends, co-workers, and advisors. The best help will explain the problem, and/or be able to tell you what concept you need to read about to understand what the error is. Or hey, use the Slack ! If this doesn’t work, you can also ask questions online. One very common place is StackOverflow. You’ll need to have an account. Make sure to have researched the problem to be reasonably sure you’re not asking a duplicate question. The people answering are volunteers. If you’re using a specific package or tool, you can often ask these questions in the “Issues” page on the git repo for the tool. Most tools will be on a git repo, either GitHub, GitLab, or BitBucket. For example, feel free to “open an issue” on the repo for this site. There are specific sites for other areas. Biostars is one similar to StackOverflow, for bioinformatics. You can also blast it out on twitter. People will probably help. 4.4.3 Minimal reproducible examples Often, people will be much better able (and willing) to help you if you have done the work of making the problem simpler. This is often achieved by creating a “minimal reproducible example” This means it’s an example, that reproduces the problem, but it’s as simple and minimal as possible (to still get the error). This is a useful exercise for you to do, as you may use it to find a way forward without ever asking someone. To do this: Identify as best you can where the problem is - a specific script, code chunk, code block, line, variable? Make a copy of your code where you can make a really minimal version of this. In that minimal copy, cut out everything that doesn’t affect the generation of that error. Remove all extra metadata, extra plots, extra libraries, anything that is not necessary to get to the error. If you have a big dataset, figure out if you can reproduce the error with a smaller dataset - is it just one record that creates the error? Use tools and ideas from Day1/2 to do this, or select only certain problematic rows. Figure out if it is possible to upload the minimal data and minimal script somewhere (respecting HIPAA!), so that others can have an example they can work on to help you. Remember to include packages, and the output of sessionInfo(). Some bugs may be “impossible”, but these are rare.↩ "],["the-struggle.html", "4.5 The Struggle", " 4.5 The Struggle It never gets any easier. You just have to love it. – Professor Suse Broyde People get good at R, linux, biochemistry, trombone, whatever by spending years in the practice of those skills, facts, and ideas. To develop this expertise, you must simply attempt problems. Some say this is a productive way to learn things, to engage in a productive struggle. It’s this way for R, same thing for research. How challenging should things be? Some folks offer the simple guidance that you should be doing just fine 1/3rd of the time, completely confident to the point of boredom for another 1/3rd, and struggling completely out of your depth for another 1/3rd. These are arbitrary numbers, but the point is that you should be struggling with new things from a solid basis of routine skill. Build up the basics, and practice them in your work, by applying your new skills. The biggest advantage you can build for yourself may be in maintaining a learning community. It’s really handy to have supervisors, co-workers, and friends who you can go to for help. Additionally, by helping others you hone the skills that are the basis for your further development. A teacher’s skills often dull without students. Engaging in a learning community as a teacher or student, and switching roles between these, builds an ecology of learning that helps everyone. Through this summer, try to do that for and with each other! Check out the end of the worksheet document for problems to attempt in this section. "],["conclusion-1.html", "4.6 Conclusion", " 4.6 Conclusion Today we covered: Workflows Project organization Strings, patterns and lengths Loops Packages Finding help And we gave this a try with starting one of a handful of different projects We hope you are better oriented and prepared to build scalable workflows in R in your summer work and share these with others, as you make your contribution to our collective scientific endeavour. "],["appendix.html", "5 Appendix ", " 5 Appendix "],["some-links-and-some-extras.html", "5.1 Some links and some extras", " 5.1 Some links and some extras This is for collecting factoids, ideas, and links that may be useful as reference (if not inspiration) for your future R practice. 5.1.1 Ideas for figures An interactive page showing different types of figures A R graph gallery Claus Wilke’s cowplot package is a great way to make multi-panel figures with ggplot, so A B etc. 5.1.2 More links swirl] is an excellent interactive tutorial package you can install and work through yourself for more practice, get started here A whole bunch of cheatsheets and example sheets of common commands and ways to do things, check under “Contributed Cheatsheets” for some great ones, or browse them all Claus Wilke’s dataviz bookdown Computational Genomics with R UC Davis bioinformatics trainings R Programming for Data Science Biostars books on scripting, programming, genomics Some basic non-tidyverse common commands 5.1.3 More about workflows We talked about workflows in R, but much of bioinformatics workflows are happening outside of R, in bash (etc). To handle those, there are more sophisticated workflows that will generate steps from combinations of parameters, schedule and allocate those across HPC and remote commercial (amazon/google) computing, and automatically generate analytics and reports to help you optimize your workflows. These include SnakeMake, Nextflow, and CWL. And others, but these are the main ones. This author recommends Nextflow. There’s a bit more of a learning curve than SnakeMake, but it is well built and very flexible. These will not be necessary for summer projects, but may be useful skills for bioinformatics research for at least the next 5-10 years. 5.1.4 Rmarkdown for slides Want to put a bunch of R plots into some slides? You can also turn your Rmarkdown file into a set of presentation slides, by adding the following line in the YAML header: output: slidy_presentation output: slidy_presentation: theme: default This makes a HTML file you can open in your browser, like slides (left, right for the slides). "],["style-writing-reusable-code.html", "5.2 Style - writing (re)usable code", " 5.2 Style - writing (re)usable code Crack open your code - can you use it again? Can you adapt it to modify your question, feed in new data, and modify the scientifically-important bits easily? It might be useful for you to be able to do these two things! Modular design with functions is one way to do that. 5.2.1 Functions as modular steps The beauty and danger of any programming project is that there are many ways to organize equivalent functionality. While liberating, unorganized variety in your code is more difficult for to read and help you with, and is more difficult for future you to read and re-use. One way to clearly organize code is to use a “functional” approach. This approach uses functions, as you learned a bit about on Wednesday, to organize your code into “code blocks”. You “call” this function by giving it “arguments”, and the function will return “return values”. This approach allows you to focus your attention on making (and testing that) a function does one thing, and does it well. Then later you can use these larger pieces in novel combinations to flexibly adapt to new requirements in your analysis workflows. For example, in one of the workshop’s author’s recent papers, they defined a function that wrote out multiple ggplot formats at once so that they wouldn’t have to re-type that code ever again. Functions are handy. For example, let’s calculate the standard error of a sample of values: values &lt;- c(4,3,2,2,5,3,6,2,2,4) stderr(values) ## Error in stderr(values): unused argument (values) Er … what? What does stderr() do? Use the help functionality of ? to figure out what the stderr() function is built for, and if there are any other functions that calculate the standard error of the mean. It would appear that functionality is not built into base R. Perhaps they assume that everyone will write their own standard error function? Let’s write one. How do you calculate the standard error of the mean of a sample? 9 Does that look familiar from your stats classes? Let’s code it up. I recommend you first develop the code in your functions by playing with it interactively: sd(values)/sqrt(length(values)) ## [1] 0.4484541 To use this multiple times, we could just copy and paste it into each script or workflow we want to use it in. However, this leaves lots of complexity for the future user to have to handle. For example, we can copy and paste a new variable name into the code: values2 &lt;- c(10,30,20) sd(values2)/sqrt(length(values)) ## [1] 3.162278 This can lead to errors in writing, small incorrect parts. In the above example, I purposefully neglected to change the second values to values2. Did you catch that easily, or did you have to look for it? Additionally, how do we save it, document it, and share it? Copy and pasting from a file is okay, but it takes up a lot more work/space. We could email around a file of code chunks, or share on a website. How do we make sure it works? This is hard to do with copy-paste code chunks. Changing the chunk to work on a new input can be an opportunity for introducing errors, typos. 5.2.1.1 Write it as a function! This would make a useful function. What are the parameters? Are any defaults set? How does it calculate with the parameters? What does it return? Once it is a function, it can be copy and pasted, or source()’d from a .R file. As you develop it, you can test that it works and change inputs easily. Later, you could change the function to fix bugs, and you don’t have to then fix all the places you copied it to - it’s fixing the code inside the function. sez &lt;- function(x) {sd(x)/sqrt(length(x))} sez(values) ## [1] 0.4484541 sez(rnorm(10)) ## [1] 0.2786735 sez(rnorm(100)) ## [1] 0.1049428 sez(rpois(1e3,3)) ## [1] 0.05554657 5.2.2 Tips for modular workflows Try to not “hardcode” things - if it’s a number, consider if it can be a parameter that is “passed in” as an argument. Group repeated code functions - some folks say you should never repeat code (but do what works for you!). Try to read inputs and outputs as general, flexible formats - strings of filenames, vectors of values Write a comment at the top of the function that says what it’s doing and what to expect, generally comment things. Consider, the list.files() and hist() functions are already built this way! 5.2.3 Apply is a popular tool Apply is another common way of doing something over and over. It is a very compact way to take pieces of a list, vector, dataframe, or matrix and put them into a function. There are: apply - for 2D objects lapply - for lists and vectors sapply - is lapply but with simplified returns mapply - is for combinations of multiple variables replicate - calls a function multiple times You would be helped by knowing what these are, roughly what the idea is, and how to read other code that has these. Some people strongly prefer coding this way. Here is an example: list_of_protein_files &lt;- lapply( list.files(&quot;data/viral_structural_proteins/&quot;,full.names=T), read.delim, sep=&quot;\\t&quot;,header=F) These all: take some variable with multiple values, either along a list or along the rows or columns of a data.frame/matrix sticks each one of these into a function returns the values, often as a list (unlist() is helpful here) Checkout one of the help pages (?lapply) to see specifics. These are similar to the Map() function and “map” functional programming in other languages. You don’t need to program this way, but there are some advantages to this style if you’d like to learn. 5.2.4 Learn from examples Let’s look at two chunks of code from a paper (lightly edited). The experiment is counting barcoded lineages of yeast cells to estimate “PPIs” (protein-protein interactions) 10 Here’s an example of one style of writing R script: # filter out bad barcode lineages ( &lt;= 2 time points counts &gt; 0, or maximum of each time point &lt;= 5 or total counts of a lineage &lt; 10) bad_index = rep(0, nrow(DBC_known_counts)) for(i in 1:length(bad_index)){ counts = as.numeric(DBC_known_counts[i, 4:8]) if (length(which(counts != 0)) &lt; 3 | max(counts) &lt; 5 | sum(counts) &lt; 10){ bad_index[i] = 1 } } length(which(bad_index == 1)) # 1447775 What is going on here? How do you feed in new data? How do you run this multiple times? How do you change the logic? Here’s another example from the same author: Random_reference &lt;- function(PPI_multiple, all_PPI_filtered, size, sampling_number){ PPI_PPiseq = PPI_multiple[,1] RRS_size= 0 RRS= character(length=0) while (RRS_size &lt; size) { yeast_PPI_random= all_PPI_filtered[sample(1: length(all_PPI_filtered), sampling_number, replace=F)] RRS= unique(c(RRS, yeast_PPI_random)) RRS_overlap= intersect(RRS, PPI_PPiseq) RRS_size= length(RRS_overlap) } RRS_duplicate_marked = mark_duplicates_fast(RRS_overlap) return (RRS_duplicate_marked[,1]) } # ... and later ... Random_reference(PPI_multiple, all_PPI_filtered, number_PPI, sample_number) Same questions. What are some differences? What’s useful for you? What’s a lot of effort to do? 5.2.5 Style guides can be inspiring What’s your coding style going to be? tidyverse style guide google-specific changes Jean Fan’s search for “R style guide” Be inconsistently consistent! Do what works! Balance for yourself: How easy is it to write? How easy is it for you to read? How easy is it for others to read? How similar is it to what everyone else is doing (a very good thing)? But most importantly, use what folks around you are using. Be lazy and copy others! How would you describe your code writing style? How do you name variables, functions? How good are your comments? Are there any ideas here you would like to incorporate? I always forget and look at wikipedia…↩ Ask Darach if you want details, or read the paper↩ "]]
